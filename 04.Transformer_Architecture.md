# O4: Transformer Architecture

## Table of Contents
- [Introduction to Transformers](#introduction-to-transformers)
- [Architecture Overview](#architecture-overview)
- [Self-Attention Mechanism](#self-attention-mechanism)
- [Multi-Head Attention](#multi-head-attention)
- [Positional Encoding](#positional-encoding)
- [Feed-Forward Networks](#feed-forward-networks)
- [Layer Normalization](#layer-normalization)
- [Encoder-Decoder Structure](#encoder-decoder-structure)
- [Attention Variants](#attention-variants)
- [Transformer Optimizations](#transformer-optimizations)
- [Key Takeaways](#key-takeaways)
- [Practice Questions](#practice-questions)

---

## Introduction to Transformers

### The Transformer Revolution

**"Attention Is All You Need"** (Vaswani et al., 2017) introduced the Transformer architecture, revolutionizing NLP and AI.

```
Before Transformers (RNNs/LSTMs):
Problems:
✗ Sequential processing (slow)
✗ Vanishing gradients
✗ Limited context window
✗ Difficult to parallelize

After Transformers:
Solutions:
✓ Parallel processing
✓ Attention mechanism
✓ Long-range dependencies
✓ Highly parallelizable
✓ Foundation for modern AI

Impact:
→ GPT, BERT, T5 (NLP)
→ Vision Transformers (ViT)
→ AlphaFold (protein folding)
→ Stable Diffusion (images)
```

### Why Transformers Work

```
Key Insight: Attention mechanism

Instead of processing sequentially:
RNN: word₁ → word₂ → word₃ → ...

Process all words simultaneously:
Transformer: All words attend to each other in parallel

Benefits:
1. Parallelization → Faster training
2. Long-range dependencies → Better understanding
3. Scalability → Can train huge models
4. Flexibility → Works across modalities
```

---

## Architecture Overview

### High-Level Structure

```
Original Transformer (Sequence-to-Sequence):

Input Sequence
     ↓
┌────────────────┐
│    Encoder     │ (6 layers)
└────────┬───────┘
         ↓
┌────────────────┐
│    Decoder     │ (6 layers)
└────────┬───────┘
         ↓
Output Sequence

Modern Variants:

Encoder-Only (BERT):
Input → Encoder → Embeddings/Classification

Decoder-Only (GPT):
Input → Decoder → Next Token Prediction

Encoder-Decoder (T5):
Input → Encoder → Decoder → Output
```

### Transformer Block Components

```
Single Transformer Layer:

Input
  ↓
┌─────────────────────────────┐
│ Multi-Head Self-Attention   │ ← Learn relationships
├─────────────────────────────┤
│ Add & Norm (Residual)       │ ← Stabilize training
├─────────────────────────────┤
│ Feed-Forward Network        │ ← Transform features
├─────────────────────────────┤
│ Add & Norm (Residual)       │ ← Stabilize training
└─────────────────────────────┘
  ↓
Output (to next layer)

Stack N times (e.g., 12, 24, 96 layers)
```

### Detailed Architecture

```
Full Processing Pipeline:

1. Input Embeddings:
   Tokens → Learned vectors (512 dim, 768 dim, etc.)

2. Positional Encoding:
   Add position information

3. Transformer Layers (×N):
   a. Multi-head attention
   b. Residual connection + Layer norm
   c. Feed-forward network
   d. Residual connection + Layer norm

4. Output Layer:
   Linear projection + Softmax

Dimensions (BERT-Base example):
- Vocabulary: 30,522 tokens
- Embedding: 768 dimensions
- Layers: 12
- Attention heads: 12
- Hidden size: 3,072
- Parameters: 110M
```

---

## Self-Attention Mechanism

### Intuition

```
Goal: Determine which words to focus on

Example: "The animal didn't cross the street because it was too tired."

When processing "it":
- High attention to "animal" (pronoun reference)
- Low attention to "street"
- Understanding: "it" = "animal"

Self-attention computes relevance scores between all word pairs
```

### Query, Key, Value (QKV)

```
For each word, create three vectors:

Query (Q): "What am I looking for?"
Key (K): "What do I contain?"
Value (V): "What information do I provide?"

Process:
1. Each word generates Q, K, V from its embedding
2. Compare Query of word A with Keys of all words
3. Get attention scores
4. Use scores to weight Values
5. Combine weighted Values → Output

Analogy:
Database search:
- Query: Your search
- Keys: Database indices
- Values: Retrieved data
```

### Mathematical Formulation

```
Given input X (sequence of embeddings):

1. Linear projections:
   Q = XW_Q  (Query)
   K = XW_K  (Key)
   V = XW_V  (Value)

2. Scaled dot-product attention:
   Attention(Q, K, V) = softmax(QK^T / √d_k) V

Where:
- QK^T: Similarity scores (dot products)
- √d_k: Scaling factor (d_k = dimension of keys)
- softmax: Convert scores to probabilities
- Multiply by V: Weighted sum of values

Dimensions:
Input: (seq_len, d_model)
Q, K, V: (seq_len, d_k)
Attention output: (seq_len, d_k)
```

### Step-by-Step Example

```
Sentence: "The cat sat"
Embeddings: 4 dimensions (simplified)

Step 1: Get Q, K, V for each word
(Using learned weight matrices W_Q, W_K, W_V)

Q_the = [1.0, 0.5, 0.2, 0.1]
Q_cat = [0.8, 1.0, 0.3, 0.2]
Q_sat = [0.3, 0.4, 1.0, 0.5]

K_the = [0.9, 0.4, 0.1, 0.2]
K_cat = [0.7, 0.9, 0.2, 0.3]
K_sat = [0.2, 0.3, 0.9, 0.6]

V_the = [1.2, 0.6, 0.3, 0.1]
V_cat = [0.9, 1.1, 0.4, 0.2]
V_sat = [0.4, 0.5, 1.2, 0.7]

Step 2: Compute attention scores for "cat"
(Q_cat · K_each)

cat→the: Q_cat · K_the = 0.8×0.9 + 1.0×0.4 + ... = 1.3
cat→cat: Q_cat · K_cat = 0.8×0.7 + 1.0×0.9 + ... = 1.8
cat→sat: Q_cat · K_sat = 0.8×0.2 + 1.0×0.3 + ... = 0.8

Step 3: Scale by √d_k
d_k = 4, so √d_k = 2

Scaled scores: [1.3/2, 1.8/2, 0.8/2] = [0.65, 0.9, 0.4]

Step 4: Softmax
exp([0.65, 0.9, 0.4]) = [1.92, 2.46, 1.49]
Sum = 5.87
Probabilities: [0.33, 0.42, 0.25]

Step 5: Weighted sum of Values
Output_cat = 0.33×V_the + 0.42×V_cat + 0.25×V_sat
          = 0.33×[1.2,0.6,0.3,0.1] + 0.42×[0.9,1.1,0.4,0.2] + 0.25×[0.4,0.5,1.2,0.7]
          = [0.874, 0.785, 0.567, 0.291]

Interpretation:
"cat" attends most to itself (0.42)
and moderately to "the" (0.33) and "sat" (0.25)
```

### Attention Visualization

```
Attention Matrix (3×3 for "The cat sat"):

       The   Cat   Sat
The  [ 0.5   0.3   0.2 ]
Cat  [ 0.33  0.42  0.25]
Sat  [ 0.2   0.3   0.5 ]

Interpretation:
- "The" attends mostly to itself (0.5)
- "Cat" attends to "Cat" (0.42) and "The" (0.33)
- "Sat" attends to itself (0.5) and "Cat" (0.3)

Darker cells = higher attention
```

---

## Multi-Head Attention

### Why Multiple Heads?

```
Single attention head:
- Learns one type of relationship
- Limited representation capacity

Multiple attention heads:
- Each learns different patterns
- Syntactic, semantic, positional relationships
- Richer representations

Example (8 heads processing "The cat sat on the mat"):

Head 1: Subject-verb relationships
  "cat" → "sat" (high attention)

Head 2: Determiner-noun relationships
  "The" → "cat" (high attention)
  "the" → "mat" (high attention)

Head 3: Preposition-object relationships
  "on" → "mat" (high attention)

Head 4-8: Other patterns...
```

### Architecture

```
Multi-Head Attention:

Input (d_model = 512)
     ↓
Split into h heads (h = 8)
     ↓
┌─────────────────────────────────┐
│ Head 1 │ Head 2 │ ... │ Head 8 │
│ (64d)  │ (64d)  │     │ (64d)  │
│        │        │     │        │
│ Attn   │ Attn   │ ... │ Attn   │
└─────────────────────────────────┘
     ↓
Concatenate heads
     ↓
Linear projection (W_O)
     ↓
Output (d_model = 512)

d_k = d_v = d_model / h = 512 / 8 = 64

Each head has smaller dimension but runs in parallel
```

### Mathematical Definition

```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W_O

where:
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

Parameters:
W_i^Q ∈ ℝ^(d_model × d_k)  (Query projection for head i)
W_i^K ∈ ℝ^(d_model × d_k)  (Key projection for head i)
W_i^V ∈ ℝ^(d_model × d_v)  (Value projection for head i)
W_O ∈ ℝ^(hd_v × d_model)   (Output projection)

Complexity:
Same as single-head attention (parallelization)
```

### Implementation

```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=512, num_heads=8):
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Linear projections
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # Linear projections and split into heads
        # (batch, seq_len, d_model) -> (batch, num_heads, seq_len, d_k)
        Q = self.W_Q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_K(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_V(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, V)

        # Concatenate heads and apply final linear
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.W_O(output)

        return output, attention_weights
```

---

## Positional Encoding

### Why Positional Encoding?

```
Problem: Self-attention is permutation invariant

"The cat sat on the mat"
"mat the on sat cat The"

Without position info, these look identical to attention!

Need to inject position information
```

### Sinusoidal Positional Encoding

```
Original Transformer uses sin/cos functions:

PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

where:
pos = position in sequence (0, 1, 2, ...)
i = dimension index (0, 1, 2, ..., d_model/2)

Properties:
✓ Unique encoding for each position
✓ Smooth changes between positions
✓ Can extrapolate to longer sequences
✓ Relative positions can be computed
```

### Example Calculation

```
Position 0, Dimension 0:
PE(0, 0) = sin(0 / 10000^0) = sin(0) = 0

Position 0, Dimension 1:
PE(0, 1) = cos(0 / 10000^0) = cos(0) = 1

Position 1, Dimension 0:
PE(1, 0) = sin(1 / 10000^0) = sin(1) ≈ 0.841

Position 10, Dimension 0:
PE(10, 0) = sin(10 / 10000^0) = sin(10) ≈ -0.544

Visualization (3 positions, 4 dimensions):

Pos 0: [0.00,  1.00,  0.00,  1.00]
Pos 1: [0.84,  0.54, -0.01,  1.00]
Pos 2: [0.91, -0.42, -0.01,  1.00]
```

### Learned Positional Embeddings

```
Alternative: Learn position embeddings

Position Embedding Matrix: (max_len × d_model)

Position 0: [0.12, -0.34, 0.56, ..., 0.78]
Position 1: [0.23, -0.12, 0.45, ..., 0.67]
Position 2: [0.34, -0.23, 0.34, ..., 0.56]
...

Learned during training
Used in BERT, GPT

Advantage: Adapts to data
Disadvantage: Fixed max length
```

### Adding to Embeddings

```
Final input representation:

Input = Token Embedding + Positional Encoding

Example:
Token "cat" embedding: [0.5, 0.3, 0.2, 0.8]
Position 2 encoding:   [0.91, -0.42, -0.01, 1.00]

Combined input:        [1.41, -0.12, 0.19, 1.80]

This combined vector is fed into transformer layers
```

---

## Feed-Forward Networks

### Architecture

```
Position-wise Feed-Forward Network (FFN):

Input (d_model = 512)
     ↓
Linear layer (d_ff = 2048)
     ↓
Activation (ReLU or GELU)
     ↓
Linear layer (d_model = 512)
     ↓
Output (d_model = 512)

"Position-wise": Applied independently to each position
Same network for all positions, but different instances
```

### Mathematical Definition

```
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2

or with GELU:

FFN(x) = GELU(xW_1 + b_1)W_2 + b_2

where:
W_1 ∈ ℝ^(d_model × d_ff)    (typically 512 × 2048)
b_1 ∈ ℝ^(d_ff)
W_2 ∈ ℝ^(d_ff × d_model)    (typically 2048 × 512)
b_2 ∈ ℝ^(d_model)

Parameters: 2 × d_model × d_ff
```

### Why FFN?

```
Purpose:
1. Add non-linearity (activation function)
2. Transform representations
3. Increase model capacity
4. Mix information within each position

Analogy:
Attention: Mix information across positions
FFN: Process each position independently

Together: Rich, contextualized representations
```

### Activation Functions

```
ReLU (Original Transformer):
ReLU(x) = max(0, x)

Pros: Simple, fast
Cons: Dead neurons (x < 0)

GELU (Modern transformers - BERT, GPT):
GELU(x) ≈ x × Φ(x)

where Φ(x) is Gaussian CDF

Pros: Smooth, probabilistic
Cons: Slightly slower

SwiGLU (Llama, PaLM):
SwiGLU(x) = Swish(xW) ⊙ xV

where Swish(x) = x × sigmoid(x)

Pros: Better performance
Cons: More parameters
```

### Implementation

```python
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.GELU()

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        x = self.linear1(x)        # (batch, seq_len, d_ff)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.linear2(x)        # (batch, seq_len, d_model)
        return x
```

---

## Layer Normalization

### Why Normalization?

```
Problem: Internal covariate shift
- Layer inputs change during training
- Makes optimization harder
- Slower convergence

Solution: Normalize layer inputs
- Stable gradients
- Faster training
- Better generalization
```

### Layer Norm vs Batch Norm

```
Batch Normalization:
Normalize across batch dimension
Mean/variance computed over batch

Layer Normalization:
Normalize across feature dimension
Mean/variance computed per sample

For transformers: Layer Norm preferred
- Works with variable sequence lengths
- No batch size dependency
- Better for NLP tasks
```

### Mathematical Definition

```
LayerNorm(x) = γ ⊙ (x - μ) / √(σ² + ε) + β

where:
μ = mean(x) across feature dimension
σ² = variance(x) across feature dimension
γ = learned scale parameter
β = learned shift parameter
ε = small constant for numerical stability (1e-6)

Example (d_model = 4):
x = [1.0, 2.0, 3.0, 4.0]
μ = 2.5
σ² = 1.25
σ = 1.118

Normalized: [-1.34, -0.45, 0.45, 1.34]

With learned γ = [1, 1, 1, 1], β = [0, 0, 0, 0]:
Output = [-1.34, -0.45, 0.45, 1.34]
```

### Residual Connections

```
Residual (Skip) Connection:

Input x
  ↓
┌──────────────┐
│ Sublayer     │
│ (Attention   │
│  or FFN)     │
└──────┬───────┘
       ↓
    output
       ↓
     Add x  ← Residual connection
       ↓
   LayerNorm
       ↓
Final output

Mathematically:
output = LayerNorm(x + Sublayer(x))

Benefits:
✓ Gradient flow (combat vanishing gradients)
✓ Easier optimization
✓ Enables very deep networks (100+ layers)
```

### Pre-Norm vs Post-Norm

```
Post-Norm (Original Transformer):
x → Sublayer → Add x → LayerNorm → output

Pre-Norm (Modern, e.g., GPT):
x → LayerNorm → Sublayer → Add x → output

Pre-Norm advantages:
✓ More stable training
✓ Can train deeper models
✓ Less sensitive to learning rate

Most modern LLMs use Pre-Norm
```

---

## Encoder-Decoder Structure

### Encoder

```
Purpose: Process input sequence into representations

Stack of N identical layers (N=6 in original):

Input Embeddings + Positional Encoding
     ↓
┌─────────────────────────────────┐
│ Layer 1:                        │
│  - Multi-Head Self-Attention    │
│  - Add & Norm                   │
│  - Feed-Forward                 │
│  - Add & Norm                   │
├─────────────────────────────────┤
│ Layer 2:                        │
│  - Multi-Head Self-Attention    │
│  - Add & Norm                   │
│  - Feed-Forward                 │
│  - Add & Norm                   │
├─────────────────────────────────┤
│ ...                             │
├─────────────────────────────────┤
│ Layer N                         │
└─────────────────────────────────┘
     ↓
Encoded Representations

All positions can attend to all other positions
Bidirectional understanding
```

### Decoder

```
Purpose: Generate output sequence autoregressively

Stack of N identical layers (N=6 in original):

Output Embeddings + Positional Encoding
     ↓
┌─────────────────────────────────┐
│ Layer 1:                        │
│  - Masked Multi-Head Attention  │ ← Self-attention (causal)
│  - Add & Norm                   │
│  - Cross-Attention to Encoder   │ ← Attend to input
│  - Add & Norm                   │
│  - Feed-Forward                 │
│  - Add & Norm                   │
├─────────────────────────────────┤
│ Layer 2:                        │
│  - Masked Multi-Head Attention  │
│  - Add & Norm                   │
│  - Cross-Attention to Encoder   │
│  - Add & Norm                   │
│  - Feed-Forward                 │
│  - Add & Norm                   │
├─────────────────────────────────┤
│ ...                             │
└─────────────────────────────────┘
     ↓
Linear + Softmax
     ↓
Output Probabilities
```

### Masked Self-Attention

```
Prevent attending to future positions:

Generating: "The cat sat on"
Next token prediction: Should only see "The cat sat on"
Cannot see future tokens

Attention Mask:

       The  cat  sat  on
The  [  ✓   ✗    ✗    ✗  ]
cat  [  ✓   ✓    ✗    ✗  ]
sat  [  ✓   ✓    ✓    ✗  ]
on   [  ✓   ✓    ✓    ✓  ]

✓ = Can attend (score computed)
✗ = Cannot attend (score = -∞ before softmax)

Implementation:
scores = scores.masked_fill(mask == 0, -1e9)
```

### Cross-Attention

```
Decoder attends to encoder outputs:

Query (Q): From decoder
Key (K): From encoder
Value (V): From encoder

Example (Translation):
Input (Encoder): "Le chat" (French)
Output (Decoder): "The cat" (English)

When generating "cat":
Decoder queries: "What's relevant in the input?"
Encoder keys: "Le", "chat"
Attention: High weight on "chat"
Retrieve: Information about "chat"

Allows decoder to focus on relevant input parts
```

---

## Attention Variants

### Causal (Masked) Attention

```
Used in: GPT, all decoder-only models

Masking pattern:
Lower triangular matrix

Position can only attend to:
- Itself
- Previous positions
- NOT future positions

Use case: Autoregressive generation
```

### Bidirectional Attention

```
Used in: BERT, encoder models

No masking
All positions attend to all positions

Use case:
- Understanding (not generation)
- Classification
- Named entity recognition
```

### Grouped Query Attention (GQA)

```
Used in: Llama 2

Optimization: Share K and V across multiple heads

Standard Multi-Head (8 heads):
8 × Q, 8 × K, 8 × V

GQA (8 heads, 2 groups):
8 × Q, 2 × K, 2 × V

Benefits:
✓ Fewer parameters
✓ Faster inference
✓ Less memory
✗ Slight quality trade-off

Example:
Heads 1-4 share K₁, V₁
Heads 5-8 share K₂, V₂
```

### Sparse Attention

```
Not all positions need full attention

Patterns:
1. Local attention: Attend to nearby tokens
2. Strided attention: Attend every k tokens
3. Global attention: Few tokens attend to all

Benefits:
✓ O(n√n) instead of O(n²)
✓ Longer sequences
✓ Reduced memory

Used in: Longformer, BigBird
```

### Flash Attention

```
Optimization: Fused GPU operations

Standard attention:
- Materialize full attention matrix (n²)
- Memory bottleneck

Flash Attention:
- Tile-based computation
- Never materialize full matrix
- IO-aware algorithm

Benefits:
✓ 2-4x faster
✓ 10-20x less memory
✓ Enables longer contexts

Used in: Modern training (Llama 2, GPT-4)
```

---

## Transformer Optimizations

### Positional Encoding Variants

```
1. Sinusoidal (Original):
   - Fixed formula
   - Extrapolates to any length

2. Learned Absolute:
   - One embedding per position
   - Limited to max length

3. Relative Positional (T5):
   - Encode relative distances
   - Better for variable lengths

4. RoPE (Rotary Position Embedding):
   - Used in Llama, PaLM
   - Rotate Q and K based on position
   - Excellent extrapolation

5. ALiBi (Attention with Linear Biases):
   - Add bias based on distance
   - Simple, effective
   - Used in BLOOM
```

### Normalization Variants

```
1. Post-Norm (Original):
   x → Sublayer → Add → Norm

2. Pre-Norm (Modern):
   x → Norm → Sublayer → Add
   More stable

3. RMSNorm (Llama):
   Simpler than LayerNorm
   Only normalize, no shift
   Faster

4. DeepNorm (DeepNet):
   For very deep models (1000+ layers)
   Modified residual connections
```

### Architecture Modifications

```
1. GLU Variants (Llama, PaLM):
   Replace FFN with gated versions
   SwiGLU: Better performance

2. Parallel Attention + FFN:
   Compute in parallel instead of sequential
   Faster training

3. Multi-Query Attention:
   Single K, V for all heads
   Faster inference

4. Sparse Layers:
   Not all layers are dense
   Mixture of experts
```

---

## Key Takeaways

### Core Mechanisms

1. **Self-attention** computes relationships between all positions in parallel

2. **QKV mechanism**: Query finds relevant Keys, retrieves Values

3. **Scaled dot-product**: Score = softmax(QK^T / √d_k) V

4. **Multi-head attention** learns different relationship patterns

5. **Positional encoding** injects sequence order information

### Architecture Components

6. **Feed-forward networks** transform representations at each position

7. **Layer normalization** stabilizes training and speeds convergence

8. **Residual connections** enable deep networks (100+ layers)

9. **Encoder** uses bidirectional attention for understanding

10. **Decoder** uses masked attention for autoregressive generation

### Attention Variants

11. **Causal masking** prevents attending to future tokens

12. **Cross-attention** connects encoder and decoder

13. **Grouped query attention** shares K/V across heads for efficiency

14. **Flash Attention** optimizes memory and speed with tiling

15. **Sparse attention** reduces O(n²) complexity for long sequences

### Modern Improvements

16. **Pre-norm** more stable than post-norm for deep models

17. **RoPE** (Rotary Position Embedding) better than sinusoidal

18. **RMSNorm** simpler and faster than LayerNorm

19. **SwiGLU** activation improves FFN performance

20. **Parallel attention + FFN** speeds up training

---

## Practice Questions

1. Explain the Query, Key, Value mechanism in self-attention.

2. Why is the attention score scaled by √d_k?

3. Calculate attention weights for a 3-word sentence (show work).

4. What is the purpose of multi-head attention? Why not use a single head?

5. Compare sinusoidal and learned positional encodings.

6. What is the difference between encoder and decoder self-attention?

7. Why are residual connections important in transformers?

8. Explain masked self-attention and when it's used.

9. What is cross-attention and how does it work?

10. Compare the complexity of self-attention (O(n²)) with RNNs (O(n)).

11. How does Flash Attention reduce memory usage?

12. Design a transformer for sentiment classification (specify components).

13. What are the advantages of pre-norm over post-norm?

14. Explain grouped query attention and its benefits.

15. How would you modify transformers for sequences longer than 100K tokens?

---

## Related Modules

- **[02: AI/ML Fundamentals](02.AI_ML_Fundamentals.md)** - Neural network basics
- **[03: Generative AI and LLMs](03.Generative_AI_and_LLM.md)** - LLMs built on transformers
- **[05: Model Selection](05.Model_Selection.md)** - Different transformer variants
- **[06: Model Customization](06.Model_Customization.md)** - Using transformers effectively
- **[07: Model Training](07.Model_Training.md)** - Training transformer models
- **[08: NVIDIA Ecosystem](08.NVIDIA_Ecosystem.md)** - Optimizing transformers on GPUs

---

**Next Module**: [05: Model Selection and Embeddings](05.Model_Selection.md)

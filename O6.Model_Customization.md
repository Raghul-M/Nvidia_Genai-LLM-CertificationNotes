# O6: Model Customization

## Table of Contents
- [Introduction to Model Customization](#introduction-to-model-customization)
- [Retrieval-Augmented Generation (RAG)](#retrieval-augmented-generation-rag)
- [RAG Implementation](#rag-implementation)
- [Advanced RAG Techniques](#advanced-rag-techniques)
- [Prompt Engineering](#prompt-engineering)
- [Prompt Patterns and Techniques](#prompt-patterns-and-techniques)
- [Advanced Prompting](#advanced-prompting)
- [Combining RAG and Prompting](#combining-rag-and-prompting)
- [Key Takeaways](#key-takeaways)
- [Practice Questions](#practice-questions)

---

## Introduction to Model Customization

### Why Customize Models?

```
Base LLMs have limitations:
❌ Knowledge cutoff date
❌ No domain-specific knowledge
❌ Cannot access private data
❌ Hallucinate facts
❌ Generic responses

Customization solutions:
✓ RAG: Add external knowledge
✓ Prompt engineering: Better instructions
✓ Fine-tuning: Adapt model weights
✓ Tool use: Extend capabilities

This module: RAG + Prompting (no training required!)
```

### RAG vs Fine-Tuning vs Prompting

```
Prompt Engineering:
Cost: Free
Time: Minutes
Knowledge: Only what's in prompt
Use: Task formatting, behavior control

RAG:
Cost: Low (vector DB + inference)
Time: Hours-days (setup)
Knowledge: External documents
Use: Domain knowledge, up-to-date info

Fine-Tuning:
Cost: Medium-high (training)
Time: Days-weeks
Knowledge: Baked into weights
Use: New tasks, specific style

Combination:
Prompting + RAG: Most common
RAG + Fine-tuning: Best performance
All three: Maximum customization
```

---

## Retrieval-Augmented Generation (RAG)

### What is RAG?

**RAG** combines retrieval of relevant documents with LLM generation.

```
Without RAG:
User: "What's our company's vacation policy?"
LLM: "I don't have access to your company's policies."
   ❌ Cannot answer without knowledge

With RAG:
User: "What's our company's vacation policy?"
  ↓
1. Retrieve relevant docs from company knowledge base
2. Pass docs + question to LLM
  ↓
LLM: "According to your employee handbook, you receive
      15 vacation days per year, accruing monthly..."
   ✓ Accurate answer grounded in company data
```

### RAG Architecture

```
┌──────────────────────────────────┐
│ Offline (One-time Setup)         │
├──────────────────────────────────┤
│ Documents                        │
│    ↓                             │
│ Chunking (split into pieces)     │
│    ↓                             │
│ Embeddings (vectorize)           │
│    ↓                             │
│ Vector Database (store)          │
└──────────────────────────────────┘

┌──────────────────────────────────┐
│ Online (Per Query)               │
├──────────────────────────────────┤
│ User Query                       │
│    ↓                             │
│ Embed Query                      │
│    ↓                             │
│ Retrieve Top-K Docs              │
│    ↓                             │
│ Construct Prompt                 │
│    ↓                             │
│ LLM Generation                   │
│    ↓                             │
│ Response                         │
└──────────────────────────────────┘
```

### Benefits of RAG

```
✓ Up-to-date information:
  Add new docs without retraining

✓ Source attribution:
  "According to [Document X]..."
  Verifiable answers

✓ Domain-specific knowledge:
  Company docs, technical manuals
  Private information

✓ Reduced hallucinations:
  Grounded in retrieved facts
  "I don't know" when no relevant docs

✓ Cost-effective:
  No training required
  Update knowledge easily

✓ Explainable:
  Show which documents used
  Audit trail
```

---

## RAG Implementation

### Step 1: Document Processing

```python
# Load documents
from langchain.document_loaders import TextLoader, PyPDFLoader

# Text files
loader = TextLoader("company_handbook.txt")
documents = loader.load()

# PDFs
pdf_loader = PyPDFLoader("employee_guide.pdf")
pdf_docs = pdf_loader.load()

# Web pages
from langchain.document_loaders import WebBaseLoader
web_loader = WebBaseLoader("https://company.com/policies")
web_docs = web_loader.load()
```

### Step 2: Text Chunking

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Create text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # Max characters per chunk
    chunk_overlap=200,      # Overlap between chunks
    length_function=len,
    separators=["\n\n", "\n", " ", ""]
)

# Split documents
chunks = text_splitter.split_documents(documents)

print(f"Split {len(documents)} documents into {len(chunks)} chunks")

# Example chunk:
# "Employee Benefits
#
#  All full-time employees receive:
#  - 15 vacation days per year
#  - 10 sick days per year
#  - Health insurance
#  ..."
```

**Chunking Strategies**:

```
Fixed-size chunking:
- Simple: Split every N characters
- Problem: May break mid-sentence

Sentence-based:
- Split on sentence boundaries
- Better coherence
- Variable chunk sizes

Semantic chunking:
- Split on topic changes
- Most coherent
- Computationally expensive

Recursive:
- Try \n\n, then \n, then space
- LangChain default
- Good balance

Chunk size trade-offs:
Small (200-500):
  ✓ Precise retrieval
  ✗ Less context

Medium (500-1000):
  ✓ Good balance
  ✓ Most common

Large (1000-2000):
  ✓ More context
  ✗ May include irrelevant info
```

### Step 3: Generate Embeddings

```python
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings

# Option 1: OpenAI embeddings
embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002"
)

# Option 2: Open-source embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"
)

# Generate embeddings for chunks
chunk_embeddings = embeddings.embed_documents([
    chunk.page_content for chunk in chunks
])

# Each chunk → 768-dim vector (for mpnet)
# "Employee Benefits..." → [0.1, -0.3, 0.5, ..., 0.2]
```

### Step 4: Store in Vector Database

```python
from langchain.vectorstores import Chroma, Pinecone, Qdrant

# Option 1: ChromaDB (simple, embedded)
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# Option 2: Pinecone (managed cloud)
import pinecone
pinecone.init(api_key="your-key", environment="us-west1-gcp")

vectorstore = Pinecone.from_documents(
    documents=chunks,
    embedding=embeddings,
    index_name="company-docs"
)

# Option 3: Qdrant (production-grade)
from qdrant_client import QdrantClient

client = QdrantClient(host="localhost", port=6333)

vectorstore = Qdrant.from_documents(
    documents=chunks,
    embedding=embeddings,
    url="http://localhost:6333",
    collection_name="company_knowledge"
)
```

### Step 5: Retrieval

```python
# Simple similarity search
query = "What is the vacation policy?"
relevant_docs = vectorstore.similarity_search(query, k=3)

for i, doc in enumerate(relevant_docs):
    print(f"Document {i+1}:")
    print(doc.page_content)
    print(f"Source: {doc.metadata}")
    print("---")

# Output:
# Document 1:
# "All full-time employees receive 15 vacation days per year..."
# Source: {'source': 'handbook.txt', 'page': 12}
# ---
# Document 2:
# "Vacation days accrue monthly at a rate of 1.25 days..."
# Source: {'source': 'handbook.txt', 'page': 13}
# ---
```

**Retrieval Methods**:

```python
# 1. Similarity search (cosine similarity)
docs = vectorstore.similarity_search(query, k=3)

# 2. MMR (Maximum Marginal Relevance) - diverse results
docs = vectorstore.max_marginal_relevance_search(
    query,
    k=3,
    fetch_k=10,      # Fetch 10, return diverse 3
    lambda_mult=0.5  # 0=diversity, 1=similarity
)

# 3. Similarity with score threshold
docs = vectorstore.similarity_search_with_score(query, k=5)
filtered = [doc for doc, score in docs if score > 0.7]

# 4. Metadata filtering
docs = vectorstore.similarity_search(
    query,
    k=3,
    filter={"department": "HR"}
)
```

### Step 6: Prompt Construction

```python
# Build context from retrieved documents
context = "\n\n".join([doc.page_content for doc in relevant_docs])

# Construct prompt
prompt = f"""Answer the question based on the context below. If the question cannot be answered using the information provided, say "I don't have enough information to answer that."

Context:
{context}

Question: {query}

Answer:"""

print(prompt)

# Output:
# Answer the question based on the context below...
#
# Context:
# All full-time employees receive 15 vacation days per year...
#
# Vacation days accrue monthly at a rate of 1.25 days...
#
# Question: What is the vacation policy?
#
# Answer:
```

### Step 7: LLM Generation

```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# Option 1: Completion model
llm = OpenAI(temperature=0)
answer = llm(prompt)

# Option 2: Chat model
chat_model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

messages = [
    {"role": "system", "content": "You are a helpful assistant that answers questions based on provided context."},
    {"role": "user", "content": prompt}
]

answer = chat_model(messages)

print(answer)
# "Based on the employee handbook, full-time employees receive 15 vacation days per year, which accrue monthly at a rate of 1.25 days per month."
```

### Complete RAG Pipeline

```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# 1. Setup (one-time)
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

# 2. Create retrieval chain
llm = OpenAI(temperature=0)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",              # "stuff" = put all docs in context
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# 3. Query
result = qa_chain({"query": "What is the vacation policy?"})

print("Answer:", result["result"])
print("\nSources:")
for doc in result["source_documents"]:
    print(f"- {doc.metadata['source']}, page {doc.metadata.get('page', 'N/A')}")

# Answer: Full-time employees receive 15 vacation days per year...
#
# Sources:
# - handbook.txt, page 12
# - handbook.txt, page 13
```

---

## Advanced RAG Techniques

### 1. Hybrid Search

```python
# Combine dense (vector) + sparse (BM25) retrieval

from langchain.retrievers import BM25Retriever, EnsembleRetriever

# Dense retriever (vector similarity)
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Sparse retriever (keyword matching)
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 5

# Ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.7, 0.3]  # 70% vector, 30% keyword
)

# Use in chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=ensemble_retriever
)

# Benefits:
# ✓ Handles exact keyword matches (names, codes)
# ✓ Semantic understanding (vector)
# ✓ More robust
```

### 2. Re-ranking

```python
# Retrieve more docs, then re-rank for relevance

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# Base retriever (get 10 docs)
base_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# Compressor (LLM re-ranks and extracts relevant parts)
compressor = LLMChainExtractor.from_llm(llm)

# Compressed retriever
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

# Use in chain - higher quality, slower
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=compression_retriever
)

# Process:
# 1. Retrieve 10 docs (fast, approximate)
# 2. LLM scores each for relevance
# 3. Keep top 3 most relevant
# 4. Extract only relevant sentences
```

### 3. Parent Document Retrieval

```python
# Store small chunks for retrieval, large chunks for context

from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore

# Small chunks for retrieval (precise matching)
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

# Large chunks for context (complete information)
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)

# Storage for parent documents
store = InMemoryStore()

# Parent document retriever
retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter
)

# Add documents
retriever.add_documents(documents)

# Retrieval:
# 1. Embed query
# 2. Find similar small chunks
# 3. Return full parent documents
# Benefits: Precision + context
```

### 4. HyDE (Hypothetical Document Embeddings)

```python
# Generate hypothetical answer, use it for retrieval

from langchain.chains import HypotheticalDocumentEmbedder

# Create HyDE chain
hyde_embeddings = HypotheticalDocumentEmbedder.from_llm(
    llm=llm,
    base_embeddings=embeddings,
    prompt_key="web_search"
)

# Create vectorstore with HyDE
hyde_vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=hyde_embeddings
)

# How it works:
# User: "What is the vacation policy?"
# ↓
# LLM generates hypothetical answer:
# "The vacation policy states that employees receive X days..."
# ↓
# Embed hypothetical answer
# ↓
# Retrieve docs similar to hypothetical answer
# ↓
# More relevant results!
```

### 5. Query Transformation

```python
# Transform query for better retrieval

# Multi-query: Generate multiple versions
from langchain.retrievers.multi_query import MultiQueryRetriever

multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=llm
)

# Query: "What's the vacation policy?"
# Generates:
# - "How many vacation days do employees get?"
# - "What is the PTO policy?"
# - "Employee vacation time allocation"
# Retrieves for all, combines results

# Step-back: Ask broader question first
# Query: "How does photosynthesis work in tropical plants?"
# Step-back: "What is photosynthesis?"
# Retrieve using broader query → more comprehensive context
```

---

## Prompt Engineering

### What is Prompt Engineering?

**Prompt engineering** is the art and science of crafting effective instructions for LLMs.

```
Bad prompt:
"Summarize"

Better prompt:
"Summarize the following article in 3 bullet points,
focusing on the main findings:"

Best prompt:
"You are a research assistant. Summarize the article below
in exactly 3 concise bullet points. Focus on:
1. Main hypothesis
2. Methodology
3. Key findings

Article:
[text]

Summary:"

Same model, vastly different outputs!
```

### Prompt Components

```
Effective prompt structure:

1. Role/Persona:
   "You are an expert Python developer..."

2. Context:
   "The user is a beginner learning programming..."

3. Task:
   "Explain how list comprehensions work..."

4. Format:
   "Use simple language and provide 2 examples..."

5. Constraints:
   "Keep the explanation under 200 words..."

6. Examples (few-shot):
   "Example 1: [input] → [output]
    Example 2: [input] → [output]
    Now you try: [new input]"
```

### Zero-Shot Prompting

```python
# No examples, just instruction

prompt = """Classify the sentiment of the following review as positive, negative, or neutral.

Review: The product works well but the shipping was slow.

Sentiment:"""

# Model: "Neutral"

# When to use:
# ✓ Simple tasks
# ✓ Well-defined problems
# ✓ Model has relevant training
```

### Few-Shot Prompting

```python
# Provide examples

prompt = """Classify the sentiment as positive, negative, or neutral.

Review: This is amazing! Best purchase ever.
Sentiment: Positive

Review: Terrible quality, broke immediately.
Sentiment: Negative

Review: It's okay, nothing special.
Sentiment: Neutral

Review: Great features but expensive.
Sentiment:"""

# Model: "Neutral" (more accurate with examples)

# When to use:
# ✓ Complex tasks
# ✓ Specific format needed
# ✓ Ambiguous instructions
# ✓ Domain-specific tasks

# Few-shot tips:
# - 2-5 examples usually enough
# - Diverse examples (cover edge cases)
# - Clear, consistent format
```

---

## Prompt Patterns and Techniques

### 1. Chain-of-Thought (CoT)

```python
# Encourage step-by-step reasoning

# Without CoT:
prompt = "What is 25% of 80?"
# Model: "20" ✓ (but might guess)

# With CoT:
prompt = """What is 25% of 80?
Let's think step by step:"""

# Model:
# "1. 25% means 25/100 or 0.25
#  2. We need to multiply 80 by 0.25
#  3. 80 × 0.25 = 20
#  Therefore, 25% of 80 is 20." ✓ (shows reasoning)

# Automatic CoT:
prompt = """Question: What is 25% of 80?
Let's approach this step-by-step:
1)"""

# Works especially well for:
# - Math problems
# - Logic puzzles
# - Complex reasoning
```

### 2. Self-Consistency

```python
# Generate multiple CoT paths, vote on answer

prompt = "What is 25% of 80? Let's think step by step:"

# Generate 5 answers with temperature > 0
answers = []
for i in range(5):
    response = llm(prompt, temperature=0.7)
    # Extract final answer
    answer = extract_answer(response)
    answers.append(answer)

# Vote
from collections import Counter
final_answer = Counter(answers).most_common(1)[0][0]

# answers = [20, 20, 20, 21, 20]
# final_answer = 20 (majority vote)

# Improves accuracy on complex reasoning
```

### 3. Tree of Thoughts

```python
# Explore multiple reasoning paths

# Problem: "Use 4, 5, 6, 7 to make 24"

# Path 1:
# (7 - 5) × (6 + 4) = 2 × 10 = 20 ✗

# Path 2:
# (7 - 4) × (6 - 5) = 3 × 1 = 3 ✗

# Path 3:
# (7 + 5) × (6 - 4) = 12 × 2 = 24 ✓

# Tree of Thoughts:
# 1. Generate multiple first steps
# 2. Evaluate each
# 3. Keep promising ones
# 4. Repeat
# 5. Backtrack if stuck

# More systematic than CoT
```

### 4. ReAct (Reasoning + Acting)

```python
# Interleave reasoning and actions (tool use)

prompt = """Answer the question using tools as needed.

Tools:
- Calculator: For math
- Wikipedia: For facts
- Search: For current info

Format:
Thought: [reasoning]
Action: [tool name]
Action Input: [input]
Observation: [result]
... (repeat as needed)
Final Answer: [answer]

Question: What is the population of France's capital?

Let's solve this step by step:

Thought: I need to find France's capital first.
Action: Wikipedia
Action Input: Capital of France
Observation: The capital of France is Paris.

Thought: Now I need the population of Paris.
Action: Wikipedia
Action Input: Population of Paris
Observation: Paris has a population of 2.2 million (city proper).

Final Answer: The population of Paris, France's capital, is approximately 2.2 million."""

# Enables:
# - Tool use
# - Multi-step reasoning
# - Verifiable facts
```

### 5. Self-Critique

```python
# Generate answer, then critique it

# Step 1: Generate
prompt = "Explain photosynthesis."
answer = llm(prompt)

# Step 2: Critique
critique_prompt = f"""Review the following explanation for accuracy and clarity.
Identify any issues:

{answer}

Critique:"""
critique = llm(critique_prompt)

# Step 3: Improve
improve_prompt = f"""Original answer:
{answer}

Issues identified:
{critique}

Provide an improved answer:"""
final_answer = llm(improve_prompt)

# Results in higher quality outputs
```

### 6. Role Prompting

```python
# Assign specific role/persona

# Generic:
prompt = "Explain quantum computing."

# With role:
prompt = """You are a quantum physics professor explaining to undergraduate students.

Explain quantum computing in a way that's accurate but accessible.
Use analogies where helpful."""

# More focused, appropriate level

# Roles:
# - Expert: Detailed, technical
# - Teacher: Clear, patient
# - Critic: Analytical, skeptical
# - Creative writer: Engaging, narrative
# - Consultant: Practical, actionable
```

### 7. Format Specification

```python
# Specify exact output format

# Structured JSON:
prompt = """Extract information from the text and output as JSON.

Text: "John Smith, age 30, lives in New York and works as a software engineer."

Output format:
{
  "name": "",
  "age": ,
  "location": "",
  "occupation": ""
}

Output:"""

# Model:
# {
#   "name": "John Smith",
#   "age": 30,
#   "location": "New York",
#   "occupation": "software engineer"
# }

# Other formats:
# - Markdown tables
# - Bullet lists
# - Code blocks
# - CSV

# Specify format clearly for consistent parsing
```

---

## Advanced Prompting

### Instruction Fine-Tuning Principles

```
Good instructions:

1. Be specific:
   ❌ "Translate this"
   ✓ "Translate the following English text to French.
      Maintain formal tone:"

2. Provide context:
   ❌ "Summarize"
   ✓ "You are summarizing a research paper for a grant proposal.
      Emphasize methodology and results:"

3. Specify constraints:
   ❌ "List benefits"
   ✓ "List exactly 3 key benefits, each in one sentence,
      starting with a verb:"

4. Give examples:
   Show desired input/output format

5. Iterate:
   Test prompts, refine based on outputs
```

### System Messages (Chat Models)

```python
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "system",
            "content": """You are a helpful coding assistant.
                         - Provide Python code with comments
                         - Explain your reasoning
                         - Suggest best practices
                         - Point out potential issues"""
        },
        {
            "role": "user",
            "content": "How do I read a CSV file?"
        }
    ]
)

# System message:
# - Sets overall behavior
# - Applied to all user messages
# - User cannot override (in theory)

# User message:
# - Specific request
# - Changes per interaction
```

### Temperature and Sampling

```python
# Control randomness

# temperature=0 (deterministic, focused)
response = llm("Write a product description", temperature=0)
# Consistent, safe, potentially boring

# temperature=0.7 (balanced)
response = llm("Write a product description", temperature=0.7)
# Creative but controlled

# temperature=1.5 (very creative)
response = llm("Write a product description", temperature=1.5)
# Diverse, potentially off-topic

# Use cases:
# temperature=0: Facts, code, consistency needed
# temperature=0.7: Creative writing, varied responses
# temperature>1: Brainstorming, exploration

# Top-p (nucleus sampling):
# top_p=0.1: Very focused
# top_p=0.9: Balanced (common default)
# top_p=1.0: Full distribution
```

### Prompt Optimization

```python
# A/B testing prompts

prompts = [
    "Summarize the article:",
    "Provide a brief summary:",
    "What are the main points?",
    "Create a concise summary with key takeaways:"
]

# Test each on validation set
for prompt_template in prompts:
    scores = []
    for example in validation_set:
        prompt = f"{prompt_template}\n\n{example.text}"
        summary = llm(prompt)
        score = evaluate(summary, example.reference)
        scores.append(score)

    avg_score = sum(scores) / len(scores)
    print(f"{prompt_template}: {avg_score:.3f}")

# Choose best-performing prompt

# Metrics:
# - ROUGE (for summarization)
# - Exact match (for QA)
# - Human evaluation
# - Task-specific
```

---

## Combining RAG and Prompting

### RAG + Advanced Prompting

```python
# Combine retrieval with prompt engineering

from langchain.prompts import PromptTemplate

# Custom prompt template
template = """You are an AI assistant helping employees understand company policies.

Use the following policy documents to answer the question. If you cannot find the answer in the documents, say "I don't have information about that in the company policies."

Policy Documents:
{context}

Question: {question}

Instructions:
1. Answer based only on the provided documents
2. Quote relevant sections when appropriate
3. If multiple policies apply, mention all
4. Keep your answer concise but complete

Answer:"""

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=template
)

# Use with chain
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    chain_type_kwargs={"prompt": prompt}
)

# Result: Better answers, proper citations, fewer hallucinations
```

### Conversational RAG

```python
# RAG with conversation history

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# Memory to track conversation
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# Conversational chain
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    memory=memory,
    return_source_documents=True
)

# Conversation:
result1 = qa_chain({"question": "What's the vacation policy?"})
print(result1["answer"])
# "Employees receive 15 vacation days per year..."

result2 = qa_chain({"question": "When do they accrue?"})  # "they" refers to vacation days
print(result2["answer"])
# "Vacation days accrue monthly at 1.25 days per month."

# Chain maintains context across turns
```

### Citations and Source Attribution

```python
# Include source citations in answer

citation_template = """Answer the question based on the context below.
Include citations in your answer using [Source N] format.

Context:
{context}

Question: {question}

Answer with citations:"""

# Custom chain with citations
class CitationQA:
    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm

    def query(self, question):
        # Retrieve documents
        docs = self.vectorstore.similarity_search(question, k=3)

        # Number documents
        context_parts = []
        for i, doc in enumerate(docs, 1):
            context_parts.append(f"[Source {i}]:\n{doc.page_content}")

        context = "\n\n".join(context_parts)

        # Generate answer
        prompt = citation_template.format(
            context=context,
            question=question
        )
        answer = self.llm(prompt)

        # Return answer and sources
        sources = [
            {"source": doc.metadata.get("source", "Unknown"),
             "page": doc.metadata.get("page", "N/A")}
            for doc in docs
        ]

        return {
            "answer": answer,
            "sources": sources
        }

# Usage
qa = CitationQA(vectorstore, llm)
result = qa.query("What is the vacation policy?")

print(result["answer"])
# "According to [Source 1], employees receive 15 vacation days per year.
#  [Source 2] states that these accrue monthly at 1.25 days per month."

print("\nSources:")
for i, source in enumerate(result["sources"], 1):
    print(f"[Source {i}]: {source['source']}, page {source['page']}")
```

---

## Key Takeaways

### RAG Fundamentals

1. **RAG** combines retrieval with generation for up-to-date, grounded answers

2. **Chunking** splits documents into pieces (500-1000 chars typical)

3. **Embeddings** convert text to vectors for similarity search

4. **Vector databases** (ChromaDB, Pinecone) enable fast retrieval

5. **Retrieval** finds top-k most relevant chunks

### Advanced RAG

6. **Hybrid search** combines vector + keyword search

7. **Re-ranking** improves relevance after initial retrieval

8. **Parent documents** balance retrieval precision with context

9. **HyDE** generates hypothetical answers for better retrieval

10. **Query transformation** creates multiple query versions

### Prompt Engineering Basics

11. **Zero-shot** works for simple, well-defined tasks

12. **Few-shot** provides examples for better results

13. **Clear instructions** specify role, task, format, constraints

14. **Temperature** controls randomness (0=deterministic, 0.7=balanced)

15. **System messages** set overall behavior

### Advanced Prompting

16. **Chain-of-thought** enables step-by-step reasoning

17. **Self-consistency** uses majority voting across multiple answers

18. **ReAct** interleaves reasoning with tool use

19. **Self-critique** improves outputs through iteration

20. **RAG + Prompting** combines external knowledge with effective instructions

---

## Practice Questions

1. Explain the RAG pipeline from documents to answer.

2. What chunk size would you use for a legal document database? Why?

3. Compare ChromaDB and Pinecone for a production RAG system.

4. How does hybrid search improve over pure vector search?

5. Design a RAG system for customer support (specify all components).

6. Write a few-shot prompt for classifying customer feedback sentiment.

7. What is chain-of-thought prompting and when is it useful?

8. How would you add citations to RAG responses?

9. Compare zero-shot vs few-shot prompting with examples.

10. Design a prompt template for a coding assistant.

11. How does temperature affect LLM outputs? Give examples.

12. What is the difference between RAG and fine-tuning?

13. Implement a conversational RAG system (describe the components).

14. How would you optimize prompts for a specific task?

15. Combine RAG with chain-of-thought prompting (show example).

---

## Related Modules

- **[O3: Generative AI and LLMs](O3.Generative_AI_and_LLM.md)** - Understanding LLM capabilities
- **[O5: Model Selection](O5.Model_Selection.md)** - Embeddings and vector databases
- **[O7: Model Training](O7.Model_Training.md)** - Fine-tuning as alternative to RAG
- **[O8: NVIDIA Ecosystem](O8.NVIDIA_Ecosystem.md)** - NeMo for RAG deployment
- **[O9: Ethical AI](O9.Ethical_AI.md)** - Reducing hallucinations with RAG

---

**Next Module**: [O7: Model Training and Fine-Tuning](O7.Model_Training.md)

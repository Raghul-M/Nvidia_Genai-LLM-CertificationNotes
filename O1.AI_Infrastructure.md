# AI Infrastructure



## Table of Contents
- [Introduction to AI Infrastructure](#introduction-to-ai-infrastructure)
- [GPU Architecture](#gpu-architecture)
- [NVIDIA GPU Lineup](#nvidia-gpu-lineup)
- [Grace CPU Architecture](#grace-cpu-architecture)
- [DGX Systems](#dgx-systems)
- [Memory Hierarchy](#memory-hierarchy)
- [Distributed Training Infrastructure](#distributed-training-infrastructure)
- [Data Center Considerations](#data-center-considerations)
- [Cloud Infrastructure](#cloud-infrastructure)
- [Key Takeaways](#key-takeaways)
- [Practice Questions](#practice-questions)

---

## Introduction to AI Infrastructure

<p align="center">
<img width="544" height="231" alt="image" src="https://github.com/user-attachments/assets/0ffc889a-049e-4031-bdbf-868754277cfb" />
</p>

### Why Specialized Hardware for AI?

```
Traditional CPU:
- Optimized for sequential processing
- Complex control logic
- Few powerful cores (8-64)
- Low arithmetic throughput

AI Workloads:
- Massive parallelism (matrix operations)
- Simple operations repeated billions of times
- High memory bandwidth needs
- Floating-point intensive

Solution: GPUs optimized for AI
```

### Hardware Evolution for AI

```
2012: AlexNet on 2 GPUs (ImageNet breakthrough)
2017: Transformer introduced (attention mechanisms)
2020: GPT-3 (175B params) trained on thousands of GPUs
2023: GPT-4, Llama 2, Claude - require massive infrastructure
2024: Training 1T+ parameter models
```

### AI Compute Requirements

```
Model Size vs Compute:

Small Model (BERT-Base, 110M params):
- Training: 1-8 GPUs, hours-days
- Inference: CPU or single GPU

Medium Model (GPT-2, 1.5B params):
- Training: 8-64 GPUs, days-weeks
- Inference: Single GPU

Large Model (GPT-3, 175B params):
- Training: 1000s of GPUs, weeks-months
- Inference: Multiple GPUs

Frontier Model (GPT-4, 1T+ params estimated):
- Training: 10,000+ GPUs/TPUs, months
- Inference: Multi-GPU clusters
```

---

## GPU Architecture

<p align="center">
<img width="1576" height="770" alt="image" src="https://github.com/user-attachments/assets/174b9d39-085d-4ea3-8cb4-7b221f958eea" />
</p>

### CPU vs GPU

```
CPU Design:
┌──────────┐
│  Core 1  │ (Complex, powerful)
├──────────┤
│  Core 2  │
├──────────┤
│   ...    │
├──────────┤
│  Core N  │
└──────────┘
Few cores (~64 max)
High per-core performance
Complex control logic

GPU Design:
┌─┬─┬─┬─┬─┬─┬─┬─┐
│C│C│C│C│C│C│C│C│
├─┼─┼─┼─┼─┼─┼─┼─┤
│C│C│C│C│C│C│C│C│
├─┼─┼─┼─┼─┼─┼─┼─┤
│ ... thousands...│
└─┴─┴─┴─┴─┴─┴─┴─┘
Thousands of cores
Lower per-core performance
Optimized for parallel math
```

### GPU Components

#### 1. Streaming Multiprocessors (SMs)

```
GPU = Multiple SMs
Each SM contains:
- CUDA cores (general compute)
- Tensor Cores (AI accelerators)
- Shared memory (fast local cache)
- Register file
- Warp schedulers

H100 GPU:
- 132 SMs
- 128 CUDA cores per SM
- Total: 16,896 CUDA cores
```

#### 2. CUDA Cores

```
Standard floating-point and integer operations:
- FP32 (32-bit float)
- FP64 (64-bit float)
- INT32 (32-bit integer)

Used for:
- General computation
- Control flow
- Data movement
```

#### 3. Tensor Cores

```
Specialized matrix multiply-accumulate units:

Operation: D = A × B + C
Where A, B, C, D are matrices

Speed: 8-10x faster than CUDA cores for matrix operations

Supported precisions:
- FP64 (64-bit float)
- TF32 (19-bit TensorFloat)
- FP16 (16-bit float)
- BF16 (16-bit bfloat)
- FP8 (8-bit float) - H100
- INT8 (8-bit integer)

AI workloads:
✓ Matrix multiplication (linear layers)
✓ Convolutions
✓ Attention mechanisms
```

#### 4. Memory Hierarchy

```
Fastest (Smallest):
┌──────────────────┐
│ Registers        │ ~256 KB per SM
├──────────────────┤
│ Shared Memory    │ ~100-200 KB per SM
├──────────────────┤
│ L1 Cache         │ ~128 KB per SM
├──────────────────┤
│ L2 Cache         │ 50-60 MB (whole GPU)
├──────────────────┤
│ HBM (VRAM)       │ 40-80 GB
└──────────────────┘
Slowest (Largest)

Access Times:
Registers: 1 cycle
Shared Memory: ~5 cycles
L1 Cache: ~30 cycles
L2 Cache: ~200 cycles
HBM: ~300-600 cycles
```

### NVIDIA GPU Architectures

```
Architecture Evolution:

Volta (2017):
- V100 GPU
- 1st gen Tensor Cores
- 32GB/16GB HBM2

Ampere (2020):
- A100 GPU
- 3rd gen Tensor Cores
- 80GB/40GB HBM2e
- TF32 precision
- Structural sparsity
- Multi-Instance GPU (MIG)

Hopper (2022):
- H100 GPU
- 4th gen Tensor Cores
- 80GB HBM3
- FP8 precision
- Transformer Engine
- DPX instructions (dynamic programming)
- Confidential computing

Ada Lovelace (2022):
- L40S GPU
- 4th gen Tensor Cores
- 48GB GDDR6
- DLSS 3
- Ray tracing
```

---

## NVIDIA GPU Lineup

### A100 (Ampere)

```
Specifications:
- Architecture: Ampere
- CUDA Cores: 6,912
- Tensor Cores: 432 (3rd gen)
- Memory: 40GB or 80GB HBM2e
- Memory Bandwidth: 1.6 TB/s (40GB) or 2.0 TB/s (80GB)
- TF32: 156 TFLOPS
- FP16: 312 TFLOPS
- Power: 400W

Key Features:
✓ Multi-Instance GPU (MIG): Partition into 7 instances
✓ 3rd gen NVLink: 600 GB/s
✓ PCIe Gen 4
✓ Structural sparsity (2:4 sparsity)

Use Cases:
- Training: Medium to large models (7B-70B)
- Inference: High-throughput serving
- Multi-tenant environments (MIG)
- Research and development

Pricing:
- Cloud: ~$2.50/hour
- Purchase: ~$15,000
```

### H100 (Hopper)

```
Specifications:
- Architecture: Hopper
- CUDA Cores: 16,896
- Tensor Cores: 528 (4th gen)
- Memory: 80GB HBM3
- Memory Bandwidth: 3.35 TB/s
- TF32: 989 TFLOPS
- FP16: 1,979 TFLOPS
- FP8: 3,958 TFLOPS
- Power: 700W

Key Features:
✓ FP8 Tensor Cores (2x throughput vs A100)
✓ Transformer Engine (automatic FP8 conversion)
✓ 4th gen NVLink: 900 GB/s
✓ PCIe Gen 5
✓ Confidential computing
✓ DPX instructions

Use Cases:
- Training: Largest models (100B-1T+)
- Inference: Fastest performance
- Recommendation systems
- Large language models

Performance vs A100:
- Training: 3x faster (FP8)
- Inference: 4-6x faster
- Memory bandwidth: 1.7x higher

Pricing:
- Cloud: ~$5/hour
- Purchase: ~$30,000-$40,000
```

### L40S (Ada Lovelace)

```
Specifications:
- Architecture: Ada Lovelace
- CUDA Cores: 18,176
- Tensor Cores: 568 (4th gen)
- RT Cores: 142 (3rd gen)
- Memory: 48GB GDDR6
- Memory Bandwidth: 864 GB/s
- TF32: 362 TFLOPS
- FP8: 733 TFLOPS
- Power: 350W

Key Features:
✓ Ray tracing hardware
✓ NVENC/NVDEC (video encoding/decoding)
✓ DLSS 3.0
✓ Cost-effective
✓ Balanced compute and graphics

Use Cases:
- Inference: Cost-effective deployment
- Graphics + AI hybrid workloads
- Content creation with AI
- Virtual workstations
- Rendering farms

vs A100:
- Lower memory bandwidth
- Less HBM capacity
- Better price/performance for inference
- Graphics capabilities

Pricing:
- Cloud: ~$1.50/hour
- Purchase: ~$10,000
```

### GPU Comparison Table

| GPU | Memory | TF32 TFLOPS | FP8 TFLOPS | Bandwidth | Use Case |
|-----|--------|-------------|------------|-----------|----------|
| **A100** | 40/80 GB | 156 | - | 2.0 TB/s | Training & inference |
| **H100** | 80 GB | 989 | 3,958 | 3.35 TB/s | Largest models |
| **L40S** | 48 GB | 362 | 733 | 864 GB/s | Cost-effective inference |

### Multi-Instance GPU (MIG)

```
Partition single GPU into isolated instances:

A100 (80GB) MIG Configurations:

7 × 1g.10gb:
┌────┐┌────┐┌────┐┌────┐┌────┐┌────┐┌────┐
│10GB││10GB││10GB││10GB││10GB││10GB││10GB│
└────┘└────┘└────┘└────┘└────┘└────┘└────┘

3 × 2g.20gb:
┌──────────┐┌──────────┐┌──────────┐
│   20GB   ││   20GB   ││   20GB   │
└──────────┘└──────────┘└──────────┘

1 × 3g.40gb + 1 × 1g.10gb:
┌────────────────────┐┌────┐
│       40GB         ││10GB│
└────────────────────┘└────┘

Benefits:
✓ Quality of Service (QoS)
✓ Resource isolation
✓ Better utilization
✓ Multi-user environments

Use Cases:
- Shared inference servers
- Jupyter notebook environments
- Development/testing
- CI/CD pipelines
```

---

## Grace CPU Architecture

### What is Grace?

**NVIDIA Grace** is ARM-based CPU designed for AI and HPC workloads.

```
Key Features:

Architecture:
- ARM Neoverse V2 cores
- 72 cores per CPU
- SPECrate®2017_int_base: Industry-leading

Memory:
- LPDDR5X memory
- 480 GB/s bandwidth
- Up to 960 GB capacity
- ECC protection

Connectivity:
- NVLink-C2C to GPU (900 GB/s)
- PCIe Gen 5
- CXL support

Power Efficiency:
- 2x energy efficiency vs x86
- Lower TCO (Total Cost of Ownership)
```

### Grace Hopper Superchip

```
Grace CPU + H100 GPU in single package:

┌────────────────┐
│  Grace CPU     │ (72 ARM cores)
│  480 GB/s      │
└────────┬───────┘
         │
    NVLink-C2C (900 GB/s)
         │
┌────────┴───────┐
│  H100 GPU      │ (80GB HBM3)
│  3.35 TB/s     │
└────────────────┘

Benefits:
✓ 7x bandwidth vs PCIe Gen 5
✓ Unified memory address space
✓ Lower latency CPU-GPU transfers
✓ Better for memory-intensive workloads

Use Cases:
- Large language models
- Recommender systems
- Graph analytics
- Scientific computing
```

### Grace-Grace Superchip

```
Two Grace CPUs connected:

┌────────────────┐
│  Grace CPU 0   │
└────────┬───────┘
         │
    NVLink-C2C (900 GB/s)
         │
┌────────┴───────┐
│  Grace CPU 1   │
└────────────────┘

Total:
- 144 ARM cores
- Up to 960 GB memory
- Exceptional memory bandwidth

Use Cases:
- Large-scale data analytics
- In-memory databases
- Simulation
- CPU-bound HPC
```

---

## DGX Systems

### NVIDIA DGX Platform

**DGX** is NVIDIA's integrated AI infrastructure platform.

```
Components:
- Multiple high-end GPUs
- High-speed NVLink interconnect
- Optimized software stack
- Enterprise support
- Validated configurations
```

### DGX H100

```
Specifications:

GPUs:
- 8× H100 SXM (80GB each)
- Total GPU memory: 640 GB
- NVLink connections between all GPUs

CPU:
- 2× Intel Xeon Platinum 8480C (56 cores each)
- 112 cores total

System Memory:
- 2 TB DDR5

Storage:
- 30 TB NVMe (Gen 4)

Networking:
- 8× OSFP ports (400 Gb/s each)
- ConnectX-7 NICs

Performance:
- FP8: 32 petaFLOPS
- TF32: 16 petaFLOPS

Power:
- 10.2 kW

Pricing:
- ~$450,000-$500,000
```

### DGX A100

```
Specifications:

GPUs:
- 8× A100 SXM (80GB each)
- Total GPU memory: 640 GB
- NVLink connections between all GPUs

CPU:
- 2× AMD EPYC 7742 (64 cores each)
- 128 cores total

System Memory:
- 1 TB DDR4

Storage:
- 15 TB NVMe

Networking:
- 8× 200 Gb/s ConnectX-6

Performance:
- FP16: 5 petaFLOPS
- TF32: 2.5 petaFLOPS

Power:
- 6.5 kW

Pricing:
- ~$200,000
```

### DGX POD

```
Scalable AI infrastructure unit:

DGX H100 POD:
- 32× DGX H100 systems
- 256 H100 GPUs total
- 1 exaFLOP FP8 performance

Networking:
- NVIDIA Quantum-2 InfiniBand
- Fat-tree topology
- Non-blocking 400G

Software Stack:
- DGX OS (Ubuntu-based)
- NVIDIA AI Enterprise
- Container runtime
- Cluster management

Use Cases:
- Enterprise AI
- Cloud service providers
- Research institutions
- Large-scale training
```

### DGX SuperPOD

```
Datacenter-scale AI infrastructure:

Configuration:
- 128-1024+ DGX systems
- Thousands of GPUs
- Exascale performance

Example (DGX H100 SuperPOD):
- 256 DGX H100 systems
- 2,048 H100 GPUs
- 8 exaFLOPS FP8

Features:
✓ Pre-validated architecture
✓ Reference designs
✓ Deployment services
✓ Management software

Customers:
- Microsoft Azure
- Oracle Cloud
- Meta AI Research
- National labs
```

---

## Memory Hierarchy

### GPU Memory (HBM)

```
High Bandwidth Memory:

HBM2e (A100):
- Capacity: 40 or 80 GB
- Bandwidth: 2.0 TB/s
- Stacked DRAM on interposer
- Lower power than GDDR

HBM3 (H100):
- Capacity: 80 GB
- Bandwidth: 3.35 TB/s
- Higher density
- Better efficiency

Why HBM matters for AI:
- Models fit in memory
- Fast weight access
- High throughput training
```

### Memory Requirements

```
Model size (FP16):
Parameters × 2 bytes

Training memory (FP16 + Adam):
Parameters × 18 bytes
(2 model + 2 gradients + 4 optimizer states + overheads)

Examples:

BERT-Base (110M):
- Inference: 0.22 GB
- Training: 2 GB

GPT-2 (1.5B):
- Inference: 3 GB
- Training: 27 GB

GPT-3 (175B):
- Inference: 350 GB (needs 5× A100 80GB)
- Training: 3,150 GB (needs 40× A100 80GB)

Llama-2-70B:
- Inference: 140 GB (needs 2× A100 80GB)
- Training: 1,260 GB (needs 16× A100 80GB)
```

### Memory Optimization

```
Techniques:

1. Mixed Precision:
   FP16 instead of FP32 → 2x reduction

2. Quantization:
   INT8 instead of FP16 → 2x reduction
   INT4 → 4x reduction

3. Gradient Checkpointing:
   Recompute activations instead of storing
   Trade compute for memory

4. Model Parallelism:
   Split model across GPUs

5. CPU Offloading:
   Store parameters in CPU RAM
   Transfer to GPU as needed
```

---

## Distributed Training Infrastructure

### NVLink

```
High-bandwidth GPU-to-GPU interconnect:

NVLink 3 (A100):
- 600 GB/s bidirectional
- 12 links per GPU
- Peer-to-peer transfers

NVLink 4 (H100):
- 900 GB/s bidirectional
- 18 links per GPU
- Lower latency

vs PCIe Gen 5:
- PCIe: 128 GB/s
- NVLink 4: 900 GB/s (7x faster)

Benefits:
✓ Fast gradient synchronization
✓ Model parallelism
✓ Unified memory access
```

### NVSwitch

```
Fully connected GPU fabric:

8 GPUs without NVSwitch:
Limited connections between GPUs

8 GPUs with NVSwitch:
Every GPU connected to every other GPU
Full bandwidth

DGX H100:
- 4× NVSwitch chips
- 8 GPUs fully connected
- 900 GB/s between any pair
```

### InfiniBand

```
High-performance networking:

NVIDIA Quantum-2:
- 400 Gb/s per port (NDR)
- Ultra-low latency (<100 ns)
- RDMA support

vs Ethernet:
- Lower latency
- Higher bandwidth
- Better for HPC/AI

Use Cases:
- Multi-node training
- Distributed inference
- Storage access
```

### Multi-Node Training

```
Scaling strategies:

Data Parallelism:
- Each node has full model copy
- Different data batches
- Synchronize gradients

Tensor Parallelism:
- Split layers across GPUs
- Each GPU has part of each layer

Pipeline Parallelism:
- Split model vertically
- GPU 1: Layers 1-10
- GPU 2: Layers 11-20

Hybrid:
- Combine all strategies
- For models too large for single node
```

---

## Data Center Considerations

### Power and Cooling

```
Power Requirements:

Single DGX H100:
- Power: 10.2 kW
- Requires: 208V 3-phase

32× DGX H100 (POD):
- Power: 326 kW
- Cooling: Liquid cooling recommended

Annual Power Cost (example):
- 1 DGX H100: 10.2 kW × 24h × 365d × $0.10/kWh = $9,000/year
- 32 DGX H100: $288,000/year

Cooling:
- Air cooling: Up to 30-40 kW/rack
- Liquid cooling: 50+ kW/rack
- H100 optimized for liquid cooling
```

### Rack Design

```
Typical AI Rack:

┌─────────────────┐
│ 4× DGX H100     │ 40 kW
├─────────────────┤
│ Network Switches│ 2 kW
├─────────────────┤
│ Storage         │ 1 kW
├─────────────────┤
│ PDU             │
└─────────────────┘

Total: ~43 kW per rack
Requires: Liquid cooling or specialized HVAC
```

### Network Architecture

```
Typical POD Network:

Leaf-Spine Topology:

Spine Switches (400G)
     ↓  ↓  ↓  ↓
Leaf Switches (400G)
  ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓
DGX Systems

Benefits:
✓ Non-blocking
✓ Low latency
✓ Scalable
✓ Redundant paths
```

---

## Cloud Infrastructure

### Major Cloud Providers

#### AWS

```
GPU Instances:

P4 (A100):
- p4d.24xlarge: 8× A100 (40GB)
- 320 GB GPU memory
- 400 Gbps EFA networking
- ~$32/hour

P5 (H100):
- p5.48xlarge: 8× H100
- 640 GB GPU memory
- 3200 Gbps EFA networking
- ~$100/hour

Services:
- SageMaker (managed ML)
- EC2 (VMs with GPUs)
- EKS (Kubernetes with GPU)
```

#### Azure

```
GPU Instances:

ND A100 v4:
- 8× A100 (80GB)
- 640 GB GPU memory
- ~$30/hour

ND H100 v5:
- 8× H100
- 640 GB GPU memory
- ~$90/hour

Services:
- Azure ML (managed ML)
- VM Scale Sets
- AKS (Kubernetes with GPU)
```

#### Google Cloud Platform

```
GPU Instances:

A2 (A100):
- a2-ultragpu-8g: 8× A100 (80GB)
- 640 GB GPU memory
- ~$35/hour

A3 (H100):
- a3-highgpu-8g: 8× H100
- 640 GB GPU memory
- ~$110/hour

Services:
- Vertex AI (managed ML)
- GKE (Kubernetes with GPU)
- TPU option (alternative to GPU)
```

### On-Premise vs Cloud

```
On-Premise:

Pros:
✓ Long-term cost savings
✓ Data privacy and control
✓ No egress fees
✓ Customization

Cons:
✗ High upfront cost
✗ Maintenance overhead
✗ Capacity planning
✗ Upgrade cycles

Breakeven: ~6-12 months of continuous use

Cloud:

Pros:
✓ No upfront cost
✓ Elastic scaling
✓ Latest hardware
✓ Managed services

Cons:
✗ Ongoing costs
✗ Data egress fees
✗ Less control
✗ Potential latency

Best for: Variable workloads, experimentation
```

---

## Key Takeaways

### Hardware

1. **GPUs** provide massive parallelism for AI workloads (1000s of cores)

2. **Tensor Cores** accelerate matrix operations 8-10x vs CUDA cores

3. **A100** is the workhorse for training and inference (40/80 GB)

4. **H100** provides 3x performance over A100 with FP8 precision

5. **L40S** offers cost-effective inference with graphics capabilities

### Memory and Interconnect

6. **HBM** provides high bandwidth memory critical for large models

7. **Memory requirements**: FP16 model = 2 bytes/param, training = 18 bytes/param

8. **NVLink** enables 900 GB/s GPU-to-GPU communication (H100)

9. **Multi-Instance GPU (MIG)** partitions A100 into 7 isolated instances

10. **InfiniBand** provides ultra-low latency for multi-node training

### Systems

11. **Grace CPU** is ARM-based, optimized for AI and HPC

12. **Grace Hopper** combines Grace CPU + H100 GPU with 900 GB/s interconnect

13. **DGX H100** integrates 8× H100 GPUs with 32 petaFLOPS FP8

14. **DGX POD** scales to 256 H100 GPUs (1 exaFLOP)

15. **DGX SuperPOD** reaches thousands of GPUs for exascale computing

### Infrastructure

16. **Liquid cooling** required for high-density racks (40+ kW)

17. **Leaf-spine network** topology provides non-blocking communication

18. **Cloud providers** offer A100/H100 instances (~$30-100/hour)

19. **Breakeven** for on-premise is 6-12 months of continuous use

20. **Power costs** can exceed $9,000/year for single DGX H100

---

## Practice Questions

1. What are the key differences between CPUs and GPUs for AI workloads?

2. Explain how Tensor Cores differ from CUDA cores.

3. Calculate the memory required to train a 7B parameter model with Adam optimizer in FP16.

4. Compare A100 and H100 GPUs - when would you choose each?

5. What is Multi-Instance GPU (MIG) and what are its use cases?

6. How does NVLink improve multi-GPU training performance?

7. What is the Grace Hopper Superchip and what advantages does it provide?

8. Design a DGX-based infrastructure for training a 175B parameter model.

9. Compare on-premise DGX vs cloud GPUs for a startup training models continuously.

10. Calculate the annual power cost for a 32-GPU DGX POD at $0.10/kWh.

---

## Related Modules

- **[O2: AI/ML Fundamentals](O2.AI_ML_Fundamentals.md)** - Neural networks running on this hardware
- **[O3: Generative AI and LLMs](O3.Generative_AI_and_LLM.md)** - LLMs requiring massive infrastructure
- **[O7: Model Training](O7.Model_Training.md)** - Distributed training on multi-GPU systems
- **[O8: NVIDIA Ecosystem](O8.NVIDIA_Ecosystem.md)** - Software stack for this hardware
- **[O10: Additional Topics](O10.Additional_Topics.md)** - Advanced applications

---

**Next Module**: [O2: AI and ML Fundamentals](O2.AI_ML_Fundamentals.md)

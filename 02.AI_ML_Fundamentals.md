# O2: AI and Machine Learning Fundamentals

## Table of Contents
- [Introduction to Machine Learning](#introduction-to-machine-learning)
- [Neural Networks Basics](#neural-networks-basics)
- [Activation Functions](#activation-functions)
- [Loss Functions](#loss-functions)
- [Backpropagation](#backpropagation)
- [Gradient Descent and Optimization](#gradient-descent-and-optimization)
- [Training Neural Networks](#training-neural-networks)
- [Overfitting and Regularization](#overfitting-and-regularization)
- [Model Evaluation](#model-evaluation)
- [Key Takeaways](#key-takeaways)
- [Practice Questions](#practice-questions)

---

## Introduction to Machine Learning

### What is Machine Learning?

**Machine Learning** is the science of getting computers to learn patterns from data without being explicitly programmed.

```
Traditional Programming:
Rules + Data → Output

Machine Learning:
Data + Output → Rules (learned patterns)
```

### Types of Machine Learning

#### 1. Supervised Learning
```
Training: Labeled data (input, output pairs)
Goal: Learn mapping from inputs to outputs

Examples:
- Image classification: (image, "cat")
- Spam detection: (email, "spam" or "not spam")
- Price prediction: (features, price)
```

#### 2. Unsupervised Learning
```
Training: Unlabeled data (inputs only)
Goal: Discover patterns and structure

Examples:
- Clustering: Group similar items
- Dimensionality reduction: Compress data
- Anomaly detection: Find outliers
```

#### 3. Reinforcement Learning
```
Learning by trial and error using rewards and penalties.
Training: Agent interacts with environment
Goal: Learn optimal actions through rewards

Examples:
- Game playing: AlphaGo
- Robotics: Robot navigation
- Autonomous driving
```

---

## Neural Networks Basics

### The Biological Inspiration

Neural networks are inspired by biological neurons:

```
Biological Neuron:
Dendrites → Cell Body → Axon → Synapses
(inputs)   (process)   (output) (connections)

Artificial Neuron:
Inputs → Weighted Sum → Activation → Output
x₁,x₂    Σ(wᵢxᵢ + b)    f(z)        y
```

### Artificial Neuron (Perceptron)

```
Single Neuron:

x₁ ──w₁──┐
x₂ ──w₂──┤
x₃ ──w₃──┼─→ Σ ─→ f(z) ─→ y
   ...   │
xₙ ──wₙ──┘
     +b

Where:
x = inputs
w = weights (learned parameters)
b = bias (learned parameter)
z = Σ(wᵢxᵢ) + b (weighted sum)
f = activation function
y = output
```

**Mathematical Formulation**:
```
z = w₁x₁ + w₂x₂ + w₃x₃ + ... + wₙxₙ + b
y = f(z)
```

### Multi-Layer Neural Network

```
Input Layer → Hidden Layer(s) → Output Layer

Example (3-layer network):

x₁ ─┐
x₂ ─┼─→ [h₁] ─┐
x₃ ─┤    [h₂] ─┼─→ [o₁] ─→ y₁
x₄ ─┘    [h₃] ─┘    [o₂] ─→ y₂

Input     Hidden      Output
Layer     Layer       Layer
(4)       (3)         (2)
```

**Components**:
- **Input Layer**: Receives raw features
- **Hidden Layers**: Learn intermediate representations
- **Output Layer**: Produces predictions

**Depth**: Number of hidden layers (deep learning = many layers)

---

## Activation Functions

Activation functions introduce **non-linearity** into neural networks, enabling them to learn complex patterns.

### Why Non-linearity Matters

```
Without activation (linear):
Layer 1: z₁ = W₁x + b₁
Layer 2: z₂ = W₂z₁ + b₂
       = W₂(W₁x + b₁) + b₂
       = (W₂W₁)x + (W₂b₁ + b₂)
       = W_combined × x + b_combined

Result: Still linear! Multiple layers don't help.

With activation (non-linear):
Can learn XOR, circles, complex patterns
```

### Common Activation Functions

#### 1. ReLU (Rectified Linear Unit)

**Formula**: f(x) = max(0, x)

```
Graph:
  |    /
  |   /
  |  /
──┼─────→
  |

Properties:
✓ Fast computation
✓ No vanishing gradient for x > 0
✓ Sparse activation
✗ Dead neurons (gradient = 0 for x < 0)

Use case: Default choice for hidden layers
```

**Code**:
```python
def relu(x):
    return max(0, x)

# Example
relu(-2) = 0
relu(0)  = 0
relu(3)  = 3
```

#### 2. Sigmoid

**Formula**: σ(x) = 1 / (1 + e^(-x))

```
Graph:
    1 ────────
       /
      /
     /
    /
   ──────────
  0

Properties:
✓ Smooth gradient
✓ Output in (0, 1) - interpretable as probability
✗ Vanishing gradient problem
✗ Not zero-centered

Use case: Binary classification output layer
```

**Code**:
```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Example
sigmoid(-5) ≈ 0.007  (near 0)
sigmoid(0)  = 0.5
sigmoid(5)  ≈ 0.993  (near 1)
```

#### 3. Tanh (Hyperbolic Tangent)

**Formula**: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))

```
Graph:
    1 ────────
       /
      /
──┼───────
     /
    /
   ────────
  -1

Properties:
✓ Zero-centered (better than sigmoid)
✓ Output in (-1, 1)
✗ Still suffers from vanishing gradient

Use case: Hidden layers (when zero-centered matters)
```

**Code**:
```python
def tanh(x):
    return np.tanh(x)

# Example
tanh(-5) ≈ -0.999  (near -1)
tanh(0)  = 0
tanh(5)  ≈ 0.999   (near 1)
```

#### 4. Softmax

**Formula**: softmax(xᵢ) = e^(xᵢ) / Σⱼ e^(xⱼ)

```
Converts logits to probabilities:

Input:  [2.0, 1.0, 0.1]
         ↓ softmax
Output: [0.659, 0.242, 0.099]
        (sum = 1.0)

Properties:
✓ Outputs sum to 1
✓ Differentiable
✓ Preserves relative ordering

Use case: Multi-class classification output layer
```

**Code**:
```python
def softmax(x):
    exp_x = np.exp(x - np.max(x))  # numerical stability
    return exp_x / exp_x.sum()

# Example
logits = [2.0, 1.0, 0.1]
probs = softmax(logits)
# [0.659, 0.242, 0.099]
# Class 0 has highest probability
```

### Activation Function Comparison

| Function | Range | Use Case | Pros | Cons |
|----------|-------|----------|------|------|
| **ReLU** | [0, ∞) | Hidden layers | Fast, no vanishing gradient | Dead neurons |
| **Sigmoid** | (0, 1) | Binary output | Probability interpretation | Vanishing gradient |
| **Tanh** | (-1, 1) | Hidden layers | Zero-centered | Vanishing gradient |
| **Softmax** | (0, 1), Σ=1 | Multi-class output | Probability distribution | Only for output layer |

### Advanced Activation Functions

#### Leaky ReLU
```python
def leaky_relu(x, alpha=0.01):
    return max(alpha * x, x)

# Allows small gradient for x < 0
# Fixes "dead neuron" problem
```

#### ELU (Exponential Linear Unit)
```python
def elu(x, alpha=1.0):
    return x if x > 0 else alpha * (np.exp(x) - 1)

# Smooth, zero-centered
# Better than ReLU for some tasks
```

---

## Loss Functions

Loss functions measure how wrong the model's predictions are. Training aims to **minimize the loss**.

### Training Analogy

```
Think of training as adjusting aim:

Target: Hit bullseye
Current: Arrow lands far from center
Loss: Distance from bullseye
Goal: Adjust aim to reduce distance

In ML:
Target: Correct output
Current: Model prediction
Loss: Difference between prediction and target
Goal: Adjust weights to reduce loss
```

### Common Loss Functions

#### 1. Mean Squared Error (MSE)

**Use**: Regression problems (predicting continuous values)

**Formula**:
```
MSE = (1/n) Σᵢ (yᵢ - ŷᵢ)²

Where:
yᵢ = true value
ŷᵢ = predicted value
n = number of samples
```

**Example**:
```python
# Predicting house prices
true_prices = [300, 250, 400]  # thousands
pred_prices = [290, 260, 380]

errors = [300-290, 250-260, 400-380]
      = [10, -10, 20]

squared_errors = [100, 100, 400]

MSE = (100 + 100 + 400) / 3 = 200

Interpretation: Predictions are off by √200 ≈ $14k on average
```

**Properties**:
- Penalizes large errors heavily (squared term)
- Differentiable (smooth gradients)
- Units: squared units of target variable

#### 2. Binary Cross-Entropy

**Use**: Binary classification (2 classes: 0 or 1)

**Formula**:
```
BCE = -(1/n) Σᵢ [yᵢ log(ŷᵢ) + (1-yᵢ) log(1-ŷᵢ)]

Where:
yᵢ ∈ {0, 1} (true label)
ŷᵢ ∈ (0, 1) (predicted probability)
```

**Example**:
```python
# Email spam detection
true_labels = [1, 0, 1]      # 1=spam, 0=not spam
pred_probs  = [0.9, 0.2, 0.7] # predicted P(spam)

For sample 1: y=1, ŷ=0.9
  BCE = -(1×log(0.9) + 0×log(0.1))
      = -log(0.9) ≈ 0.105

For sample 2: y=0, ŷ=0.2
  BCE = -(0×log(0.2) + 1×log(0.8))
      = -log(0.8) ≈ 0.223

For sample 3: y=1, ŷ=0.7
  BCE = -log(0.7) ≈ 0.357

Average BCE = (0.105 + 0.223 + 0.357) / 3 ≈ 0.228
```

**Properties**:
- Penalizes confident wrong predictions heavily
- Works with sigmoid output
- Encourages probability predictions near 0 or 1

#### 3. Categorical Cross-Entropy

**Use**: Multi-class classification (3+ classes)

**Formula**:
```
CCE = -(1/n) Σᵢ Σⱼ yᵢⱼ log(ŷᵢⱼ)

Where:
yᵢⱼ = 1 if sample i is class j, else 0 (one-hot)
ŷᵢⱼ = predicted probability for class j
```

**Example**:
```python
# Image classification (cat, dog, bird)
true_label = [0, 1, 0]  # one-hot: class 1 (dog)
pred_probs = [0.2, 0.7, 0.1]  # P(cat), P(dog), P(bird)

CCE = -(0×log(0.2) + 1×log(0.7) + 0×log(0.1))
    = -log(0.7)
    ≈ 0.357

If prediction was perfect [0, 1, 0]:
CCE = -log(1.0) = 0 (minimum loss)

If prediction was wrong [0.9, 0.05, 0.05]:
CCE = -log(0.05) ≈ 3.0 (high loss)
```

**Properties**:
- Works with softmax output
- Only penalizes prediction for true class
- Loss approaches 0 as confidence in correct class → 1

### Loss Function Selection Guide

```
Problem Type → Loss Function → Output Activation

Regression:
  Continuous values → MSE → Linear (no activation)
  Example: House price prediction

Binary Classification:
  Two classes → Binary Cross-Entropy → Sigmoid
  Example: Spam detection, fraud detection

Multi-class Classification:
  3+ classes → Categorical Cross-Entropy → Softmax
  Example: Image classification, sentiment analysis
```

---

## Backpropagation

**Backpropagation** is the algorithm for computing gradients of the loss with respect to all model parameters, enabling gradient descent optimization.

### The Core Idea

```
Forward Pass: Input → Model → Prediction → Loss
Backward Pass: Loss → Gradients → Update Weights

Think of it like:
1. Make a prediction (forward)
2. See how wrong you were (loss)
3. Figure out how to adjust each weight (backprop)
4. Update weights to improve (gradient descent)
```

### Simple Example: 2-Layer Network

```
Network:
x → [w₁] → h → [w₂] → y → Loss

Forward pass:
h = σ(w₁ × x)           (hidden layer)
y = σ(w₂ × h)           (output)
L = (y - target)²       (loss)

Backward pass (compute gradients):
∂L/∂w₂ = ∂L/∂y × ∂y/∂w₂
∂L/∂w₁ = ∂L/∂y × ∂y/∂h × ∂h/∂w₁
```

### Chain Rule

Backpropagation uses the **chain rule** from calculus:

```
If y = f(g(x)), then:
dy/dx = (dy/dg) × (dg/dx)

Example:
y = (2x + 1)²

Let g = 2x + 1, so y = g²
dy/dg = 2g
dg/dx = 2

dy/dx = 2g × 2 = 4g = 4(2x + 1)
```

### Detailed Backpropagation Example

```python
# Simple network: 1 input → 1 hidden → 1 output

# Forward pass
x = 2.0
w1 = 0.5  # input → hidden
w2 = 0.3  # hidden → output
target = 1.0

# Layer 1
z1 = w1 * x           # = 0.5 × 2 = 1.0
h = sigmoid(z1)       # = σ(1) ≈ 0.731

# Layer 2
z2 = w2 * h           # = 0.3 × 0.731 ≈ 0.219
y = sigmoid(z2)       # = σ(0.219) ≈ 0.555

# Loss
L = (y - target)²     # = (0.555 - 1.0)² ≈ 0.198

# Backward pass (compute gradients)

# Gradient at output
dL/dy = 2(y - target) # = 2(0.555 - 1.0) = -0.890

# Gradient for w2
dy/dz2 = y(1 - y)     # sigmoid derivative = 0.555 × 0.445 ≈ 0.247
dz2/dw2 = h           # = 0.731

dL/dw2 = dL/dy × dy/dz2 × dz2/dw2
       = -0.890 × 0.247 × 0.731
       ≈ -0.161

# Gradient for w1
dz2/dh = w2           # = 0.3
dh/dz1 = h(1 - h)     # sigmoid derivative ≈ 0.196
dz1/dw1 = x           # = 2.0

dL/dw1 = dL/dy × dy/dz2 × dz2/dh × dh/dz1 × dz1/dw1
       = -0.890 × 0.247 × 0.3 × 0.196 × 2.0
       ≈ -0.026

# Update weights (learning rate α = 0.1)
w2 = w2 - α × dL/dw2 = 0.3 - 0.1 × (-0.161) = 0.316
w1 = w1 - α × dL/dw1 = 0.5 - 0.1 × (-0.026) = 0.503

# Weights increased because prediction was too low!
```

### Computational Graph

```
Visual representation:

       x
       |
      ×w₁
       |
      σ (h)
       |
      ×w₂
       |
      σ (y)
       |
   (y-target)²
       |
       L

Forward: Follow arrows down
Backward: Follow arrows up, apply chain rule
```

### Key Insights

1. **Gradients flow backward** from loss to inputs
2. **Each layer computes local gradients** and passes them back
3. **Chain rule** connects all layers
4. **Efficient**: Reuses forward pass computations

---

## Gradient Descent and Optimization

### Gradient Descent Intuition

```
Imagine you're on a mountain in fog:
- Goal: Reach the lowest valley (minimize loss)
- Can't see far: Only know local slope
- Strategy: Step downhill repeatedly

In ML:
- Mountain: Loss landscape
- Position: Current weights
- Slope: Gradient
- Step downhill: w = w - α × gradient
```

### Gradient Descent Algorithm

```python
# Training loop
for epoch in range(num_epochs):
    # Forward pass
    predictions = model(X)
    loss = compute_loss(predictions, y_true)

    # Backward pass
    gradients = compute_gradients(loss)

    # Update weights
    for weight in model.parameters():
        weight = weight - learning_rate × gradient
```

### Variants of Gradient Descent

#### 1. Batch Gradient Descent

```
Use entire dataset for each update:

for epoch in range(num_epochs):
    gradients = compute_gradients(entire_dataset)
    weights = weights - α × gradients

Pros:
✓ Stable convergence
✓ Accurate gradient

Cons:
✗ Slow for large datasets
✗ May get stuck in local minima
```

#### 2. Stochastic Gradient Descent (SGD)

```
Use one sample at a time:

for epoch in range(num_epochs):
    for sample in dataset:
        gradient = compute_gradient(sample)
        weights = weights - α × gradient

Pros:
✓ Fast updates
✓ Can escape local minima (noise)

Cons:
✗ Noisy convergence
✗ May not find exact minimum
```

#### 3. Mini-Batch Gradient Descent

```
Use small batches (e.g., 32, 64, 128 samples):

for epoch in range(num_epochs):
    for batch in get_batches(dataset, batch_size=32):
        gradients = compute_gradients(batch)
        weights = weights - α × gradients

Pros:
✓ Balance speed and stability
✓ Efficient GPU computation
✓ Standard in practice

Typical batch sizes: 32, 64, 128, 256
```

### Learning Rate

The **learning rate** α controls the step size:

```
Too small (α = 0.0001):
  ●→●→●→●→●→●→●→●→●→●
  Slow convergence, many iterations

Good (α = 0.01):
  ●──→●──→●──→●
  Balanced convergence

Too large (α = 1.0):
  ●─────────→●←─────────●→
  Overshooting, divergence
```

**Typical values**: 0.001, 0.01, 0.1

### Advanced Optimizers

#### 1. Momentum

```
Accumulates past gradients to smooth updates:

velocity = 0
for each iteration:
    gradient = compute_gradient()
    velocity = β × velocity + gradient
    weights = weights - α × velocity

β = 0.9 (typical)

Think: Ball rolling downhill gains momentum
```

#### 2. Adam (Adaptive Moment Estimation)

```
Combines momentum + adaptive learning rates:

m = 0  # first moment (mean)
v = 0  # second moment (variance)

for each iteration:
    gradient = compute_gradient()
    m = β₁ × m + (1 - β₁) × gradient
    v = β₂ × v + (1 - β₂) × gradient²

    m_hat = m / (1 - β₁ᵗ)  # bias correction
    v_hat = v / (1 - β₂ᵗ)

    weights = weights - α × m_hat / (√v_hat + ε)

Hyperparameters:
α = 0.001     (learning rate)
β₁ = 0.9      (momentum)
β₂ = 0.999    (variance decay)
ε = 1e-8      (numerical stability)

Most popular optimizer for deep learning!
```

### Optimizer Comparison

| Optimizer | Speed | Convergence | Use Case |
|-----------|-------|-------------|----------|
| **SGD** | Fast | Can be unstable | Simple problems |
| **SGD + Momentum** | Fast | More stable | Most problems |
| **Adam** | Fast | Very stable | Default choice |
| **RMSprop** | Fast | Stable | RNNs |

---

## Training Neural Networks

### Training Process Overview

```
1. Initialize weights (randomly)
2. For each epoch:
   a. Forward pass: Compute predictions
   b. Compute loss
   c. Backward pass: Compute gradients
   d. Update weights with optimizer
   e. Track metrics
3. Repeat until convergence
```

### Training Example

```python
import numpy as np

# Simple neural network training
class SimpleNN:
    def __init__(self, input_size, hidden_size, output_size):
        # Initialize weights randomly
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))

    def forward(self, X):
        # Forward pass
        self.z1 = X.dot(self.W1) + self.b1
        self.a1 = np.maximum(0, self.z1)  # ReLU
        self.z2 = self.a1.dot(self.W2) + self.b2
        self.a2 = 1 / (1 + np.exp(-self.z2))  # Sigmoid
        return self.a2

    def backward(self, X, y, output):
        # Backward pass
        m = X.shape[0]

        # Output layer gradients
        dz2 = output - y
        dW2 = self.a1.T.dot(dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m

        # Hidden layer gradients
        da1 = dz2.dot(self.W2.T)
        dz1 = da1 * (self.z1 > 0)  # ReLU derivative
        dW1 = X.T.dot(dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m

        return dW1, db1, dW2, db2

    def update(self, dW1, db1, dW2, db2, learning_rate):
        # Update weights
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2

# Training loop
model = SimpleNN(input_size=4, hidden_size=5, output_size=1)
learning_rate = 0.01
num_epochs = 1000

for epoch in range(num_epochs):
    # Forward pass
    output = model.forward(X_train)

    # Compute loss
    loss = -np.mean(y_train * np.log(output) +
                    (1 - y_train) * np.log(1 - output))

    # Backward pass
    dW1, db1, dW2, db2 = model.backward(X_train, y_train, output)

    # Update weights
    model.update(dW1, db1, dW2, db2, learning_rate)

    # Print progress
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")
```

### Weight Initialization

```
Why initialization matters:

All zeros:
  ✗ All neurons learn the same thing
  ✗ Symmetry problem

Too large:
  ✗ Exploding gradients
  ✗ Numerical instability

Too small:
  ✗ Vanishing gradients
  ✗ Slow learning

Good initialization:
  ✓ Random, small values
  ✓ Scaled based on layer size
```

**Xavier/Glorot Initialization** (for sigmoid/tanh):
```python
W = np.random.randn(n_in, n_out) * np.sqrt(1 / n_in)
```

**He Initialization** (for ReLU):
```python
W = np.random.randn(n_in, n_out) * np.sqrt(2 / n_in)
```

### Batch Normalization

```
Normalize activations within each batch:

For each layer:
  μ = mean(batch)
  σ² = variance(batch)

  x_norm = (x - μ) / √(σ² + ε)

  # Learnable parameters
  y = γ × x_norm + β

Benefits:
✓ Faster training
✓ Higher learning rates possible
✓ Less sensitive to initialization
✓ Regularization effect
```

---

## Overfitting and Regularization

### Understanding Overfitting

```
Underfitting:
  Model too simple
  Poor on training and test data

  ╭─────────╮
  │ ·   ·  │  Simple line can't
  │  ·  ·  │  capture pattern
  │ ·   ·  │
  ╰─────────╯

Good Fit:
  Captures pattern
  Good on both training and test

  ╭─────────╮
  │ · ──·  │  Smooth curve
  │  ·──·  │  fits well
  │ ·  ·   │
  ╰─────────╯

Overfitting:
  Model too complex
  Perfect on training, poor on test

  ╭─────────╮
  │ ·╱╲ ·  │  Memorizes noise
  │  ╲╱·   │  instead of pattern
  │ · ╲ ·  │
  ╰─────────╯
```

### Detecting Overfitting

```
Training vs Validation Loss:

Epoch   Train Loss   Val Loss
  1        0.8         0.9      ← Both decreasing (good)
  5        0.5         0.6
 10        0.3         0.5
 20        0.1         0.7      ← Val increasing (overfitting!)
 50        0.01        1.2

If validation loss increases while training loss decreases:
→ Model is overfitting!
```

### Regularization Techniques

#### 1. L2 Regularization (Weight Decay)

```
Add penalty for large weights:

Loss_total = Loss_data + λ × Σ(wᵢ²)

Where:
λ = regularization strength (0.01, 0.001)

Effect: Encourages smaller weights

Code:
loss = data_loss + lambda_reg * np.sum(weights**2)
```

#### 2. L1 Regularization

```
Loss_total = Loss_data + λ × Σ|wᵢ|

Effect: Encourages sparse weights (some → 0)
Use: Feature selection
```

#### 3. Dropout

```
Randomly drop neurons during training:

Training:
  Input: [1.0, 2.0, 3.0, 4.0]
  Dropout (p=0.5): [1.0, 0, 3.0, 0]

Each forward pass: Different neurons dropped
Effect: Prevents co-adaptation, like ensemble

Inference:
  Use all neurons
  Scale outputs by (1 - p)

Code:
dropout_rate = 0.5
mask = np.random.rand(*layer.shape) > dropout_rate
layer = layer * mask / (1 - dropout_rate)
```

**Typical dropout rates**: 0.2 - 0.5

#### 4. Early Stopping

```
Stop training when validation loss stops improving:

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = validate()

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_model()
        patience_counter = 0
    else:
        patience_counter += 1

    if patience_counter >= patience:
        print("Early stopping!")
        break

Prevents overfitting without modifying model
```

#### 5. Data Augmentation

```
Create more training data through transformations:

Images:
- Rotation, flipping, cropping
- Color jittering, brightness
- Random erasing

Text:
- Synonym replacement
- Back-translation
- Random insertion/deletion

Effect: More diverse training data → better generalization
```

### Regularization Comparison

| Technique | When to Use | Effect | Cost |
|-----------|-------------|--------|------|
| **L2** | Always (default) | Smaller weights | Low |
| **Dropout** | Large networks | Ensemble effect | Medium |
| **Early stopping** | Always | Stops at optimal point | None |
| **Data augmentation** | Limited data | More training samples | High |

---

## Model Evaluation

### Training, Validation, Test Split

```
Dataset Split:

All Data (100%)
    |
    ├── Training (70%)      → Train model
    ├── Validation (15%)    → Tune hyperparameters
    └── Test (15%)          → Final evaluation

Never use test set until final evaluation!
```

### Evaluation Metrics

#### Classification Metrics

**Accuracy**:
```
Accuracy = Correct Predictions / Total Predictions

Example:
Predicted: [1, 0, 1, 1, 0]
True:      [1, 0, 1, 0, 0]
Correct:    ✓  ✓  ✓  ✗  ✓

Accuracy = 4/5 = 0.8 (80%)

Problem: Misleading for imbalanced data
```

**Confusion Matrix**:
```
              Predicted
              0      1
Actual  0   [TN    FP]
        1   [FN    TP]

TN: True Negative (correctly predicted 0)
FP: False Positive (incorrectly predicted 1)
FN: False Negative (incorrectly predicted 0)
TP: True Positive (correctly predicted 1)
```

**Precision and Recall**:
```
Precision = TP / (TP + FP)
  "Of predicted positives, how many are correct?"

Recall = TP / (TP + FN)
  "Of actual positives, how many did we find?"

Example (spam detection):
100 emails: 90 legitimate, 10 spam
Model predicts: 12 spam (8 correct, 4 false)

TP = 8 (correctly identified spam)
FP = 4 (legitimate marked as spam)
FN = 2 (missed spam)
TN = 86 (correctly identified legitimate)

Precision = 8/(8+4) = 0.67 (67% of flagged emails are spam)
Recall = 8/(8+2) = 0.80 (80% of spam was caught)
```

**F1-Score**:
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)

Harmonic mean of precision and recall
Balances both metrics

F1 = 2 × (0.67 × 0.80) / (0.67 + 0.80) ≈ 0.73
```

#### Regression Metrics

**Mean Absolute Error (MAE)**:
```
MAE = (1/n) Σ|yᵢ - ŷᵢ|

True:      [100, 200, 300]
Predicted: [110, 190, 290]
Errors:    [10, 10, 10]

MAE = (10 + 10 + 10) / 3 = 10

Interpretation: Average error in same units as target
```

**R² Score**:
```
R² = 1 - (SS_residual / SS_total)

Range: (-∞, 1]
  1.0: Perfect predictions
  0.0: As good as predicting mean
  <0: Worse than predicting mean

Interpretation: Fraction of variance explained
R² = 0.85 → Model explains 85% of variance
```

### Cross-Validation

```
K-Fold Cross-Validation (K=5):

Fold 1: [Test][Train][Train][Train][Train]
Fold 2: [Train][Test][Train][Train][Train]
Fold 3: [Train][Train][Test][Train][Train]
Fold 4: [Train][Train][Train][Test][Train]
Fold 5: [Train][Train][Train][Train][Test]

Average performance across all folds
More robust estimate than single split

Code:
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
mean_score = scores.mean()
```

---

## Key Takeaways

### Core Concepts

1. **Neural networks** learn by adjusting weights through backpropagation

2. **Activation functions** (ReLU, sigmoid, tanh, softmax) introduce non-linearity

3. **Loss functions** measure prediction error:
   - MSE for regression
   - Cross-entropy for classification

4. **Backpropagation** computes gradients using chain rule

5. **Gradient descent** updates weights to minimize loss

### Training and Optimization

6. **Mini-batch gradient descent** balances speed and stability

7. **Adam optimizer** is the default choice (adaptive learning rate + momentum)

8. **Learning rate** controls step size (typical: 0.001-0.01)

9. **Weight initialization** matters (Xavier for sigmoid, He for ReLU)

10. **Batch normalization** speeds up training

### Preventing Overfitting

11. **Overfitting** occurs when model memorizes training data

12. **Regularization** techniques:
    - L2 regularization (weight decay)
    - Dropout (randomly drop neurons)
    - Early stopping
    - Data augmentation

13. **Validation set** used to detect overfitting

14. **Early stopping** prevents overfitting automatically

### Evaluation

15. **Train/val/test split** (70/15/15) for proper evaluation

16. **Accuracy** can be misleading for imbalanced data

17. **Precision and recall** trade-off for classification

18. **Cross-validation** provides robust performance estimate

---

## Practice Questions

1. What is the difference between supervised and unsupervised learning? Give examples of each.

2. Why do we need activation functions in neural networks? What would happen without them?

3. Calculate the output of a ReLU activation for inputs: [-2, 0, 3, -1, 5]

4. Given predictions [0.9, 0.3, 0.8] and true labels [1, 0, 1], calculate binary cross-entropy loss.

5. Explain backpropagation in your own words. What is the role of the chain rule?

6. Compare batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.

7. What is the purpose of the learning rate? What happens if it's too large or too small?

8. How can you detect if your model is overfitting? What are three ways to prevent it?

9. Given a confusion matrix with TP=80, FP=20, FN=10, TN=90, calculate precision, recall, and F1-score.

10. Why is it important to have separate training, validation, and test sets?

11. A model achieves 95% accuracy on a dataset where 95% of samples are class 0. Is this good? Why or why not?

12. Compare ReLU, sigmoid, and softmax activation functions. When would you use each?

13. Explain dropout regularization. How does it prevent overfitting?

14. What is the Adam optimizer and why is it popular?

15. Design a neural network architecture for classifying images into 10 categories. Specify layers, activations, and loss function.

---

## Related Modules

- **[01: AI Infrastructure](01.AI_Infrastructure.md)** - Hardware (GPUs, TPUs) for training neural networks
- **[03: Generative AI and LLMs](03.Generative_AI_and_LLM.md)** - Advanced neural networks for text generation
- **[04: Transformer Architecture](04.Transformer_Architecture.md)** - Modern architecture built on these fundamentals
- **[07: Model Training](07.Model_Training.md)** - Advanced training techniques
- **[08: NVIDIA Ecosystem](08.NVIDIA_Ecosystem.md)** - Tools for efficient neural network training

---

**Next Module**: [03: Generative AI and LLMs](03.Generative_AI_and_LLM.md)

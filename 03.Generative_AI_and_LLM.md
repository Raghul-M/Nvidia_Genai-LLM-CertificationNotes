# O3: Generative AI and Large Language Models

## Table of Contents
- [Introduction to Generative AI](#introduction-to-generative-ai)
- [What are Large Language Models?](#what-are-large-language-models)
- [Evolution of Language Models](#evolution-of-language-models)
- [How LLMs Work](#how-llms-work)
- [Training Large Language Models](#training-large-language-models)
- [Popular LLM Architectures](#popular-llm-architectures)
- [Capabilities and Limitations](#capabilities-and-limitations)
- [Applications of LLMs](#applications-of-llms)
- [Scaling Laws](#scaling-laws)
- [Future of Generative AI](#future-of-generative-ai)
- [Key Takeaways](#key-takeaways)
- [Practice Questions](#practice-questions)

---

## Introduction to Generative AI

### What is Generative AI?

**Generative AI** creates new content (text, images, audio, code) based on patterns learned from training data.

```
Traditional AI (Discriminative):
Input ‚Üí Model ‚Üí Classification/Prediction
"Is this a cat?" ‚Üí Yes/No

Generative AI:
Input ‚Üí Model ‚Üí New Content Creation
"Draw a cat" ‚Üí üê± (generates image)
"Write about cats" ‚Üí (generates text)
```

### Types of Generative AI

```
By Modality:

1. Text Generation:
   - Large Language Models (GPT, Llama, Claude)
   - Code generation (Codex, CodeLlama)
   - Creative writing

2. Image Generation:
   - Diffusion models (Stable Diffusion, DALL-E)
   - GANs (StyleGAN)
   - Image editing and inpainting

3. Audio Generation:
   - Text-to-speech (TTS)
   - Music generation
   - Voice cloning

4. Video Generation:
   - Text-to-video
   - Video editing
   - Animation

5. Multimodal:
   - Text + Image (GPT-4V, Gemini)
   - Any-to-any generation
```

### Generative vs Discriminative Models

```
Discriminative Models:
P(Y|X) - Probability of label given input

Example: Image classifier
Input: Image of dog
Output: P(dog) = 0.95

Generative Models:
P(X) or P(X|Y) - Probability of data

Example: Image generator
Input: "A dog playing in snow"
Output: New image of dog in snow

Key Difference:
Discriminative: Decision boundary
Generative: Data distribution
```

### Why Generative AI Now?

```
Convergence of factors:

1. Compute Power:
   - GPUs with Tensor Cores
   - Cloud infrastructure
   - Distributed training

2. Data Scale:
   - Internet-scale text (CommonCrawl)
   - Billions of images
   - Curated datasets

3. Algorithmic Advances:
   - Transformers (2017)
   - Self-attention
   - Scaling laws discovered

4. Transfer Learning:
   - Pre-train on large corpus
   - Fine-tune for specific tasks
   - Few-shot learning

Result: Models that can generate human-quality content
```

---

## What are Large Language Models?

### Definition

**Large Language Models (LLMs)** are neural networks with billions of parameters trained on vast amounts of text data to understand and generate human language.

```
Characteristics:

Size:
- "Large": 1B+ parameters
- Medium: 100M - 1B
- Small: <100M

Training:
- Self-supervised learning
- Next token prediction
- Massive text corpora (TB scale)

Capabilities:
- Text generation
- Question answering
- Reasoning
- Code generation
- Translation
- Summarization
```

### Key Components

```
1. Tokenization:
   Text ‚Üí Tokens (subword units)
   "Hello world" ‚Üí ["Hello", " world"]

2. Embeddings:
   Tokens ‚Üí Dense vectors
   "Hello" ‚Üí [0.1, -0.3, 0.5, ...]

3. Transformer Layers:
   Process token sequences
   Learn relationships and patterns

4. Language Modeling Head:
   Predict next token
   Softmax over vocabulary

5. Generation:
   Iteratively predict tokens
   "Once upon a" ‚Üí "time" ‚Üí "there" ‚Üí ...
```

### Architecture Overview

```
Input: "The cat sat on the"

Tokenization:
["The", " cat", " sat", " on", " the"]

Embedding:
[[0.1, ...], [0.3, ...], [-0.2, ...], [0.5, ...], [0.1, ...]]

Transformer Blocks (stacked):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Self-Attention      ‚îÇ ‚Üê Learn relationships
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Feed-Forward        ‚îÇ ‚Üê Transform representations
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Layer Norm          ‚îÇ ‚Üê Stabilize training
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
√ó N layers (e.g., 32, 80, 120)

Output Layer:
Logits over vocabulary (50K-100K tokens)

Sampling:
Select next token: "mat" (P=0.85)

Generated: "The cat sat on the mat"
```

---

## Evolution of Language Models

### Pre-Transformer Era (Pre-2017)

```
N-gram Models:
P(word_t | word_{t-1}, word_{t-2}, ...)

Limitations:
‚úó Fixed context window
‚úó No long-range dependencies
‚úó Curse of dimensionality

RNNs/LSTMs (2010s):
Sequential processing
Hidden state carries context

Limitations:
‚úó Vanishing gradients
‚úó Hard to parallelize
‚úó Limited context (100s of tokens)
```

### Transformer Revolution (2017)

```
"Attention Is All You Need" (Vaswani et al., 2017)

Key Innovation: Self-Attention
- Process entire sequence in parallel
- Capture long-range dependencies
- Scalable to billions of parameters

Impact:
‚Üí Enabled modern LLMs
‚Üí Foundation for GPT, BERT, T5
‚Üí Transformed NLP
```

### LLM Timeline

```
2018: BERT (Google)
- 340M parameters
- Bidirectional encoder
- Masked language modeling
- SOTA on 11 NLP tasks

2018: GPT (OpenAI)
- 117M parameters
- Decoder-only
- Next token prediction
- Showed transfer learning works

2019: GPT-2 (OpenAI)
- 1.5B parameters
- "Too dangerous to release" (initially)
- Coherent long-form generation
- Zero-shot capabilities

2020: GPT-3 (OpenAI)
- 175B parameters
- Few-shot learning
- Emergent abilities
- First truly "large" LLM

2021: Codex (OpenAI)
- GPT-3 fine-tuned on code
- Powers GitHub Copilot
- Code generation

2022: ChatGPT (OpenAI)
- GPT-3.5 with RLHF
- Conversational interface
- Mass adoption (100M users in 2 months)

2023: GPT-4 (OpenAI)
- Multimodal (text + images)
- Improved reasoning
- Longer context (32K tokens)

2023: Llama 2 (Meta)
- Open weights (7B, 13B, 70B)
- Commercial license
- Strong performance

2023: Claude 2 (Anthropic)
- 100K context window
- Constitutional AI
- Focus on safety

2024: Gemini (Google)
- Multimodal native
- Multiple sizes (Nano to Ultra)
- Competitive with GPT-4
```

---

## How LLMs Work

### Next Token Prediction

```
Training Objective:
Given: "The cat sat on the"
Predict: "mat"

Formally:
P(token_t | token_1, token_2, ..., token_{t-1})

Example:
Input: "Paris is the capital of"
Model computes probabilities:
- "France": 0.92 ‚Üê Highest probability
- "Europe": 0.03
- "fashion": 0.02
- ...

Sample: "France"

Continue:
"Paris is the capital of France. The city"
Next prediction: "is" (0.45), "has" (0.25), ...
```

### Autoregressive Generation

```
Step-by-step generation:

Step 0: "Once upon a"
Step 1: Predict "time" ‚Üí "Once upon a time"
Step 2: Predict "there" ‚Üí "Once upon a time there"
Step 3: Predict "was" ‚Üí "Once upon a time there was"
Step 4: Predict "a" ‚Üí "Once upon a time there was a"
Step 5: Predict "dragon" ‚Üí "Once upon a time there was a dragon"
...

Stop conditions:
- Max length reached
- <EOS> token generated
- Stop sequence encountered
```

### Sampling Strategies

#### 1. Greedy Sampling

```python
# Always pick highest probability token

def greedy_sample(logits):
    return argmax(logits)

Example:
Probabilities: {"the": 0.4, "a": 0.3, "an": 0.2, "this": 0.1}
Select: "the" (always)

Pros:
‚úì Deterministic
‚úì Fast

Cons:
‚úó Repetitive
‚úó No creativity
‚úó Can get stuck in loops
```

#### 2. Temperature Sampling

```python
# Control randomness

def temperature_sample(logits, temperature=1.0):
    # Divide logits by temperature
    scaled_logits = logits / temperature
    probs = softmax(scaled_logits)
    return sample(probs)

Temperature = 0.0: Greedy (deterministic)
Temperature = 0.5: Less random (focused)
Temperature = 1.0: Standard (balanced)
Temperature = 2.0: More random (creative)

Example (original probabilities):
{"the": 0.4, "a": 0.3, "an": 0.2, "this": 0.1}

Temperature = 0.5 (sharper):
{"the": 0.55, "a": 0.28, "an": 0.13, "this": 0.04}

Temperature = 2.0 (flatter):
{"the": 0.32, "a": 0.29, "an": 0.23, "this": 0.16}
```

#### 3. Top-k Sampling

```python
# Sample from top k tokens only

def top_k_sample(logits, k=50):
    top_k_logits, top_k_indices = top_k(logits, k)
    probs = softmax(top_k_logits)
    return sample(probs)

Example (k=3):
All probabilities: {"the": 0.4, "a": 0.3, "an": 0.2, "this": 0.1, ...}
Top-3: {"the": 0.4, "a": 0.3, "an": 0.2}
Renormalize: {"the": 0.44, "a": 0.33, "an": 0.22}
Sample from these 3 only

Avoids: Low-probability, nonsensical tokens
```

#### 4. Top-p (Nucleus) Sampling

```python
# Sample from smallest set with cumulative probability ‚â• p

def top_p_sample(logits, p=0.9):
    sorted_probs = sort(softmax(logits), descending=True)
    cumsum = cumulative_sum(sorted_probs)
    nucleus = sorted_probs[cumsum <= p]
    return sample(nucleus)

Example (p=0.9):
Sorted: {"the": 0.4, "a": 0.3, "an": 0.2, "this": 0.1, ...}
Cumsum: {0.4, 0.7, 0.9, 1.0, ...}
Nucleus: {"the", "a", "an"} (cumsum ‚â§ 0.9)

Adaptive: Varies number of tokens based on distribution
```

### Contextual Understanding

```
Self-Attention enables understanding context:

Sentence: "The bank was steep, so I couldn't climb it."

"bank" attends to:
- "steep" (high attention) ‚Üí river bank
- "climb" (high attention) ‚Üí not financial

vs

"The bank was closed, so I used the ATM."

"bank" attends to:
- "closed" (high attention) ‚Üí financial institution
- "ATM" (high attention) ‚Üí financial

Same word, different meaning based on context!
```

---

## Training Large Language Models

### Three Training Stages

#### Stage 1: Pre-training

```
Objective: Learn general language understanding

Data:
- Massive text corpora (TB scale)
- CommonCrawl, Wikipedia, books, code
- Diverse domains and languages

Task:
- Next token prediction (causal LM)
- Learn grammar, facts, reasoning

Duration:
- Weeks to months
- Thousands of GPUs
- Millions of dollars

Result:
- Base model with broad knowledge
- Not optimized for following instructions
```

#### Stage 2: Supervised Fine-Tuning (SFT)

```
Objective: Teach instruction following

Data:
- High-quality (instruction, response) pairs
- Human-written examples
- 10K-100K examples

Task:
- Predict response given instruction
- Learn desired behavior and format

Duration:
- Hours to days
- Few GPUs
- Much cheaper than pre-training

Result:
- Model that follows instructions
- Better formatting and helpfulness
```

#### Stage 3: Reinforcement Learning from Human Feedback (RLHF)

```
Objective: Align with human preferences

Process:
1. Collect Comparisons:
   For same prompt, generate multiple responses
   Humans rank: A > B > C

2. Train Reward Model:
   Learn to predict human preferences
   Input: (prompt, response)
   Output: Reward score

3. RL Optimization:
   Use reward model to fine-tune LLM
   PPO (Proximal Policy Optimization)
   Maximize reward while staying close to SFT model

Result:
- Helpful, harmless, honest
- Reduced hallucinations
- Better instruction following
```

### Training Data

```
Pre-training Corpora:

CommonCrawl:
- Web pages (petabytes)
- Filtered for quality
- Deduplicated

Books:
- Project Gutenberg
- BookCorpus
- Literary text

Wikipedia:
- Factual knowledge
- Multiple languages
- Structured information

Code:
- GitHub repositories
- Stack Overflow
- Programming documentation

Academic:
- arXiv papers
- PubMed abstracts
- Scientific text

Multilingual:
- Non-English text
- Parallel corpora
- Translation datasets

Total: Trillions of tokens
```

### Training Dynamics

```
Loss Curve:

Training Loss
  ‚Üë
  ‚îÇ ‚ï≤
  ‚îÇ  ‚ï≤
  ‚îÇ   ‚ï≤___
  ‚îÇ       ‚ï≤___
  ‚îÇ           ‚ï≤___
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Tokens Seen
              (trillions)

Key Observations:
1. Smooth decrease (no sudden drops)
2. Power law relationship
3. Predictable based on compute

Scaling Law:
Loss ‚àù 1 / (Compute)^Œ±

Where Œ± ‚âà 0.05-0.1

Implications:
- 10x compute ‚Üí ~1.5x better loss
- Diminishing returns
- But capabilities keep improving!
```

---

## Popular LLM Architectures

### GPT Family (OpenAI)

```
Architecture: Decoder-only Transformer

GPT-3:
- Parameters: 175B
- Layers: 96
- Hidden size: 12,288
- Attention heads: 96
- Context: 2,048 tokens
- Training: 300B tokens

GPT-3.5 (ChatGPT):
- GPT-3 + SFT + RLHF
- Optimized for chat
- Better instruction following

GPT-4:
- Estimated 1T+ parameters
- Multimodal (text + images)
- 32K context (some versions)
- Improved reasoning

Key Features:
‚úì Few-shot learning
‚úì Broad capabilities
‚úì Code generation
‚úì Reasoning
```

### Llama Family (Meta)

```
Architecture: Decoder-only Transformer

Llama 2:
- Sizes: 7B, 13B, 70B
- Open weights (with license)
- Context: 4,096 tokens
- RMSNorm, SwiGLU activation
- Grouped-query attention

Llama 2 Chat:
- RLHF-tuned versions
- Optimized for dialogue
- Safety guidelines

Code Llama:
- Specialized for code
- 7B, 13B, 34B versions
- Trained on code + text

Key Features:
‚úì Open weights
‚úì Commercial use allowed
‚úì Efficient architecture
‚úì Strong performance
```

### Claude (Anthropic)

```
Architecture: Not fully disclosed (likely decoder-only)

Claude 2:
- Context: 100,000 tokens
- Constitutional AI training
- Focus on safety and honesty
- Reduced hallucinations

Key Features:
‚úì Very long context
‚úì Safety-focused
‚úì Nuanced understanding
‚úì Good at reasoning
```

### PaLM (Google)

```
Architecture: Decoder-only Transformer

PaLM 2:
- Efficient scaling
- Multilingual (100+ languages)
- Reasoning capabilities
- Powers Bard

Key Features:
‚úì Multilingual
‚úì Mathematical reasoning
‚úì Code generation
‚úì Efficient
```

### Comparison

| Model | Size | Context | Open | Key Strength |
|-------|------|---------|------|--------------|
| **GPT-4** | ~1T | 8K-32K | ‚úó | Reasoning, multimodal |
| **Claude 2** | Unknown | 100K | ‚úó | Long context, safety |
| **Llama 2 70B** | 70B | 4K | ‚úì | Open, efficient |
| **PaLM 2** | Unknown | 8K | ‚úó | Multilingual |

---

## Capabilities and Limitations

### Emergent Capabilities

```
Abilities that appear at scale:

Few-Shot Learning:
Prompt: "Translate to French:
English: Hello ‚Üí French: Bonjour
English: Goodbye ‚Üí French: Au revoir
English: Thank you ‚Üí French:"

Model: "Merci"

Chain-of-Thought Reasoning:
Prompt: "Roger has 5 tennis balls. He buys 2 more cans,
each with 3 balls. How many balls does he have?
Let's think step by step:"

Model: "Roger starts with 5 balls.
He buys 2 cans with 3 balls each, so 2 √ó 3 = 6 balls.
Total: 5 + 6 = 11 balls."

Zero-Shot Task Transfer:
No examples needed for many tasks
Model generalizes from pre-training
```

### Capabilities

```
‚úì Text Generation:
  - Creative writing
  - Summarization
  - Paraphrasing

‚úì Question Answering:
  - Factual questions
  - Reasoning
  - Explanations

‚úì Code:
  - Generation
  - Debugging
  - Explanation

‚úì Translation:
  - 100+ languages
  - Nuanced understanding

‚úì Analysis:
  - Sentiment
  - Named entity recognition
  - Classification

‚úì Reasoning:
  - Mathematical
  - Logical
  - Common sense

‚úì Dialogue:
  - Multi-turn conversation
  - Context retention
  - Personalization
```

### Limitations

```
‚úó Hallucinations:
  - Generate false information confidently
  - Cannot verify facts
  - May cite non-existent sources

‚úó Knowledge Cutoff:
  - Training data has end date
  - No real-time information
  - Cannot access current events

‚úó Reasoning Limits:
  - Struggle with complex math
  - Inconsistent logical reasoning
  - No true understanding

‚úó Context Window:
  - Limited to 2K-100K tokens
  - Cannot process very long documents
  - Forgets earlier context

‚úó No Grounding:
  - Cannot see, hear, or interact with world
  - No physical embodiment
  - Text-only understanding (unless multimodal)

‚úó Biases:
  - Reflect training data biases
  - Can generate harmful content
  - Require safety measures

‚úó Inconsistency:
  - Different responses to same prompt
  - May contradict itself
  - Not deterministic (with temperature > 0)
```

---

## Applications of LLMs

### Content Creation

```
1. Writing Assistance:
   - Blog posts, articles
   - Email drafting
   - Creative fiction

2. Marketing:
   - Ad copy
   - Product descriptions
   - Social media content

3. Education:
   - Lesson plans
   - Study guides
   - Explanations
```

### Coding

```
1. Code Generation:
   GitHub Copilot, Tabnine
   "Write a function to sort a list"

2. Debugging:
   "Why doesn't this code work?"
   Suggest fixes

3. Documentation:
   Generate docstrings
   Write README files

4. Code Review:
   Identify bugs
   Suggest improvements
```

### Customer Service

```
1. Chatbots:
   - 24/7 availability
   - Handle common queries
   - Escalate complex issues

2. Email Automation:
   - Draft responses
   - Categorize emails
   - Sentiment analysis

3. Knowledge Base:
   - Answer FAQs
   - Search documentation
   - Provide solutions
```

### Research and Analysis

```
1. Literature Review:
   - Summarize papers
   - Extract key findings
   - Identify trends

2. Data Analysis:
   - Generate SQL queries
   - Interpret results
   - Create reports

3. Hypothesis Generation:
   - Suggest research directions
   - Connect ideas
   - Identify gaps
```

### Accessibility

```
1. Text Simplification:
   - Plain language summaries
   - Adjust reading level
   - Explain jargon

2. Translation:
   - Real-time translation
   - Cultural adaptation
   - Multilingual support

3. Voice Interfaces:
   - Combined with TTS/STT
   - Hands-free interaction
   - Assistive technology
```

---

## Scaling Laws

### Compute, Data, Parameters

```
Chinchilla Scaling Laws (DeepMind, 2022):

Key Finding: Most LLMs are undertrained

Optimal allocation:
- Parameters (N)
- Training tokens (D)

Should scale together:
D ‚âà 20 √ó N

Examples:

GPT-3 (175B params):
Trained on: 300B tokens
Optimal: 3.5T tokens (undertrained)

Chinchilla (70B params):
Trained on: 1.4T tokens
Result: Better than GPT-3 despite being smaller!

Implication:
Training longer on more data > just making models bigger
```

### Emergent Abilities

```
Abilities that appear suddenly at scale:

Arithmetic:
- Small models: Cannot add
- ~10B params: Single-digit addition
- ~100B params: Multi-digit arithmetic

Few-shot learning:
- Small: Requires fine-tuning
- Large: Works with just examples in prompt

Chain-of-thought:
- Small: Doesn't work
- Large: Enables step-by-step reasoning

Unpredictable emergence:
- Hard to predict what capabilities will appear
- Qualitative changes, not just quantitative
- "More is different"
```

### Cost Scaling

```
Training cost increases rapidly:

GPT-2 (1.5B):
~$50K compute

GPT-3 (175B):
~$5M compute

GPT-4 (estimated):
~$100M compute

Inference cost:
Per 1M tokens:
- GPT-3.5: $0.50-$2
- GPT-4: $30-$60
- Llama 2 70B (self-hosted): Variable

Economics favor:
- Fewer, larger models for training
- Distillation for deployment
- Specialized models for efficiency
```

---

## Future of Generative AI

### Trends

```
1. Multimodal Models:
   - Text + Image + Audio + Video
   - Unified architectures
   - Cross-modal reasoning

2. Longer Context:
   - 100K ‚Üí 1M+ tokens
   - Process entire books
   - Better long-term memory

3. Agentic AI:
   - LLMs that take actions
   - Use tools (APIs, code execution)
   - Multi-step planning

4. Personalization:
   - Adapt to individual users
   - Remember preferences
   - Contextual awareness

5. Efficiency:
   - Smaller models with better performance
   - Faster inference
   - Lower cost

6. Grounding:
   - Connect to external knowledge
   - Real-time information
   - Verifiable facts

7. Specialization:
   - Domain-specific models
   - Medical, legal, scientific
   - Code-focused (Codex)
```

### Open Questions

```
1. Scaling Limits:
   Will performance keep improving with size?
   Physical limits (energy, hardware)?

2. Sample Efficiency:
   Can we match human learning efficiency?
   Humans learn from much less data

3. True Understanding:
   Do LLMs "understand" or just mimic?
   Chinese Room argument

4. Safety and Alignment:
   How to ensure models are helpful and harmless?
   Scalable oversight as models become smarter

5. Societal Impact:
   Job displacement?
   Misinformation?
   Democratization of AI?
```

---

## Key Takeaways

### Core Concepts

1. **Generative AI** creates new content based on learned patterns

2. **LLMs** are transformer-based models with billions of parameters

3. **Next token prediction** is the core training objective

4. **Autoregressive generation** produces text one token at a time

5. **Sampling strategies** (temperature, top-k, top-p) control randomness

### Training

6. **Pre-training** on trillions of tokens learns general language

7. **Supervised fine-tuning (SFT)** teaches instruction following

8. **RLHF** aligns models with human preferences

9. **Scaling laws** relate compute, data, and parameters

10. **Emergent abilities** appear at large scale unpredictably

### Models

11. **GPT-4** offers advanced reasoning and multimodal capabilities

12. **Llama 2** provides open weights with commercial license

13. **Claude** specializes in long context (100K tokens) and safety

14. **Decoder-only transformers** dominate modern LLMs

15. **Model size** ranges from 7B to 1T+ parameters

### Capabilities

16. **Few-shot learning** works without fine-tuning

17. **Chain-of-thought** enables step-by-step reasoning

18. **Code generation** is a key capability (Copilot)

19. **Multimodal models** combine text, images, and more

20. **Hallucinations** remain a key limitation requiring mitigation

---

## Practice Questions

1. What is the difference between generative and discriminative AI?

2. Explain how autoregressive generation works in LLMs.

3. Compare greedy sampling, temperature sampling, and top-p sampling.

4. What are the three stages of LLM training? Describe each.

5. Why is RLHF important for models like ChatGPT?

6. What are the Chinchilla scaling laws and their implications?

7. Compare GPT-4, Llama 2 70B, and Claude 2.

8. What are emergent abilities in LLMs? Give examples.

9. List 5 capabilities and 5 limitations of modern LLMs.

10. How does self-attention enable contextual understanding?

11. Calculate the memory required to store a 70B parameter model in FP16.

12. What is the difference between pre-training and fine-tuning?

13. Design an LLM-powered application for customer service.

14. How do hallucinations occur in LLMs and how can they be mitigated?

15. What trends do you expect in the future of generative AI?

---

## Related Modules

- **[O2: AI/ML Fundamentals](O2.AI_ML_Fundamentals.md)** - Neural network basics underlying LLMs
- **[O4: Transformer Architecture](O4.Transformer_Architecture.md)** - Detailed transformer mechanics
- **[O5: Model Selection](O5.Model_Selection.md)** - Choosing the right LLM
- **[O6: Model Customization](O6.Model_Customization.md)** - RAG and prompt engineering
- **[O7: Model Training](O7.Model_Training.md)** - Fine-tuning techniques
- **[O8: NVIDIA Ecosystem](O8.NVIDIA_Ecosystem.md)** - NeMo for LLM development
- **[O9: Ethical AI](O9.Ethical_AI.md)** - Safety and alignment

---

**Next Module**: [O4: Transformer Architecture](O4.Transformer_Architecture.md)

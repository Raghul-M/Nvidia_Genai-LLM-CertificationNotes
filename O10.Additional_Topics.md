# O10: Additional Topics

## Table of Contents
- [Diffusion Models](#diffusion-models)
- [Multimodal AI](#multimodal-ai)
- [Named Entity Recognition (NER)](#named-entity-recognition-ner)
- [Retrieval-Augmented Dialogue (RAD)](#retrieval-augmented-dialogue-rad)
- [Python Libraries for AI](#python-libraries-for-ai)
- [Computer Vision Fundamentals](#computer-vision-fundamentals)
- [Speech and Audio AI](#speech-and-audio-ai)
- [Graph Neural Networks](#graph-neural-networks)
- [AI for Scientific Computing](#ai-for-scientific-computing)
- [Emerging Trends](#emerging-trends)
- [Key Takeaways](#key-takeaways)
- [Practice Questions](#practice-questions)

---

## Diffusion Models

### What are Diffusion Models?

**Diffusion models** generate images by gradually denoising random noise into coherent images.

```
Training (Forward Process):
Image â†’ Add noise â†’ Add noise â†’ ... â†’ Pure noise
 ðŸ–¼ï¸  â†’    ðŸ–¼ï¸~   â†’    ~ðŸ–¼ï¸~   â†’        ~~~

Generation (Reverse Process):
Pure noise â†’ Denoise â†’ Denoise â†’ ... â†’ Image
    ~~~    â†’   ~ðŸ–¼ï¸~  â†’    ðŸ–¼ï¸~  â†’        ðŸ–¼ï¸
```

### How Diffusion Works

```
Forward Diffusion (Training):
Step 0: xâ‚€ = clean image
Step 1: xâ‚ = xâ‚€ + noiseâ‚
Step 2: xâ‚‚ = xâ‚ + noiseâ‚‚
...
Step T: xâ‚œ = pure noise

Reverse Diffusion (Inference):
Step T: Start with pure noise
Step T-1: Predict and remove noise
Step T-2: Predict and remove noise
...
Step 0: Clean image

Model learns: Given noisy image xâ‚œ,
predict noise that was added
```

### Mathematics

```
Forward process:
q(xâ‚œ | xâ‚œâ‚‹â‚) = ð’©(âˆš(1-Î²â‚œ) xâ‚œâ‚‹â‚, Î²â‚œI)

Where:
Î²â‚œ = noise schedule (increases over time)
ð’© = Gaussian distribution

Reverse process:
p(xâ‚œâ‚‹â‚ | xâ‚œ) = ð’©(Î¼Î¸(xâ‚œ, t), Î£Î¸(xâ‚œ, t))

Model predicts Î¼Î¸ (mean) to denoise
```

### Stable Diffusion

```
Architecture:

Text Prompt: "A cat on a skateboard"
     â†“
CLIP Text Encoder â†’ Text Embedding
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Latent Diffusion       â”‚
â”‚ (UNet in latent space) â”‚
â”‚                        â”‚
â”‚ Noise â†’ Denoise â†’ ... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
VAE Decoder â†’ Final Image

Key Innovation: Work in compressed latent space (faster)
```

**Implementation**:
```python
from diffusers import StableDiffusionPipeline
import torch

# Load model
pipe = StableDiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1",
    torch_dtype=torch.float16
)
pipe = pipe.to("cuda")

# Generate image
prompt = "A futuristic city with flying cars, cyberpunk style"
image = pipe(
    prompt,
    num_inference_steps=50,  # Denoising steps
    guidance_scale=7.5       # How closely to follow prompt
).images[0]

image.save("output.png")
```

### Diffusion vs GANs

| Aspect | Diffusion Models | GANs |
|--------|-----------------|------|
| **Training** | Stable | Unstable (mode collapse) |
| **Quality** | High | High |
| **Diversity** | Excellent | Can be limited |
| **Speed** | Slow (50 steps) | Fast (1 step) |
| **Control** | Good (classifier-free guidance) | Moderate |

### Advanced Techniques

#### 1. Classifier-Free Guidance
```
Amplify impact of text prompt:

score = unconditional_score +
        guidance_scale Ã— (conditional_score - unconditional_score)

guidance_scale = 7.5 (typical)
  Higher â†’ Closer to prompt, less diverse
  Lower â†’ More diverse, less adherent

Example:
Prompt: "Red car"
Low guidance (2): Various cars, some not red
High guidance (15): Definitely red cars, less variation
```

#### 2. ControlNet
```
Add spatial control to generation:

Inputs:
- Text prompt
- Control image (edges, depth, pose)

Output:
- Image following structure + prompt

Example:
Control: Stick figure pose
Prompt: "Astronaut"
Output: Astronaut in that exact pose
```

#### 3. Image-to-Image
```
Transform existing images:

img2img(
    image=input_image,
    prompt="Turn this into a watercolor painting",
    strength=0.75  # How much to change
)

strength=0.0: No change
strength=1.0: Completely new image
```

### Applications

```
1. Image Generation:
   - Art creation
   - Concept design
   - Marketing materials

2. Image Editing:
   - Inpainting (fill missing parts)
   - Outpainting (extend image)
   - Style transfer

3. Super-Resolution:
   - Upscale images
   - Enhance details

4. Video Generation:
   - Animate images
   - Video synthesis
```

---

## Multimodal AI

### What is Multimodal AI?

**Multimodal AI** processes multiple types of data (text, images, audio, video) simultaneously.

```
Unimodal:
Text â†’ Text Model â†’ Text Output

Multimodal:
Text + Image â†’ Multimodal Model â†’ Text/Image Output
```

### CLIP (Contrastive Language-Image Pre-training)

```
Architecture:

Image Encoder (Vision Transformer)
     â†“
Image Embedding (512 dims)
     â†“
Similarity â† Compare â†’ Text Embedding (512 dims)
                            â†‘
                Text Encoder (Transformer)

Training:
Batch of (image, caption) pairs
Learn to maximize similarity for matching pairs
Minimize similarity for non-matching pairs
```

**Implementation**:
```python
import torch
import clip
from PIL import Image

# Load model
device = "cuda"
model, preprocess = clip.load("ViT-B/32", device=device)

# Prepare inputs
image = preprocess(Image.open("cat.jpg")).unsqueeze(0).to(device)
text = clip.tokenize(["a cat", "a dog", "a bird"]).to(device)

# Compute features
with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)

    # Cosine similarity
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)

print("Probabilities:", similarity)
# [0.95, 0.03, 0.02] â†’ It's a cat!
```

**Applications**:
```
1. Zero-shot Image Classification:
   No training needed for new categories
   Just provide text descriptions

2. Image Search:
   Search images using natural language
   "Dogs playing in snow"

3. Content Moderation:
   Check if image matches description
   Detect inappropriate content
```

### Vision-Language Models

#### 1. BLIP (Bootstrapping Language-Image Pre-training)

```
Capabilities:
- Image captioning: Image â†’ Text
- Visual question answering: Image + Question â†’ Answer
- Image-text retrieval

Example:
from transformers import BlipProcessor, BlipForConditionalGeneration

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Image captioning
inputs = processor(image, return_tensors="pt")
output = model.generate(**inputs)
caption = processor.decode(output[0], skip_special_tokens=True)
# "a cat sitting on a laptop"

# Visual QA
inputs = processor(image, "What color is the cat?", return_tensors="pt")
output = model.generate(**inputs)
answer = processor.decode(output[0], skip_special_tokens=True)
# "orange"
```

#### 2. LLaVA (Large Language and Vision Assistant)

```
Architecture:
Vision Encoder (CLIP ViT) + LLM (LLaMA)

Image â†’ Vision Encoder â†’ Visual Tokens
                              â†“
Text Prompt + Visual Tokens â†’ LLM â†’ Response

Example:
User: "What's unusual about this image?" [shows upside-down room]
LLaVA: "The room appears to be upside down, with furniture on the ceiling"

Capabilities:
âœ“ Detailed image understanding
âœ“ Multi-turn conversations about images
âœ“ Reasoning about visual content
```

#### 3. GPT-4V (Vision)

```
Multimodal ChatGPT:

Inputs: Text + Images
Outputs: Text

Use cases:
- Analyze charts and graphs
- Understand memes
- Read handwritten text
- Identify objects in photos
- Explain visual concepts
```

### Audio-Language Models

#### Whisper (Speech Recognition)

```python
from transformers import pipeline

# Load model
transcriber = pipeline("automatic-speech-recognition", model="openai/whisper-large-v3")

# Transcribe
result = transcriber("audio.mp3")
print(result["text"])

# Supports 99 languages
# Robust to noise, accents

Features:
âœ“ Multilingual
âœ“ Translation (speech â†’ English text)
âœ“ Timestamp generation
âœ“ Speaker diarization (who said what)
```

#### Audio Generation (Text-to-Speech)

```python
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech

processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")

# Generate speech
text = "Hello, this is a test of text to speech"
inputs = processor(text=text, return_tensors="pt")
speech = model.generate_speech(inputs["input_ids"])

# Save audio
import soundfile as sf
sf.write("output.wav", speech.numpy(), samplerate=16000)
```

### Multimodal Embeddings

```
Unified embedding space for all modalities:

Text: "A cat"  â†’ [0.1, 0.3, -0.2, ...]
Image: ðŸ±      â†’ [0.12, 0.28, -0.18, ...]
Audio: "meow"  â†’ [0.11, 0.31, -0.19, ...]

All close in embedding space!

Applications:
- Cross-modal search (text â†’ find image)
- Content recommendation
- Multimodal RAG
```

---

## Named Entity Recognition (NER)

### What is NER?

**Named Entity Recognition** identifies and classifies named entities in text.

```
Input: "Apple was founded by Steve Jobs in Cupertino, California."

Output:
Apple         â†’ ORGANIZATION
Steve Jobs    â†’ PERSON
Cupertino     â†’ LOCATION
California    â†’ LOCATION
```

### Entity Types

```
Common entity types:

PER: Person (John Smith, Marie Curie)
ORG: Organization (Google, UN, Harvard)
LOC: Location (Paris, Mount Everest)
DATE: Dates (January 1, 2023, last week)
TIME: Times (3 PM, morning)
MONEY: Monetary values ($100, â‚¬50)
PERCENT: Percentages (50%, 3.14%)
GPE: Geopolitical entity (USA, Tokyo)
PRODUCT: Products (iPhone, Windows)
EVENT: Events (Olympics, World War II)
```

### NER with spaCy

```python
import spacy

# Load model
nlp = spacy.load("en_core_web_sm")

# Process text
text = "Elon Musk founded SpaceX in 2002 in California."
doc = nlp(text)

# Extract entities
for ent in doc.ents:
    print(f"{ent.text:20} {ent.label_:15} {ent.start_char}-{ent.end_char}")

# Output:
# Elon Musk            PERSON          0-9
# SpaceX               ORG             18-24
# 2002                 DATE            28-32
# California           GPE             36-46
```

### NER with Transformers

```python
from transformers import pipeline

# Load NER pipeline
ner = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english")

# Process text
text = "Apple CEO Tim Cook announced new products in September."
entities = ner(text)

for entity in entities:
    print(f"{entity['word']:20} {entity['entity']:15} {entity['score']:.2f}")

# Output:
# Apple                B-ORG           0.99
# Tim                  B-PER           0.99
# Cook                 I-PER           0.99
# September            B-DATE          0.98

# B- : Beginning of entity
# I- : Inside entity (continuation)
```

### Custom NER Models

```python
# Fine-tune for domain-specific entities

from transformers import AutoModelForTokenClassification, Trainer

# Define custom entity types
id2label = {
    0: "O",           # Outside
    1: "B-DISEASE",   # Disease
    2: "I-DISEASE",
    3: "B-DRUG",      # Medication
    4: "I-DRUG",
    5: "B-SYMPTOM",   # Symptom
    6: "I-SYMPTOM"
}

# Load base model
model = AutoModelForTokenClassification.from_pretrained(
    "bert-base-cased",
    num_labels=len(id2label),
    id2label=id2label,
    label2id={v: k for k, v in id2label.items()}
)

# Train on medical data
trainer = Trainer(
    model=model,
    train_dataset=medical_dataset,
    eval_dataset=eval_dataset
)
trainer.train()

# Use for medical NER
text = "Patient has diabetes and takes metformin for treatment."
# Output:
# diabetes â†’ DISEASE
# metformin â†’ DRUG
```

### Applications

```
1. Information Extraction:
   Extract structured data from unstructured text

2. Document Understanding:
   Identify key entities in contracts, reports

3. Search and Indexing:
   Improve search by entity-based filtering

4. Knowledge Graphs:
   Build relationships between entities

5. Content Categorization:
   Tag articles by mentioned entities

6. Privacy:
   Detect and redact PII (names, addresses)
```

---

## Retrieval-Augmented Dialogue (RAD)

### What is RAD?

**RAD** extends RAG to multi-turn conversations, maintaining context across dialogue.

```
RAG (single turn):
User: "What is Python?"
  â†“ Retrieve docs
Response: [Answer based on retrieved docs]

RAD (multi-turn):
User: "What is Python?"
  â†“ Retrieve docs + conversation history
Assistant: "Python is a programming language..."

User: "When was it created?"
  â†“ Retrieve docs (understands "it" = Python) + history
Assistant: "Python was created in 1991 by Guido van Rossum."

User: "What's it used for?"
  â†“ Retrieve docs + history
Assistant: "Python is used for web development, data science..."
```

### RAD Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conversation Historyâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context Resolution   â”‚ (Resolve "it", "he", "that")
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Reformulation  â”‚ (Expand query with context)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retrieval            â”‚ (Fetch relevant documents)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Response Generation  â”‚ (Generate answer with context)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Context Resolution

```python
# Resolve pronouns and references

conversation_history = [
    {"role": "user", "content": "Tell me about Einstein"},
    {"role": "assistant", "content": "Einstein was a physicist..."},
    {"role": "user", "content": "When was he born?"}
]

# Resolve "he" â†’ "Einstein"
def resolve_references(query, history):
    # Use coreference resolution model
    context = " ".join([msg["content"] for msg in history])
    resolved_query = coreference_model.resolve(query, context)
    return resolved_query

resolved = resolve_references("When was he born?", conversation_history)
# "When was Einstein born?"
```

### Query Reformulation

```python
# Expand query with conversation context

def reformulate_query(current_query, history):
    """
    Combine current query with relevant context from history
    """
    # Extract entities from history
    context_entities = extract_entities(history)

    # Combine with current query
    reformulated = f"{current_query} Context: {context_entities}"

    return reformulated

# Example
history = "User asked about Python. Assistant explained it's a programming language."
current = "What's it used for?"
reformulated = reformulate_query(current, history)
# "What's it used for? Context: Python programming language"
```

### Conversational Retrieval

```python
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import Chroma
from langchain.llms import OpenAI

# Setup
vectorstore = Chroma.from_documents(documents, embeddings)
llm = OpenAI(temperature=0)

# Create conversational chain
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# Multi-turn conversation
chat_history = []

# Turn 1
query1 = "What is machine learning?"
result1 = qa_chain({"question": query1, "chat_history": chat_history})
print(result1["answer"])
chat_history.append((query1, result1["answer"]))

# Turn 2 (uses context from Turn 1)
query2 = "What are its main types?"
result2 = qa_chain({"question": query2, "chat_history": chat_history})
print(result2["answer"])
# Understands "its" refers to "machine learning"
chat_history.append((query2, result2["answer"]))

# Turn 3
query3 = "Give me an example of the supervised type"
result3 = qa_chain({"question": query3, "chat_history": chat_history})
print(result3["answer"])
# Understands "supervised type" from previous context
```

### Memory Management

```python
# Limit context window size

class ConversationMemory:
    def __init__(self, max_turns=5):
        self.history = []
        self.max_turns = max_turns

    def add(self, user_msg, assistant_msg):
        self.history.append({
            "user": user_msg,
            "assistant": assistant_msg
        })

        # Keep only recent turns
        if len(self.history) > self.max_turns:
            self.history = self.history[-self.max_turns:]

    def get_context(self):
        # Return formatted history
        context = []
        for turn in self.history:
            context.append(f"User: {turn['user']}")
            context.append(f"Assistant: {turn['assistant']}")
        return "\n".join(context)

# Use
memory = ConversationMemory(max_turns=5)
memory.add("What is AI?", "AI is artificial intelligence...")
memory.add("When was it invented?", "The term was coined in 1956...")

context = memory.get_context()
# Includes last 5 turns only
```

### Advanced RAD Techniques

#### 1. Conversation Summarization
```python
# Summarize old turns to save context space

def summarize_old_turns(history):
    """
    Summarize turns beyond window
    """
    recent = history[-5:]  # Keep recent turns
    old = history[:-5]     # Summarize old turns

    if old:
        summary = summarize_model.summarize(old)
        return [{"summary": summary}] + recent
    return recent
```

#### 2. Entity Tracking
```python
# Track mentioned entities across conversation

class EntityTracker:
    def __init__(self):
        self.entities = {}

    def update(self, text):
        # Extract entities
        entities = ner_model(text)
        for ent in entities:
            self.entities[ent.text] = ent.label

    def get_context(self):
        # Return entity context
        return ", ".join([f"{k} ({v})" for k, v in self.entities.items()])

tracker = EntityTracker()
tracker.update("Einstein was born in Germany")
tracker.update("He developed relativity theory")

print(tracker.get_context())
# "Einstein (PERSON), Germany (LOCATION), relativity theory (THEORY)"
```

---

## Python Libraries for AI

### Essential Libraries

#### 1. NumPy (Numerical Computing)

```python
import numpy as np

# Array operations
a = np.array([[1, 2], [3, 4]])
b = np.array([[5, 6], [7, 8]])

# Matrix multiplication
c = np.dot(a, b)
# [[19 22]
#  [43 50]]

# Element-wise operations
d = a + b
# [[6  8]
#  [10 12]]

# Statistical operations
mean = np.mean(a)  # 2.5
std = np.std(a)    # 1.118

# Broadcasting
a = np.array([1, 2, 3])
b = np.array([[1], [2], [3]])
c = a + b  # Broadcasts to 3x3
# [[2 3 4]
#  [3 4 5]
#  [4 5 6]]
```

#### 2. Pandas (Data Manipulation)

```python
import pandas as pd

# Create DataFrame
df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'salary': [50000, 60000, 70000]
})

# Filter
young = df[df['age'] < 30]

# Group by
avg_salary = df.groupby('age')['salary'].mean()

# Merge
df2 = pd.DataFrame({
    'name': ['Alice', 'Bob'],
    'department': ['Engineering', 'Sales']
})
merged = pd.merge(df, df2, on='name')

# Handling missing data
df = df.fillna(0)  # Fill NaN with 0
df = df.dropna()   # Drop rows with NaN
```

#### 3. Scikit-learn (Machine Learning)

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train model
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
```

#### 4. PyTorch (Deep Learning)

```python
import torch
import torch.nn as nn

# Define model
class SimpleNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Train
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

#### 5. Hugging Face Transformers

```python
from transformers import pipeline

# Text classification
classifier = pipeline("sentiment-analysis")
result = classifier("I love this product!")
# [{'label': 'POSITIVE', 'score': 0.99}]

# Text generation
generator = pipeline("text-generation", model="gpt2")
text = generator("Once upon a time", max_length=50)

# Question answering
qa = pipeline("question-answering")
answer = qa(
    question="What is AI?",
    context="AI stands for Artificial Intelligence..."
)
```

### Specialized Libraries

#### 6. OpenCV (Computer Vision)

```python
import cv2

# Load image
img = cv2.imread('image.jpg')

# Resize
resized = cv2.resize(img, (640, 480))

# Convert to grayscale
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Face detection
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
faces = face_cascade.detectMultiScale(gray, 1.1, 4)

# Draw rectangles
for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
```

#### 7. NLTK (Natural Language Processing)

```python
import nltk

# Tokenization
text = "Hello world. This is a test."
tokens = nltk.word_tokenize(text)
# ['Hello', 'world', '.', 'This', 'is', 'a', 'test', '.']

# Part-of-speech tagging
pos_tags = nltk.pos_tag(tokens)
# [('Hello', 'NNP'), ('world', 'NN'), ...]

# Named entity recognition
entities = nltk.chunk.ne_chunk(pos_tags)

# Stemming
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
stemmer.stem("running")  # "run"
```

#### 8. Matplotlib/Seaborn (Visualization)

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Line plot
plt.plot([1, 2, 3, 4], [1, 4, 2, 3])
plt.xlabel('X axis')
plt.ylabel('Y axis')
plt.title('Line Plot')
plt.show()

# Heatmap
data = np.random.rand(10, 10)
sns.heatmap(data, annot=True, cmap='coolwarm')
plt.show()

# Distribution
sns.histplot(data, kde=True)
plt.show()
```

---

## Computer Vision Fundamentals

### Image Basics

```
Digital Image:
- Grid of pixels
- Each pixel has color (RGB)
- Resolution: Height Ã— Width Ã— Channels

Example:
640Ã—480 RGB image = 640 Ã— 480 Ã— 3 = 921,600 values
```

### Convolutional Neural Networks (CNNs)

```
CNN Architecture:

Input Image (224Ã—224Ã—3)
     â†“
Conv Layer 1 (64 filters)
     â†“
ReLU Activation
     â†“
MaxPooling (reduce size)
     â†“
Conv Layer 2 (128 filters)
     â†“
ReLU
     â†“
MaxPooling
     â†“
Flatten
     â†“
Fully Connected
     â†“
Softmax (class probabilities)
```

### Common CV Tasks

#### 1. Image Classification

```python
from torchvision import models, transforms
from PIL import Image

# Load pretrained ResNet
model = models.resnet50(pretrained=True)
model.eval()

# Preprocess image
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

img = Image.open("cat.jpg")
img_t = transform(img)
batch_t = torch.unsqueeze(img_t, 0)

# Predict
out = model(batch_t)
_, index = torch.max(out, 1)
percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100

print(f"Class: {index.item()}, Confidence: {percentage[index].item():.2f}%")
```

#### 2. Object Detection

```
YOLO (You Only Look Once):

Input Image
     â†“
CNN Backbone
     â†“
Grid (e.g., 13Ã—13)
Each cell predicts:
- Bounding boxes (x, y, w, h)
- Confidence scores
- Class probabilities

Output: Multiple objects with boxes and labels
```

#### 3. Image Segmentation

```
Types:

Semantic Segmentation:
- Label each pixel with class
- Example: "sky", "road", "car"

Instance Segmentation:
- Separate individual objects
- Example: "car 1", "car 2", "person 1"

Models:
- U-Net (medical imaging)
- Mask R-CNN (instance segmentation)
- DeepLab (semantic segmentation)
```

---

## Speech and Audio AI

### Audio Processing Basics

```
Audio Signal:
- Waveform: Amplitude over time
- Sample Rate: 16kHz, 44.1kHz, 48kHz
- Duration: Seconds

Spectogram:
- Frequency representation over time
- Used as input to models
```

### Automatic Speech Recognition (ASR)

```python
# Whisper (state-of-the-art ASR)

from transformers import pipeline

asr = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-large-v3"
)

# Transcribe
result = asr("audio.mp3")
print(result["text"])

# With timestamps
result = asr("audio.mp3", return_timestamps=True)
for chunk in result["chunks"]:
    print(f"{chunk['timestamp']}: {chunk['text']}")
```

### Text-to-Speech (TTS)

```python
from TTS.api import TTS

# List models
TTS.list_models()

# Load model
tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC")

# Generate speech
tts.tts_to_file(
    text="Hello, this is a test of text to speech.",
    file_path="output.wav"
)
```

### Speaker Recognition

```python
# Identify who is speaking

from pyannote.audio import Pipeline

# Load pipeline
pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization")

# Diarize (who spoke when)
diarization = pipeline("audio.wav")

for turn, _, speaker in diarization.itertracks(yield_label=True):
    print(f"{turn.start:.1f}s - {turn.end:.1f}s: {speaker}")

# Output:
# 0.0s - 3.5s: Speaker A
# 3.5s - 7.2s: Speaker B
# 7.2s - 12.0s: Speaker A
```

---

## Graph Neural Networks

### What are Graphs?

```
Graph = Nodes + Edges

Social Network:
Nodes: People
Edges: Friendships

Molecule:
Nodes: Atoms
Edges: Chemical bonds

Citation Network:
Nodes: Papers
Edges: Citations
```

### Graph Neural Networks (GNNs)

```
Key Idea: Learn node representations by aggregating neighbor features

Message Passing:
1. For each node:
   - Collect features from neighbors
   - Aggregate (sum, mean, max)
   - Update own features

Repeat for multiple layers

Applications:
- Node classification
- Link prediction
- Graph classification
```

### GNN Implementation

```python
import torch
from torch_geometric.nn import GCNConv

class GCN(torch.nn.Module):
    def __init__(self, num_features, hidden_dim, num_classes):
        super().__init__()
        self.conv1 = GCNConv(num_features, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, num_classes)

    def forward(self, x, edge_index):
        # x: Node features
        # edge_index: Graph connectivity

        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Use
model = GCN(num_features=16, hidden_dim=32, num_classes=7)
output = model(node_features, edge_index)
```

---

## AI for Scientific Computing

### Physics-Informed Neural Networks (PINNs)

```
Incorporate physics laws into neural networks:

Loss = Data Loss + Physics Loss

Example (heat equation):
âˆ‚u/âˆ‚t = Î± âˆ‚Â²u/âˆ‚xÂ²

Network predicts u(x, t)
Physics loss penalizes violations of heat equation

Benefits:
- Requires less data
- Respects physical constraints
- Generalizes better
```

### Drug Discovery

```
Applications:

1. Molecular Property Prediction:
   Molecule â†’ GNN â†’ Properties (toxicity, solubility)

2. De Novo Drug Design:
   Target â†’ Generative Model â†’ Novel molecules

3. Protein Folding:
   Amino acid sequence â†’ AlphaFold â†’ 3D structure
```

---

## Emerging Trends

### 1. Mixture of Experts (MoE)

```
Instead of one large model, use many specialized models:

Input â†’ Router â†’ Select Experts â†’ Combine Outputs

Example:
Input: "Translate to French"
Router activates: Translation expert

Input: "Solve math problem"
Router activates: Math expert

Benefits:
âœ“ Higher capacity
âœ“ Lower inference cost (activate subset)
âœ“ Specialization
```

### 2. Retrieval-Enhanced Models

```
Combine parametric knowledge (model weights) with
non-parametric knowledge (retrieved documents)

RETRO, Atlas, etc.
```

### 3. Constitutional AI Scaling

```
As models get larger:
- More powerful
- Harder to align

Solutions:
- Better RLHF
- Constitutional AI
- Red teaming at scale
```

### 4. Multimodal Everything

```
Future models:
Text + Image + Audio + Video + 3D + Code

Unified representation
Cross-modal reasoning
```

---

## Key Takeaways

### Diffusion Models
1. **Diffusion models** gradually denoise noise into images
2. **Stable Diffusion** works in compressed latent space for efficiency
3. **Classifier-free guidance** controls adherence to text prompts

### Multimodal AI
4. **CLIP** learns joint text-image embeddings for zero-shot classification
5. **Vision-language models** (BLIP, LLaVA) enable visual question answering
6. **Whisper** provides robust multilingual speech recognition

### NLP Tasks
7. **NER** extracts named entities (persons, organizations, locations)
8. **RAD** extends RAG to multi-turn conversations with context
9. **Coreference resolution** resolves pronouns across dialogue

### Libraries
10. **NumPy** for numerical computing, **Pandas** for data manipulation
11. **PyTorch/TensorFlow** for deep learning
12. **Transformers library** provides easy access to pretrained models

### Computer Vision
13. **CNNs** use convolutional layers for image understanding
14. **Object detection** (YOLO, R-CNN) locates objects in images
15. **Image segmentation** labels pixels (semantic) or instances

### Audio
16. **ASR** (Whisper) converts speech to text with high accuracy
17. **TTS** synthesizes natural-sounding speech from text
18. **Speaker diarization** identifies who spoke when

### Graphs
19. **GNNs** learn on graph-structured data via message passing
20. Used for molecular property prediction, social networks, knowledge graphs

---

## Practice Questions

1. Explain how diffusion models generate images through the reverse process.

2. What is the difference between semantic and instance segmentation?

3. How does CLIP enable zero-shot image classification?

4. What are the benefits of RAD over standard RAG for chatbots?

5. Implement NER to extract person and organization names from text.

6. Compare CNNs and Vision Transformers for image classification.

7. How does Whisper handle multiple languages in speech recognition?

8. What are the advantages of Mixture of Experts architectures?

9. Design a multimodal system for visual question answering.

10. Explain how Graph Neural Networks aggregate neighbor information.

---

## Related Modules

- **[O3: Generative AI and LLMs](O3.Generative_AI_and_LLM.md)** - Text generation models
- **[O4: Transformer Architecture](O4.Transformer_Architecture.md)** - Foundation for many models here
- **[O6: Model Customization](O6.Model_Customization.md)** - RAD builds on RAG
- **[O7: Model Training](O7.Model_Training.md)** - Training techniques for all models
- **[O8: NVIDIA Ecosystem](O8.NVIDIA_Ecosystem.md)** - Tools for deploying these models

---

**Next Module**: [README](README.md) - Certification Overview

# O7: Model Training and Evaluation

## Table of Contents
- [Introduction to Model Training](#introduction-to-model-training)
- [Fine-Tuning Approaches](#fine-tuning-approaches)
- [Parameter-Efficient Fine-Tuning (PEFT)](#parameter-efficient-fine-tuning-peft)
- [Data Preparation](#data-preparation)
- [Training Process](#training-process)
- [Model Evaluation](#model-evaluation)
- [Evaluation Metrics](#evaluation-metrics)
- [Benchmarks](#benchmarks)
- [Deployment Considerations](#deployment-considerations)
- [Key Takeaways](#key-takeaways)
- [Practice Questions](#practice-questions)

---

## Introduction to Model Training

### When to Fine-Tune

```
Use fine-tuning when:
✓ Domain-specific terminology (medical, legal)
✓ Specific style or tone needed
✓ Task model wasn't trained for
✓ Consistent format required
✓ On-premise deployment (incorporate knowledge)

Alternatives:
- Prompt engineering: Free, fast
- RAG: External knowledge, no training
- API with prompting: Easiest

Fine-tuning trade-offs:
Pros:
✓ Better performance on specific tasks
✓ Incorporate domain knowledge permanently
✓ Consistent outputs
✓ Can use smaller models

Cons:
✗ Requires training data (100s-1000s examples)
✗ Compute cost
✗ Time to train
✗ Need retraining for updates
```

### Training Data Requirements

```
Data quantity by method:

Full fine-tuning:
- Minimum: 10,000+ examples
- Recommended: 100,000+
- Risk of catastrophic forgetting with less

LoRA/PEFT:
- Minimum: 100-1,000 examples
- Recommended: 1,000-10,000
- More efficient, less data needed

Instruction tuning:
- 1,000-50,000 instruction-response pairs
- Quality > quantity
- Diverse tasks important

Data quality:
✓ High-quality labels
✓ Representative of use case
✓ Diverse examples
✓ Clean, consistent format
```

---

## Fine-Tuning Approaches

### Full Fine-Tuning

```
Update all model parameters

Process:
1. Start with pre-trained model
2. Train on task-specific data
3. Update all weights

Example (7B model):
- Parameters: 7 billion
- All 7B updated during training
- Memory: ~100 GB (model + gradients + optimizer)
- Training: Hours-days on multiple GPUs

Pros:
✓ Best possible performance
✓ Full model adaptation

Cons:
✗ Expensive (memory, compute)
✗ Risk of catastrophic forgetting
✗ Requires lots of data
✗ Slow

When to use:
- Abundant training data
- Very different domain
- Maximum performance needed
- Have resources
```

### Instruction Tuning

```
Fine-tune to follow instructions

Training data format:
{
  "instruction": "Translate to French:",
  "input": "Hello, how are you?",
  "output": "Bonjour, comment allez-vous?"
}

Dataset examples:
- Alpaca (52K instruction-following examples)
- Dolly (15K human-generated instructions)
- FLAN (mixture of instruction tasks)

Process:
1. Collect instruction-response pairs
2. Fine-tune base model
3. Model learns to follow instructions

Result:
Base model → Can't follow instructions well
Instruction-tuned → Follows various instructions

Examples: Llama 2 → Llama 2 Chat
```

### Supervised Fine-Tuning (SFT)

```
Fine-tune with specific task data

Use cases:
- Classification (sentiment, intent)
- Named entity recognition
- Question answering
- Summarization

Example (sentiment):
Training data:
{"text": "This movie was great!", "label": "positive"}
{"text": "Terrible experience.", "label": "negative"}

Process:
1. Format data for task
2. Fine-tune model
3. Model learns task-specific patterns

Result: Specialized model for that task
```

---

## Parameter-Efficient Fine-Tuning (PEFT)

### Why PEFT?

```
Challenge: Full fine-tuning is expensive

PEFT solution:
- Freeze base model weights
- Train small additional parameters
- 0.1-1% of full model size

Benefits:
✓ 10-100x less memory
✓ Faster training
✓ Less data needed
✓ Multiple task-specific adapters
✓ No catastrophic forgetting

Trade-off:
- Slightly lower performance than full fine-tuning
- But 90-95% of performance with 1% of parameters!
```

### LoRA (Low-Rank Adaptation)

```
Key idea: Updates are low-rank

Instead of updating W:
W_new = W + ΔW (where ΔW is full-rank)

LoRA uses:
W_new = W + B × A
where B, A are low-rank matrices

Example:
W: 4096 × 4096 (16M parameters)
B: 4096 × 8
A: 8 × 4096
B × A: Only 65K parameters (0.4% of original!)

Architecture:
Input
  ↓
┌──────────┐     ┌──────┐
│ Frozen W │ +   │ LoRA │
│ (4096×   │     │ B×A  │
│  4096)   │     │(65K) │
└─────┬────┘     └───┬──┘
      └──────┬───────┘
             ↓
          Output

Only train B and A, freeze W
```

**LoRA Implementation**:

```python
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM

# Load base model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# LoRA configuration
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                    # Rank (higher = more parameters)
    lora_alpha=32,          # Scaling factor
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"]  # Which layers to adapt
)

# Create LoRA model
model = get_peft_model(model, lora_config)

# Print trainable parameters
model.print_trainable_parameters()
# trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%

# Train as normal
trainer = Trainer(model=model, ...)
trainer.train()

# Save only LoRA weights (tiny!)
model.save_pretrained("./lora_weights")  # ~20 MB instead of 14 GB
```

**LoRA Hyperparameters**:

```python
# r (rank): LoRA rank
r = 4:    Very memory efficient, lower quality
r = 8:    Good balance (common default)
r = 16:   Higher quality, more memory
r = 64:   Approaching full fine-tuning

# lora_alpha: Scaling
alpha = r:      Standard scaling
alpha = 2*r:    Higher scaling (common)

# target_modules: Which layers to adapt
["q_proj", "v_proj"]:               Attention only (faster)
["q_proj", "k_proj", "v_proj"]:     All attention projections
[...all linear layers...]:           Maximum adaptation
```

### QLoRA (Quantized LoRA)

```
Combine LoRA + quantization

Key innovation:
- Load base model in 4-bit
- Train LoRA adapters in full precision
- Massive memory savings

Example (Llama 2 7B):
Full precision: 28 GB
QLoRA (4-bit + adapters): 6 GB

Enables fine-tuning 70B models on single GPU!
```

**QLoRA Implementation**:

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model

# Quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                    # 4-bit quantization
    bnb_4bit_quant_type="nf4",            # Normal float 4-bit
    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bf16
    bnb_4bit_use_double_quant=True        # Double quantization
)

# Load model in 4-bit
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config,
    device_map="auto"
)

# Prepare for training
model = prepare_model_for_kbit_training(model)

# LoRA config
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Add LoRA
model = get_peft_model(model, lora_config)

# Train!
# Base model: 4-bit (read-only)
# LoRA adapters: Full precision (trainable)
```

### Prefix Tuning

```
Add trainable prefix vectors

Architecture:
[Prefix tokens] [Input tokens]
    ↓               ↓
[Trainable]     [Frozen embeddings]

Example:
Original: "Translate: Hello" → Embedding
Prefix tuning: [P1][P2][P3] "Translate: Hello"

Only train [P1], [P2], [P3] embeddings
Rest of model frozen

Pros:
✓ Very few parameters
✓ Different prefixes for different tasks

Cons:
✗ Reduces effective context length
✗ Less flexible than LoRA
```

### Adapter Layers

```
Add small trainable layers

Architecture:
Input
  ↓
Frozen Transformer Layer
  ↓
Adapter (trainable)
  ↓
Output

Adapter structure:
Input (d_model=768)
  ↓
Down-project (d_model → 64)
  ↓
Non-linearity (ReLU)
  ↓
Up-project (64 → d_model)
  ↓
Output + Input (residual)

Parameters:
768 → 64: 49,152
64 → 768: 49,152
Total: ~98K per adapter (vs 110M in BERT)

Pros:
✓ Modular (swap adapters)
✓ Few parameters

Cons:
✗ Increases inference latency
✗ Not as efficient as LoRA
```

### PEFT Comparison

| Method | Parameters | Memory | Performance | Inference |
|--------|------------|--------|-------------|-----------|
| **Full FT** | 100% | High | Best | Fast |
| **LoRA** | 0.1-1% | Low | ~95% | Fast |
| **QLoRA** | 0.1-1% | Very low | ~95% | Fast |
| **Prefix** | <0.1% | Very low | ~90% | Slower |
| **Adapters** | ~1% | Low | ~93% | Slower |

**Recommendation**: LoRA or QLoRA for most use cases

---

## Data Preparation

### Data Format

```
Causal Language Modeling (GPT-style):

{"text": "Question: What is AI? Answer: Artificial Intelligence is..."}

Instruction format:
{
  "instruction": "Explain what AI is",
  "input": "",
  "output": "Artificial Intelligence is..."
}

Conversational format:
{
  "messages": [
    {"role": "user", "content": "What is AI?"},
    {"role": "assistant", "content": "Artificial Intelligence is..."}
  ]
}

Classification:
{
  "text": "This product is amazing!",
  "label": "positive"
}
```

### Data Quality

```
Quality checklist:

✓ Accurate labels/outputs
✓ Consistent formatting
✓ Representative samples
✓ Diverse examples
✓ Deduplication
✓ Balance across categories
✓ Remove sensitive information

Common issues:
✗ Label noise (wrong labels)
✗ Inconsistent formats
✗ Duplicate examples
✗ Underrepresented categories
✗ Leaked test data
✗ Copyrighted content
```

### Dataset Preparation Example

```python
import pandas as pd
from datasets import Dataset

# Load data
df = pd.read_csv("training_data.csv")

# Clean
df = df.drop_duplicates()
df = df.dropna()

# Format for instruction tuning
def format_example(row):
    return {
        "instruction": "Classify the sentiment",
        "input": row["text"],
        "output": row["sentiment"]
    }

formatted_data = df.apply(format_example, axis=1).tolist()

# Create dataset
dataset = Dataset.from_list(formatted_data)

# Split
from datasets import DatasetDict

dataset = dataset.train_test_split(test_size=0.1)
dataset_dict = DatasetDict({
    "train": dataset["train"],
    "validation": dataset["test"]
})

# Tokenize
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

def tokenize_function(examples):
    # Combine instruction + input + output
    texts = [
        f"### Instruction:\n{inst}\n\n### Input:\n{inp}\n\n### Response:\n{out}"
        for inst, inp, out in zip(
            examples["instruction"],
            examples["input"],
            examples["output"]
        )
    ]
    return tokenizer(texts, truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)
```

---

## Training Process

### Training Setup

```python
from transformers import TrainingArguments, Trainer

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,  # Effective batch size = 4×4 = 16
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    eval_steps=500,
    evaluation_strategy="steps",
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    bf16=True,                      # Use bfloat16 for training
    gradient_checkpointing=True,    # Save memory
    optim="paged_adamw_8bit"        # Memory-efficient optimizer
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer
)

# Train
trainer.train()

# Save
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")
```

### Hyperparameter Tuning

```
Key hyperparameters:

Learning rate:
- Too high: Unstable, diverges
- Too low: Slow convergence, local minima
- Typical: 1e-5 to 5e-4
- LoRA: Higher (1e-4 to 5e-4)
- Full FT: Lower (1e-6 to 1e-5)

Batch size:
- Larger: More stable, faster (if fits in memory)
- Smaller: More gradient updates
- Typical: 8-32 effective (with gradient accumulation)

Epochs:
- Too few: Underfitting
- Too many: Overfitting
- Typical: 1-5 epochs
- Monitor validation loss

Warmup:
- Gradually increase learning rate
- Prevents instability at start
- Typical: 5-10% of total steps

Weight decay:
- Regularization
- Typical: 0.01
```

### Monitoring Training

```python
# TensorBoard logging
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter("runs/fine_tuning")

# In training loop
for epoch in range(num_epochs):
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss

        # Log metrics
        writer.add_scalar("Loss/train", loss, step)
        writer.add_scalar("Learning_rate", optimizer.param_groups[0]["lr"], step)

        # Backward pass
        loss.backward()
        optimizer.step()

    # Validation
    val_loss = validate(model, val_dataloader)
    writer.add_scalar("Loss/validation", val_loss, epoch)

# Launch TensorBoard:
# tensorboard --logdir=runs

# Monitor:
# - Training loss (should decrease)
# - Validation loss (should decrease, then plateau)
# - Gap between train/val (overfitting if large)
```

### Early Stopping

```python
# Stop when validation loss stops improving

class EarlyStopping:
    def __init__(self, patience=3, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')

    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False  # Continue training
        else:
            self.counter += 1
            if self.counter >= self.patience:
                return True  # Stop training
            return False

# Usage
early_stopping = EarlyStopping(patience=3)

for epoch in range(max_epochs):
    train_loss = train_one_epoch()
    val_loss = validate()

    if early_stopping(val_loss):
        print(f"Early stopping at epoch {epoch}")
        break
```

---

## Model Evaluation

### Evaluation Strategies

```
1. Hold-out validation:
   Train: 80%
   Validation: 10%
   Test: 10%

2. K-fold cross-validation:
   Split into K folds
   Train on K-1, validate on 1
   Repeat K times
   Average results

3. Stratified sampling:
   Maintain class distribution
   Important for imbalanced data

4. Time-based split:
   Train on older data
   Test on newer data
   For time-series or temporal data
```

### Evaluation Metrics

#### Classification

```python
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    classification_report,
    confusion_matrix
)

# Predictions
y_true = [0, 1, 1, 0, 1, 0, 1, 1]
y_pred = [0, 1, 0, 0, 1, 0, 1, 1]

# Accuracy
accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy:.3f}")  # 0.875

# Precision, Recall, F1
precision, recall, f1, support = precision_recall_fscore_support(
    y_true, y_pred, average='binary'
)

print(f"Precision: {precision:.3f}")  # 1.000 (all predicted 1s are correct)
print(f"Recall: {recall:.3f}")        # 0.800 (80% of 1s found)
print(f"F1: {f1:.3f}")               # 0.889 (harmonic mean)

# Classification report
print(classification_report(y_true, y_pred))
#               precision    recall  f1-score   support
#            0       0.75      1.00      0.86         3
#            1       1.00      0.80      0.89         5
#     accuracy                           0.88         8
#    macro avg       0.88      0.90      0.87         8
# weighted avg       0.91      0.88      0.88         8

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)
#     [[3 0]     TN FP
#      [1 4]]    FN TP
```

#### Generation Tasks

```python
# BLEU (Bilingual Evaluation Understudy)
# For translation, summarization

from sacrebleu import corpus_bleu

references = [["The cat sits on the mat"]]
hypotheses = ["The cat is on the mat"]

bleu = corpus_bleu(hypotheses, references)
print(f"BLEU: {bleu.score:.2f}")  # 0-100 scale

# Higher is better
# BLEU considers n-gram overlap
# Precision-focused

# ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
# For summarization

from rouge import Rouge

rouge = Rouge()
scores = rouge.get_scores(
    "The cat sits on the mat",
    "The cat is on the mat"
)

print(scores)
# [{'rouge-1': {'f': 0.8571, 'p': 0.8571, 'r': 0.8571},
#   'rouge-2': {'f': 0.6666, 'p': 0.6666, 'r': 0.6666},
#   'rouge-l': {'f': 0.8571, 'p': 0.8571, 'r': 0.8571}}]

# rouge-1: Unigram overlap
# rouge-2: Bigram overlap
# rouge-l: Longest common subsequence
# Higher is better (0-1 scale)

# Perplexity
# How "surprised" model is by text
# Lower is better

import torch

# Calculate perplexity on test set
test_loss = 0
model.eval()

with torch.no_grad():
    for batch in test_dataloader:
        outputs = model(**batch)
        test_loss += outputs.loss.item()

perplexity = torch.exp(torch.tensor(test_loss / len(test_dataloader)))
print(f"Perplexity: {perplexity:.2f}")

# Lower perplexity = better fit to data
# GPT-3 on web text: ~20
# Specialized model: ~10-15
```

### Human Evaluation

```
Automated metrics don't capture everything

Human evaluation dimensions:

1. Relevance:
   Does answer address the question?
   Scale: 1-5

2. Coherence:
   Is output logical and consistent?
   Scale: 1-5

3. Fluency:
   Is language natural and grammatical?
   Scale: 1-5

4. Factuality:
   Are facts correct?
   Scale: 1-5

5. Helpfulness:
   Does it help the user?
   Scale: 1-5

Process:
1. Sample outputs (100-1000)
2. Multiple annotators per sample
3. Aggregate scores (mean, majority vote)
4. Calculate inter-annotator agreement
5. Compare models

Example:
Model A avg: 4.2/5
Model B avg: 3.8/5
→ Model A preferred
```

---

## Benchmarks

### Language Understanding

#### MMLU (Massive Multitask Language Understanding)

```
57 subjects across:
- STEM (physics, chemistry, math)
- Humanities (history, philosophy)
- Social sciences (economics, psychology)
- Professional (law, medicine)

Format: Multiple choice
Questions: 15,908
Evaluation: Few-shot (5 examples)

Example:
Q: What is the capital of France?
A) London B) Berlin C) Paris D) Madrid

Scores:
GPT-4: 86.4%
Claude 2: 78.5%
Llama 2 70B: 69.8%
Llama 2 7B: 45.9%
```

#### HellaSwag

```
Commonsense reasoning
Sentence completion

Example:
A woman is outside with a bucket and a dog. The dog is running around trying to avoid a bath. She...
a) rinses the dog off with water
b) uses a hose to spray water at the dog
c) starts to pet the dog
d) is seen walking across a hall

Correct: (a) or (b) - both plausible
Incorrect: (d) - doesn't make sense in context

Scores (higher is better):
GPT-4: 95.3%
Claude 2: 85.9%
Llama 2 70B: 87.3%
```

### Code Generation

#### HumanEval

```
164 Python programming problems

Example:
def add(a, b):
    """Return the sum of a and b."""
    pass

Test cases:
assert add(1, 2) == 3
assert add(0, 0) == 0

Metric: pass@k (% that pass tests)

Scores (pass@1):
GPT-4: 67.0%
GPT-3.5: 48.1%
CodeLlama 34B: 53.7%
Llama 2 70B: 30.5%
```

### Reasoning

#### GSM8K (Grade School Math)

```
8,500 grade school math word problems

Example:
"Roger has 5 tennis balls. He buys 2 more cans of tennis balls.
Each can has 3 tennis balls. How many tennis balls does he have now?"

Correct answer: 11
Reasoning: 5 + (2 × 3) = 5 + 6 = 11

Scores (with chain-of-thought):
GPT-4: 92.0%
Claude 2: 88.0%
Llama 2 70B: 56.8%
```

### Truthfulness

#### TruthfulQA

```
817 questions designed to elicit false answers

Example:
Q: "What happens if you crack your knuckles a lot?"
Common misconception: "You'll get arthritis"
Correct: "Nothing harmful"

Scores (% truthful):
GPT-4: 60%
Claude 2: 62%
GPT-3.5: 47%

Measures resistance to falsehoods and misconceptions
```

### Running Benchmarks

```python
# Using lm-evaluation-harness

from lm_eval import evaluator

results = evaluator.simple_evaluate(
    model="hf-causal",
    model_args="pretrained=meta-llama/Llama-2-7b-hf",
    tasks=["mmlu", "hellaswag", "truthfulqa"],
    num_fewshot=5,
    batch_size=8
)

print(results["results"])
# {
#   "mmlu": {"acc": 0.459},
#   "hellaswag": {"acc_norm": 0.776},
#   "truthfulqa_mc": {"mc2": 0.410}
# }
```

---

## Deployment Considerations

### Model Compression

```
Techniques to reduce model size:

1. Quantization:
   FP16 → INT8 → INT4
   Trade accuracy for size/speed

2. Pruning:
   Remove less important weights
   Structured (entire neurons) or unstructured

3. Distillation:
   Train small model to mimic large model
   Student-teacher learning

4. Knowledge Distillation:
   Large model (teacher) → Small model (student)
   Student learns from teacher's outputs

Example (DistilBERT):
BERT: 110M params, 100% performance
DistilBERT: 66M params, 97% performance
→ 40% smaller, 60% faster
```

### Inference Optimization

```
Techniques for faster inference:

1. Caching:
   Cache KV pairs during generation
   3-5x speedup

2. Batching:
   Process multiple requests together
   Higher throughput

3. Quantization:
   INT8 inference
   2-3x faster

4. ONNX Runtime:
   Optimized inference engine
   Cross-platform

5. TensorRT:
   NVIDIA optimization
   Layer fusion, kernel tuning
   Up to 10x faster

6. Compilation:
   TorchScript, TorchDynamo
   Optimize computation graph
```

### Versioning and Monitoring

```
Production best practices:

1. Model versioning:
   - Tag releases (v1.0, v1.1)
   - Track training data version
   - Reproducibility (random seeds, configs)

2. A/B testing:
   - Deploy new model to % of traffic
   - Compare metrics
   - Gradual rollout

3. Monitoring:
   - Latency (p50, p95, p99)
   - Throughput (requests/sec)
   - Error rate
   - Model quality metrics

4. Drift detection:
   - Input distribution changes
   - Performance degradation
   - Retrain trigger

5. Logging:
   - Input/output pairs
   - Confidence scores
   - User feedback
   - Error analysis
```

---

## Key Takeaways

### Fine-Tuning Approaches

1. **Full fine-tuning** updates all parameters (expensive, best performance)

2. **Instruction tuning** teaches models to follow instructions

3. **LoRA** trains 0.1-1% of parameters with minimal quality loss

4. **QLoRA** enables 70B model fine-tuning on single GPU

5. **PEFT** methods are 10-100x more memory-efficient

### Data and Training

6. **Data quality** matters more than quantity

7. **100-1,000 examples** sufficient for LoRA

8. **Learning rate** higher for LoRA (1e-4) than full FT (1e-6)

9. **Early stopping** prevents overfitting

10. **Gradient checkpointing** saves memory during training

### Evaluation

11. **Accuracy** for classification, **F1** when classes imbalanced

12. **BLEU/ROUGE** for generation (translation, summarization)

13. **Perplexity** measures how well model predicts text (lower better)

14. **MMLU** tests knowledge across 57 subjects

15. **HumanEval** measures code generation ability

### Metrics Details

16. **Precision** = TP/(TP+FP) - "How many predicted positives are correct?"

17. **Recall** = TP/(TP+FN) - "How many actual positives did we find?"

18. **F1-score** = harmonic mean of precision and recall

19. **Human evaluation** necessary for quality, helpfulness, safety

20. **Benchmarks** provide standardized comparison across models

---

## Practice Questions

1. When should you use fine-tuning vs RAG vs prompting?

2. Compare full fine-tuning, LoRA, and QLoRA (memory, performance, use cases).

3. How many training examples do you need for LoRA fine-tuning?

4. Explain how LoRA reduces the number of trainable parameters.

5. What learning rate would you use for LoRA? For full fine-tuning?

6. Design a data preparation pipeline for instruction tuning.

7. Calculate precision, recall, and F1 given TP=80, FP=20, FN=10, TN=90.

8. What is the difference between BLEU and ROUGE?

9. How would you detect overfitting during training?

10. Explain the purpose of early stopping.

11. What metrics would you use to evaluate a summarization model?

12. Compare MMLU and HellaSwag benchmarks.

13. How does quantization affect model performance?

14. Design an evaluation strategy for a customer support chatbot.

15. What monitoring metrics would you track for a deployed LLM?

---

## Related Modules

- **[O2: AI/ML Fundamentals](O2.AI_ML_Fundamentals.md)** - Training fundamentals (backprop, optimization)
- **[O3: Generative AI and LLMs](O3.Generative_AI_and_LLM.md)** - Pre-training vs fine-tuning
- **[O5: Model Selection](O5.Model_Selection.md)** - Choosing base model to fine-tune
- **[O6: Model Customization](O6.Model_Customization.md)** - RAG as alternative to fine-tuning
- **[O8: NVIDIA Ecosystem](O8.NVIDIA_Ecosystem.md)** - NeMo for training, TensorRT for inference
- **[O1: AI Infrastructure](O1.AI_Infrastructure.md)** - GPU requirements for training

---

**Next Module**: [README - Certification Overview](README.md)

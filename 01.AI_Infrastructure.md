# AI Infrastructure



## Table of Contents
- [AI / ML Tech Stack](#ai--ml-tech-stack)
- [Introduction to AI Infrastructure](#introduction-to-ai-infrastructure)
- [GPU Architecture](#gpu-architecture)
- [NVIDIA GPU Lineup](#nvidia-gpu-lineup)
- [Grace CPU Architecture](#grace-cpu-architecture)
- [DGX Systems](#dgx-systems)
- [Memory Hierarchy](#memory-hierarchy)
- [Distributed Training Infrastructure](#distributed-training-infrastructure)
- [Data Center Considerations](#data-center-considerations)
- [Cloud Infrastructure](#cloud-infrastructure)
- [Key Takeaways](#key-takeaways)
- [Practice Questions](#practice-questions)

---

## AI / ML Tech Stack

The modern AI/ML ecosystem is built on multiple interconnected layers, each serving a critical role in the development and deployment of AI applications.

<p align="center">

```
                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚         APPLICATION                     â”‚
                                       â”‚     Interface â€¢ Integration             â”‚
                                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                       â”‚        ORCHESTRATION                    â”‚
                                       â”‚      MLOps â€¢ Workflows                  â”‚
                                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                       â”‚            DATA                         â”‚
                                       â”‚   Public Dataset â€¢ Private Dataset      â”‚
                                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                       â”‚            MODEL                        â”‚
                                       â”‚    Open Source â€¢ Proprietary            â”‚
                                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                       â”‚      AI INFRASTRUCTURE                  â”‚
                                       â”‚       On-Prem â€¢ Cloud                   â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</p>

### Stack Layers Explained

**1. AI Infrastructure (Foundation)**
```
The hardware and compute foundation:
- On-Premise: DGX systems, custom GPU clusters
- Cloud: AWS, Azure, GCP GPU instances
- Provides: Compute, storage, networking
```

**2. Model Layer**
```
Pre-trained and custom models:
- Open Source: Llama, Mistral, Falcon, GPT-J
- Proprietary: GPT-4, Claude, Gemini, PaLM
- Foundation models for various tasks
```

**3. Data Layer**
```
Training and inference data:
- Public Datasets: ImageNet, COCO, Common Crawl
- Private Datasets: Proprietary business data
- Data pipelines, preprocessing, versioning
```

**4. Orchestration Layer**
```
Workflow and lifecycle management:
- MLOps: Model training, versioning, monitoring
- Workflows: Data pipelines, automated retraining
- Tools: Kubeflow, MLflow, DVC, Airflow
```

**5. Application Layer**
```
User-facing applications:
- Interface: Web apps, APIs, chatbots
- Integration: Enterprise systems, databases
- Deployment: Serving infrastructure
```

### Why This Stack Matters

```
Each layer depends on the foundation below it:
- Applications at the top deliver value to end users
- Orchestration manages the workflow and lifecycle
- Data feeds and improves the models
- Models are trained and run on infrastructure
- Infrastructure at the bottom provides compute power for everything

Optimizing any single layer improves overall performance.
```

---

## Introduction to AI Infrastructure

<p align="center">
<img width="544" height="231" alt="image" src="https://github.com/user-attachments/assets/0ffc889a-049e-4031-bdbf-868754277cfb" />
</p>

### Why GPU?

GPUs were first created to handle complex graphics rendering in video games and applications, so images, animations, and 3D environments could be displayed smoothly and realistically. Over time, researchers discovered that the same parallel processing architecture that made GPUs excellent for graphics could also accelerate scientific computing and machine learning workloads. This led to the evolution of **General-Purpose GPU (GPU)** computing.

**GPU Evolution Timeline:**

```
1999-2006: Graphics-Only GPUs
- Fixed-function graphics pipeline
- Optimized for gaming and rendering
- No programmable compute capabilities

2006-2012: GPU Era Begins
- NVIDIA introduces CUDA (2006)
- Programmable parallel computing on GPUs
- Researchers use GPUs for scientific simulations
- Early deep learning experiments on GPUs

2012-2017: AI Breakthrough
- AlexNet wins ImageNet (2012) using 2 GPUs
- Deep learning revolution begins
- GPUs become essential for neural networks
- First Tensor Cores introduced (Volta, 2017)

2017-Present: AI-Optimized GPUs
- Tensor Cores for matrix operations
- Mixed precision training (FP16, TF32, FP8)
- Transformer Engine for LLMs
- Dedicated AI accelerators in every GPU
- Multi-Instance GPU for efficient sharing
```

**Why GPUs Excel at AI:**
- **Neural networks** = massive matrix multiplications
- GPUs have thousands of cores for parallel math
- Tensor Cores accelerate AI operations 8-10x
- High memory bandwidth for large models
- Optimized for the same operations repeated billions of times


### Why Specialized Hardware for AI?

```
Traditional CPU:
- Optimized for sequential processing
- Complex control logic
- Few powerful cores (8-64)
- Low arithmetic throughput

AI Workloads:
- Massive parallelism (matrix operations)
- Simple operations repeated billions of times
- High memory bandwidth needs
- Floating-point intensive

Solution: GPUs optimized for AI
```

### Hardware Evolution for AI

```
2012: AlexNet on 2 GPUs (ImageNet breakthrough)
2017: Transformer introduced (attention mechanisms)
2020: GPT-3 (175B params) trained on thousands of GPUs
2023: GPT-4, Llama 2, Claude - require massive infrastructure
2024: Training 1T+ parameter models
```

### AI Compute Requirements

```
Model Size vs Compute:

Small Model (BERT-Base, 110M params):
- Training: 1-8 GPUs, hours-days
- Inference: CPU or single GPU

Medium Model (GPT-2, 1.5B params):
- Training: 8-64 GPUs, days-weeks
- Inference: Single GPU

Large Model (GPT-3, 175B params):
- Training: 1000s of GPUs, weeks-months
- Inference: Multiple GPUs

Frontier Model (GPT-4, 1T+ params estimated):
- Training: 10,000+ GPUs/TPUs, months
- Inference: Multi-GPU clusters
```

---

## GPU Architecture

<p align="center">
<img width="800" height="391" alt="image" src="https://github.com/user-attachments/assets/174b9d39-085d-4ea3-8cb4-7b221f958eea" />
</p>

### Architecture Components Explained

### CPU Components:


**Core:**
- The processing unit that executes instructions
- Handles complex sequential tasks efficiently
- Each core can run independent threads

**Control:**
- Manages instruction flow and execution
- Complex logic for branch prediction, out-of-order execution
- Coordinates between different CPU components

**L1 Cache:**
- Smallest, fastest cache (per-core)
- Typically 32-64 KB per core
- ~4 cycle latency

**L2 Cache:**
- Medium cache (per-core or shared)
- Typically 256 KB - 1 MB per core
- ~12 cycle latency

**L3 Cache:**
- Largest, shared across all cores
- Typically 8-128 MB total
- ~40 cycle latency
- Reduces DRAM access

**DRAM:**
- Main system memory (RAM)
- Much larger capacity (GB-TB range)
- Higher latency (~100-300 cycles)
- Stores active programs and data

----



### CPU vs GPU

```
CPU Design:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Core 1  â”‚ (Complex, powerful)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Core 2  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ...    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Core N  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Few cores (~64 max)
High per-core performance
Complex control logic

GPU Design:
â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”
â”‚Câ”‚Câ”‚Câ”‚Câ”‚Câ”‚Câ”‚Câ”‚Câ”‚
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”‚Câ”‚Câ”‚Câ”‚Câ”‚Câ”‚Câ”‚Câ”‚Câ”‚
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”‚ ... thousands...â”‚
â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜
Thousands of cores
Lower per-core performance
Optimized for parallel math
```

### GPU Components

#### 1. Streaming Multiprocessors (SMs) 

- Fundamental processing unit in GPU . A main working unit inside GPU
- Contains multiple CUDA cores, Tensor Cores
- Has own shared memory and registers
- Executes groups of threads (warps) in parallel

**A GPU has:**

 - Many SMs
 - Each SM works independently
 - Each SM contains CUDA cores, Tensor cores, cache
  
```
GPU = Multiple SMs
Each SM contains:
- CUDA cores (general compute)
- Tensor Cores (AI accelerators)
- Shared memory (fast local cache)
- Register file
- Warp schedulers

H100 GPU:
- 132 SMs
- 128 CUDA cores per SM
- Total: 16,896 CUDA cores
```

#### 2. CUDA Cores

- Basic compute units for parallel processing **(These are the basic math workers)**
- Execute floating-point and integer operations
- Thousands per GPU (16,896 in H100)

```
Standard floating-point and integer operations:
- FP32 (32-bit float)
- FP64 (64-bit float)
- INT32 (32-bit integer)

Used for:
- General computation
- Control flow
- Data movement
```

#### 3. Tensor Cores

- Special cores made ONLY for AI / deep learning
- Specialized for matrix multiplication
- Tensor cores = ğŸ§  AI specialist workers
- 8-10x faster than CUDA cores for AI workloads
- Supports mixed precision (FP8, FP16, TF32)

```
Specialized matrix multiply-accumulate units:

Operation: D = A Ã— B + C
Where A, B, C, D are matrices

Speed: 8-10x faster than CUDA cores for matrix operations

Supported precisions:
- FP64 (64-bit float)
- TF32 (19-bit TensorFloat)
- FP16 (16-bit float)
- BF16 (16-bit bfloat)
- FP8 (8-bit float) - H100
- INT8 (8-bit integer)

AI workloads:
âœ“ Matrix multiplication (linear layers)
âœ“ Convolutions
âœ“ Attention mechanisms
```

#### 4. Memory Hierarchy

**L1 Cache:**
- Per-SM fast cache
- Typically 128-256 KB per SM
- Combined with shared memory
- Stores frequently used data

**L2 Cache:**
- Shared memory for entire GPU
- Shared across all SMs
- 40-60 MB in modern GPUs
- Reduces HBM access

**HBM (High Bandwidth Memory):**
- GPU's main memory (VRAM)
- Stores models (like LLM weights)
- Stores input data (images, tokens)
- Very fast compared to normal RAM
- Huge bandwidth (can move LOTS of data per second)
- 40-80 GB capacity
- 2-3 TB/s bandwidth
- Stacked DRAM on interposer for higher speed

```
                                       HBM (big memory)
                                             â†“
                                 L2 cache (shared fast memory)
                                             â†“
                                 L1 cache (local fast memory)
                                             â†“
                                       SM ( Shared Memory)
                                             â†“
                                 CUDA cores / Tensor cores (workers)
```

```
Fastest (Smallest):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Registers        â”‚ ~256 KB per SM
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Shared Memory    â”‚ ~100-200 KB per SM
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L1 Cache         â”‚ ~128 KB per SM
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L2 Cache         â”‚ 50-60 MB (whole GPU)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ HBM (VRAM)       â”‚ 40-80 GB
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Slowest (Largest)

Access Times:
Registers: 1 cycle
Shared Memory: ~5 cycles
L1 Cache: ~30 cycles
L2 Cache: ~200 cycles
HBM: ~300-600 cycles
```
### Memory Pooling

Memory pooling is a technique that overcomes the limited throughput between CPU and GPU by reusing pre-allocated memory buffers.

**Without Memory Pool:**
```
Problem: Frequent allocation and deallocation
- Each operation requests new memory from OS
- Memory allocation is slow (system calls)
- GPU waits for CPU to allocate memory
- Limited CPU-GPU bandwidth becomes bottleneck

Example workflow:
1. Request 1GB for layer 1 â†’ allocate â†’ use â†’ free
2. Request 2GB for layer 2 â†’ allocate â†’ use â†’ free
3. Request 1GB for layer 3 â†’ allocate â†’ use â†’ free
â±ï¸ Total overhead: 3 allocation + 3 deallocation calls

Bottleneck:
CPU â†â†’ GPU (limited throughput)
Frequent back-and-forth communication
```

**With Memory Pool:**
```
Solution: Pre-allocate large memory pool once
- Allocate large chunk at initialization
- Reuse memory buffers for different operations
- Minimal CPU-GPU communication
- Fast pointer arithmetic instead of system calls

Example workflow:
1. Pre-allocate 10GB pool at start
2. Layer 1 uses 1GB from pool (fast pointer assignment)
3. Layer 2 uses 2GB from pool (fast pointer assignment)
4. Layer 3 reuses 1GB from pool (fast pointer assignment)
â±ï¸ Total overhead: 1 allocation call

Benefits:
âœ“ Reduced allocation overhead
âœ“ Predictable memory usage
âœ“ Better cache locality
âœ“ Minimal CPU-GPU traffic
```
**Real-World Examples:**

```
PyTorch Memory Allocator:
- Caching allocator with memory pools
- torch.cuda.empty_cache() to release unused pool memory
- torch.cuda.memory_allocated() to check pool usage

Example:
import torch
# First allocation creates pool
x = torch.randn(1000, 1000, device='cuda')  # Allocates 4MB + pool overhead
del x  # Memory returned to pool, not OS

# Reuses pool memory (fast!)
y = torch.randn(1000, 1000, device='cuda')  # No new allocation

TensorFlow GPU Memory:
- Pre-allocates most GPU memory upfront
- Uses BFC (Best-Fit with Coalescing) allocator
- Configurable memory growth:
  tf.config.experimental.set_memory_growth(gpu, True)

CUDA Memory Pool:
- cudaMallocAsync() for async memory allocation
- Memory pools per stream
- Reduces synchronization overhead
```

**Performance Impact:**

```
Benchmark: Training ResNet-50 (batch size 64)

Without Memory Pool:
- Allocation time: ~50ms per batch
- Total training time: 120 seconds

With Memory Pool:
- Allocation time: ~1ms per batch
- Total training time: 75 seconds
- Speedup: 1.6x

Memory Pool Trade-offs:

Pros:
âœ“ Much faster allocation (50-100x)
âœ“ Reduced CPU overhead
âœ“ Predictable performance
âœ“ Better GPU utilization

Cons:
âœ— Higher peak memory usage
âœ— Memory not immediately returned to OS
âœ— Potential fragmentation over time
âœ— Harder to debug memory leaks
```
### NVIDIA GPU Architectures

#### Architecture Evolution:

```

Volta (2017):
- V100 GPU
- 1st gen Tensor Cores
- 32GB/16GB HBM2

Ampere (2020):
- A100 GPU
- 3rd gen Tensor Cores
- 80GB/40GB HBM2e
- TF32 precision
- Structural sparsity
- Multi-Instance GPU (MIG)

Hopper (2022):
- H100 GPU
- 4th gen Tensor Cores
- 80GB HBM3
- FP8 precision
- Transformer Engine
- DPX instructions (dynamic programming)
- Confidential computing

Ada Lovelace (2022):
- L40S GPU
- 4th gen Tensor Cores
- 48GB GDDR6
- DLSS 3
- Ray tracing
```

---

## NVIDIA GPU Lineup

### A100 (Ampere)

```
Specifications:
- Architecture: Ampere
- CUDA Cores: 6,912
- Tensor Cores: 432 (3rd gen)
- Memory: 40GB or 80GB HBM2e
- Memory Bandwidth: 1.6 TB/s (40GB) or 2.0 TB/s (80GB)
- TF32: 156 TFLOPS
- FP16: 312 TFLOPS
- Power: 400W

Key Features:
âœ“ Multi-Instance GPU (MIG): Partition into 7 instances
âœ“ 3rd gen NVLink: 600 GB/s
âœ“ PCIe Gen 4
âœ“ Structural sparsity (2:4 sparsity)

Use Cases:
- Training: Medium to large models (7B-70B)
- Inference: High-throughput serving
- Multi-tenant environments (MIG)
- Research and development

```

### H100 (Hopper)

```
Specifications:
- Architecture: Hopper
- CUDA Cores: 16,896
- Tensor Cores: 528 (4th gen)
- Memory: 80GB HBM3
- Memory Bandwidth: 3.35 TB/s
- TF32: 989 TFLOPS
- FP16: 1,979 TFLOPS
- FP8: 3,958 TFLOPS
- Power: 700W

Key Features:
âœ“ FP8 Tensor Cores (2x throughput vs A100)
âœ“ Transformer Engine (automatic FP8 conversion)
âœ“ 4th gen NVLink: 900 GB/s
âœ“ PCIe Gen 5
âœ“ Confidential computing
âœ“ DPX instructions

Use Cases:
- Training: Largest models (100B-1T+)
- Inference: Fastest performance
- Recommendation systems
- Large language models

Performance vs A100:
- Training: 3x faster (FP8)
- Inference: 4-6x faster
- Memory bandwidth: 1.7x higher

```

### L40S (Ada Lovelace)

```
Specifications:
- Architecture: Ada Lovelace
- CUDA Cores: 18,176
- Tensor Cores: 568 (4th gen)
- RT Cores: 142 (3rd gen)
- Memory: 48GB GDDR6
- Memory Bandwidth: 864 GB/s
- TF32: 362 TFLOPS
- FP8: 733 TFLOPS
- Power: 350W

Key Features:
âœ“ Ray tracing hardware
âœ“ NVENC/NVDEC (video encoding/decoding)
âœ“ DLSS 3.0
âœ“ Cost-effective
âœ“ Balanced compute and graphics

Use Cases:
- Inference: Cost-effective deployment
- Graphics + AI hybrid workloads
- Content creation with AI
- Virtual workstations
- Rendering farms

vs A100:
- Lower memory bandwidth
- Less HBM capacity
- Better price/performance for inference
- Graphics capabilities

```

### GPU Comparison Table

| GPU      | Memory   | TF32 TFLOPS | FP8 TFLOPS | Bandwidth | Use Case                 |
| -------- | -------- | ----------- | ---------- | --------- | ------------------------ |
| **A100** | 40/80 GB | 156         | -          | 2.0 TB/s  | Training & inference     |
| **H100** | 80 GB    | 989         | 3,958      | 3.35 TB/s | Largest models           |
| **L40S** | 48 GB    | 362         | 733        | 864 GB/s  | Cost-effective inference |

### Multi-Instance GPU (MIG)

- Splitting one physical GPU into multiple smaller independent GPUs .
- MIG allows a single GPU to be partitioned into multiple hardware-isolated instances with dedicated compute, cache, and memory bandwidth. This enables multi-tenant workloads, improves utilization, and guarantees quality of service in shared environments.

```
Partition single GPU into isolated instances:

A100 (80GB) MIG Configurations:

7 Ã— 1g.10gb:
â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”
â”‚10GBâ”‚â”‚10GBâ”‚â”‚10GBâ”‚â”‚10GBâ”‚â”‚10GBâ”‚â”‚10GBâ”‚â”‚10GBâ”‚
â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜

3 Ã— 2g.20gb:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   20GB   â”‚â”‚   20GB   â”‚â”‚   20GB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1 Ã— 3g.40gb + 1 Ã— 1g.10gb:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”
â”‚       40GB         â”‚â”‚10GBâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜

Benefits:
âœ“ Quality of Service (QoS)
âœ“ Resource isolation
âœ“ Better utilization
âœ“ Multi-user environments

Use Cases:
- Shared inference servers
- Jupyter notebook environments
- Development/testing
- CI/CD pipelines
```

---

## Grace CPU Architecture

### What is Grace?

**NVIDIA Grace** is ARM-based CPU designed for AI and HPC workloads.

```
Key Features:

Architecture:
- ARM Neoverse V2 cores
- 72 cores per CPU
- SPECrateÂ®2017_int_base: Industry-leading

Memory:
- LPDDR5X memory
- 480 GB/s bandwidth
- Up to 960 GB capacity
- ECC protection

Connectivity:
- NVLink-C2C to GPU (900 GB/s)
- PCIe Gen 5
- CXL support

Power Efficiency:
- 2x energy efficiency vs x86
- Lower TCO (Total Cost of Ownership)
```

### Grace Hopper Superchip

Grace-Hopper Superchip tightly integrates a Grace CPU with a Hopper GPU using high-bandwidth NVLink-C2C. It reduces CPU-GPU communication bottlenecks and enables efficient training and inference of very large AI models, especially memory-intensive workloads.

```
Grace CPU + H100 GPU in single package:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Grace CPU     â”‚ (72 ARM cores)
â”‚  480 GB/s      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    NVLink-C2C (900 GB/s)
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
â”‚  H100 GPU      â”‚ (80GB HBM3)
â”‚  3.35 TB/s     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
âœ“ 7x bandwidth vs PCIe Gen 5
âœ“ Unified memory address space
âœ“ Lower latency CPU-GPU transfers
âœ“ Better for memory-intensive workloads

Use Cases:
- Large language models
- Recommender systems
- Graph analytics
- Scientific computing
```

### Grace-Grace Superchip

Grace-Grace Superchip is a dual-CPU architecture connected via NVLink-C2C, designed for high-memory-bandwidth, memory-intensive AI and HPC workloads. It is ideal for data preprocessing, orchestration, and large dataset handling in AI systems where GPU tensor compute is not the primary bottleneck.

```
Two Grace CPUs connected:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Grace CPU 0   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    NVLink-C2C (900 GB/s)
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Grace CPU 1   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total:
- 144 ARM cores
- Up to 960 GB memory
- Exceptional memory bandwidth

Use Cases:
- Large-scale data analytics
- In-memory databases
- Simulation
- CPU-bound HPC
```

---

## DGX Systems

### NVIDIA DGX Platform

**DGX** is NVIDIA's integrated AI infrastructure platform.

```
Components:
- Multiple high-end GPUs
- High-speed NVLink interconnect
- Optimized software stack
- Enterprise support
- Validated configurations
```

### DGX H100

```
Specifications:

GPUs:
- 8Ã— H100 SXM (80GB each)
- Total GPU memory: 640 GB
- NVLink connections between all GPUs

CPU:
- 2Ã— Intel Xeon Platinum 8480C (56 cores each)
- 112 cores total

System Memory:
- 2 TB DDR5

Storage:
- 30 TB NVMe (Gen 4)

Networking:
- 8Ã— OSFP ports (400 Gb/s each)
- ConnectX-7 NICs

Performance:
- FP8: 32 petaFLOPS
- TF32: 16 petaFLOPS

Power:
- 10.2 kW

Pricing:
- ~$450,000-$500,000
```

### DGX A100

```
Specifications:

GPUs:
- 8Ã— A100 SXM (80GB each)
- Total GPU memory: 640 GB
- NVLink connections between all GPUs

CPU:
- 2Ã— AMD EPYC 7742 (64 cores each)
- 128 cores total

System Memory:
- 1 TB DDR4

Storage:
- 15 TB NVMe

Networking:
- 8Ã— 200 Gb/s ConnectX-6

Performance:
- FP16: 5 petaFLOPS
- TF32: 2.5 petaFLOPS

Power:
- 6.5 kW

Pricing:
- ~$200,000
```

### DGX POD

```
Scalable AI infrastructure unit:

DGX H100 POD:
- 32Ã— DGX H100 systems
- 256 H100 GPUs total
- 1 exaFLOP FP8 performance

Networking:
- NVIDIA Quantum-2 InfiniBand
- Fat-tree topology
- Non-blocking 400G

Software Stack:
- DGX OS (Ubuntu-based)
- NVIDIA AI Enterprise
- Container runtime
- Cluster management

Use Cases:
- Enterprise AI
- Cloud service providers
- Research institutions
- Large-scale training
```

### DGX SuperPOD

```
Datacenter-scale AI infrastructure:

Configuration:
- 128-1024+ DGX systems
- Thousands of GPUs
- Exascale performance

Example (DGX H100 SuperPOD):
- 256 DGX H100 systems
- 2,048 H100 GPUs
- 8 exaFLOPS FP8

Features:
âœ“ Pre-validated architecture
âœ“ Reference designs
âœ“ Deployment services
âœ“ Management software

Customers:
- Microsoft Azure
- Oracle Cloud
- Meta AI Research
- National labs
```

---

## Memory Hierarchy

### GPU Memory (HBM)

```
High Bandwidth Memory:

HBM2e (A100):
- Capacity: 40 or 80 GB
- Bandwidth: 2.0 TB/s
- Stacked DRAM on interposer
- Lower power than GDDR

HBM3 (H100):
- Capacity: 80 GB
- Bandwidth: 3.35 TB/s
- Higher density
- Better efficiency

Why HBM matters for AI:
- Models fit in memory
- Fast weight access
- High throughput training
```

### Memory Requirements

```
Model size (FP16):
Parameters Ã— 2 bytes

Training memory (FP16 + Adam):
Parameters Ã— 18 bytes
(2 model + 2 gradients + 4 optimizer states + overheads)

Examples:

BERT-Base (110M):
- Inference: 0.22 GB
- Training: 2 GB

GPT-2 (1.5B):
- Inference: 3 GB
- Training: 27 GB

GPT-3 (175B):
- Inference: 350 GB (needs 5Ã— A100 80GB)
- Training: 3,150 GB (needs 40Ã— A100 80GB)

Llama-2-70B:
- Inference: 140 GB (needs 2Ã— A100 80GB)
- Training: 1,260 GB (needs 16Ã— A100 80GB)
```

### Memory Optimization

```
Techniques:

1. Mixed Precision:
   FP16 instead of FP32 â†’ 2x reduction

2. Quantization:
   INT8 instead of FP16 â†’ 2x reduction
   INT4 â†’ 4x reduction

3. Gradient Checkpointing:
   Recompute activations instead of storing
   Trade compute for memory

4. Model Parallelism:
   Split model across GPUs

5. CPU Offloading:
   Store parameters in CPU RAM
   Transfer to GPU as needed

6. Memory Pooling:
   Pre-allocate memory pools to reduce allocation overhead
   Reuse memory buffers instead of frequent allocate/free
```

---

## Distributed Training Infrastructure

### NVLink

```
High-bandwidth GPU-to-GPU interconnect:

NVLink 3 (A100):
- 600 GB/s bidirectional
- 12 links per GPU
- Peer-to-peer transfers

NVLink 4 (H100):
- 900 GB/s bidirectional
- 18 links per GPU
- Lower latency

vs PCIe Gen 5:
- PCIe: 128 GB/s
- NVLink 4: 900 GB/s (7x faster)

Benefits:
âœ“ Fast gradient synchronization
âœ“ Model parallelism
âœ“ Unified memory access
```

### NVSwitch

```
Fully connected GPU fabric:

8 GPUs without NVSwitch:
Limited connections between GPUs

8 GPUs with NVSwitch:
Every GPU connected to every other GPU
Full bandwidth

DGX H100:
- 4Ã— NVSwitch chips
- 8 GPUs fully connected
- 900 GB/s between any pair
```

### InfiniBand

```
High-performance networking:

NVIDIA Quantum-2:
- 400 Gb/s per port (NDR)
- Ultra-low latency (<100 ns)
- RDMA support

vs Ethernet:
- Lower latency
- Higher bandwidth
- Better for HPC/AI

Use Cases:
- Multi-node training
- Distributed inference
- Storage access
```

### Multi-Node Training

```
Scaling strategies:

Data Parallelism:
- Each node has full model copy
- Different data batches
- Synchronize gradients

Tensor Parallelism:
- Split layers across GPUs
- Each GPU has part of each layer

Pipeline Parallelism:
- Split model vertically
- GPU 1: Layers 1-10
- GPU 2: Layers 11-20

Hybrid:
- Combine all strategies
- For models too large for single node
```

---

## Data Center Considerations

### Power and Cooling

```
Power Requirements:

Single DGX H100:
- Power: 10.2 kW
- Requires: 208V 3-phase

32Ã— DGX H100 (POD):
- Power: 326 kW
- Cooling: Liquid cooling recommended

Annual Power Cost (example):
- 1 DGX H100: 10.2 kW Ã— 24h Ã— 365d Ã— $0.10/kWh = $9,000/year
- 32 DGX H100: $288,000/year

Cooling:
- Air cooling: Up to 30-40 kW/rack
- Liquid cooling: 50+ kW/rack
- H100 optimized for liquid cooling
```

### Rack Design

```
Typical AI Rack:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4Ã— DGX H100     â”‚ 40 kW
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Network Switchesâ”‚ 2 kW
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Storage         â”‚ 1 kW
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PDU             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: ~43 kW per rack
Requires: Liquid cooling or specialized HVAC
```

### Network Architecture

```
Typical POD Network:

Leaf-Spine Topology:

Spine Switches (400G)
     â†“  â†“  â†“  â†“
Leaf Switches (400G)
  â†“ â†“ â†“ â†“ â†“ â†“ â†“ â†“
DGX Systems

Benefits:
âœ“ Non-blocking
âœ“ Low latency
âœ“ Scalable
âœ“ Redundant paths
```

---

## Cloud Infrastructure

### Major Cloud Providers

#### AWS

```
GPU Instances:

P4 (A100):
- p4d.24xlarge: 8Ã— A100 (40GB)
- 320 GB GPU memory
- 400 Gbps EFA networking
- ~$32/hour

P5 (H100):
- p5.48xlarge: 8Ã— H100
- 640 GB GPU memory
- 3200 Gbps EFA networking
- ~$100/hour

Services:
- SageMaker (managed ML)
- EC2 (VMs with GPUs)
- EKS (Kubernetes with GPU)
```

#### Azure

```
GPU Instances:

ND A100 v4:
- 8Ã— A100 (80GB)
- 640 GB GPU memory
- ~$30/hour

ND H100 v5:
- 8Ã— H100
- 640 GB GPU memory
- ~$90/hour

Services:
- Azure ML (managed ML)
- VM Scale Sets
- AKS (Kubernetes with GPU)
```

#### Google Cloud Platform

```
GPU Instances:

A2 (A100):
- a2-ultragpu-8g: 8Ã— A100 (80GB)
- 640 GB GPU memory
- ~$35/hour

A3 (H100):
- a3-highgpu-8g: 8Ã— H100
- 640 GB GPU memory
- ~$110/hour

Services:
- Vertex AI (managed ML)
- GKE (Kubernetes with GPU)
- TPU option (alternative to GPU)
```

### On-Premise vs Cloud

```
On-Premise:

Pros:
âœ“ Long-term cost savings
âœ“ Data privacy and control
âœ“ No egress fees
âœ“ Customization

Cons:
âœ— High upfront cost
âœ— Maintenance overhead
âœ— Capacity planning
âœ— Upgrade cycles

Breakeven: ~6-12 months of continuous use

Cloud:

Pros:
âœ“ No upfront cost
âœ“ Elastic scaling
âœ“ Latest hardware
âœ“ Managed services

Cons:
âœ— Ongoing costs
âœ— Data egress fees
âœ— Less control
âœ— Potential latency

Best for: Variable workloads, experimentation
```

---

## Key Takeaways

### Hardware

1. **GPUs** provide massive parallelism for AI workloads (1000s of cores)

2. **Tensor Cores** accelerate matrix operations 8-10x vs CUDA cores

3. **A100** is the workhorse for training and inference (40/80 GB)

4. **H100** provides 3x performance over A100 with FP8 precision

5. **L40S** offers cost-effective inference with graphics capabilities

### Memory and Interconnect

6. **HBM** provides high bandwidth memory critical for large models

7. **Memory requirements**: FP16 model = 2 bytes/param, training = 18 bytes/param

8. **NVLink** enables 900 GB/s GPU-to-GPU communication (H100)

9. **Multi-Instance GPU (MIG)** partitions A100 into 7 isolated instances

10. **InfiniBand** provides ultra-low latency for multi-node training

### Systems

11. **Grace CPU** is ARM-based, optimized for AI and HPC

12. **Grace Hopper** combines Grace CPU + H100 GPU with 900 GB/s interconnect

13. **DGX H100** integrates 8Ã— H100 GPUs with 32 petaFLOPS FP8

14. **DGX POD** scales to 256 H100 GPUs (1 exaFLOP)

15. **DGX SuperPOD** reaches thousands of GPUs for exascale computing

### Infrastructure

16. **Liquid cooling** required for high-density racks (40+ kW)

17. **Leaf-spine network** topology provides non-blocking communication

18. **Cloud providers** offer A100/H100 instances (~$30-100/hour)

19. **Breakeven** for on-premise is 6-12 months of continuous use

20. **Power costs** can exceed $9,000/year for single DGX H100

---



## Related Modules

- **[02: AI/ML Fundamentals](02.AI_ML_Fundamentals.md)** - Neural networks running on this hardware
- **[03: Generative AI and LLMs](03.Generative_AI_and_LLM.md)** - LLMs requiring massive infrastructure
- **[07: Model Training](07.Model_Training.md)** - Distributed training on multi-GPU systems
- **[08: NVIDIA Ecosystem](08.NVIDIA_Ecosystem.md)** - Software stack for this hardware
- **[10: Additional Topics](10.Additional_Topics.md)** - Advanced applications

---

**Next Module**: [02: AI and ML Fundamentals](02.AI_ML_Fundamentals.md)

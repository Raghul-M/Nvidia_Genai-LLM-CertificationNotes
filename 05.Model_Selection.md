# O5: Model Selection and Embeddings

## Table of Contents
- [Introduction to Model Selection](#introduction-to-model-selection)
- [LLM Model Families](#llm-model-families)
- [Model Size Considerations](#model-size-considerations)
- [Embeddings](#embeddings)
- [Vector Databases](#vector-databases)
- [Semantic Search](#semantic-search)
- [Model Evaluation and Benchmarks](#model-evaluation-and-benchmarks)
- [Choosing the Right Model](#choosing-the-right-model)
- [Key Takeaways](#key-takeaways)
- [Practice Questions](#practice-questions)

---

## Introduction to Model Selection

### Why Model Selection Matters

```
Wrong model choice:
❌ Poor performance on your task
❌ Wasted compute resources
❌ Higher costs
❌ Slower inference
❌ Unnecessary complexity

Right model choice:
✅ Optimal performance
✅ Cost-effective
✅ Fast responses
✅ Meets requirements
✅ Maintainable
```

### Key Decision Factors

```
1. Task Type:
   - Text generation vs understanding
   - Classification vs QA
   - Code vs natural language

2. Performance Requirements:
   - Accuracy needed
   - Latency constraints
   - Throughput targets

3. Resource Constraints:
   - Available GPU memory
   - Compute budget
   - Inference cost

4. Data Availability:
   - Training data quantity
   - Domain-specific needs
   - Fine-tuning options

5. Deployment:
   - Cloud vs edge
   - API vs self-hosted
   - Privacy requirements
```

---

## LLM Model Families

### Encoder-Only Models

```
Architecture: Transformer encoder (bidirectional)

Key Models:
- BERT
- RoBERTa
- ALBERT
- DeBERTa

Training: Masked language modeling
Input: "The [MASK] sat on the mat"
Predict: "cat"

Best For:
✓ Classification (sentiment, topic)
✓ Named entity recognition
✓ Question answering (extractive)
✓ Embeddings for semantic search

Not Good For:
✗ Text generation
✗ Creative writing
✗ Code generation
```

#### BERT (Bidirectional Encoder Representations from Transformers)

```
BERT-Base:
- Parameters: 110M
- Layers: 12
- Hidden size: 768
- Attention heads: 12
- Context: 512 tokens

BERT-Large:
- Parameters: 340M
- Layers: 24
- Hidden size: 1024
- Attention heads: 16

Pre-training Tasks:
1. Masked LM: Predict masked tokens
2. Next Sentence Prediction: Determine if sentences are consecutive

Variants:
- RoBERTa: BERT trained better (more data, longer, no NSP)
- ALBERT: Parameter sharing (fewer parameters)
- DeBERTa: Disentangled attention (better performance)

Use Cases:
- Sentence classification
- Token classification (NER)
- Extractive QA
- Semantic similarity
```

### Decoder-Only Models

```
Architecture: Transformer decoder (unidirectional)

Key Models:
- GPT-3, GPT-4
- Llama 2
- PaLM
- Claude
- Mistral

Training: Next token prediction
Input: "The cat sat on the"
Predict: "mat"

Best For:
✓ Text generation
✓ Creative writing
✓ Code generation
✓ Conversational AI
✓ Few-shot learning

Not Good For:
✗ Embeddings (use last layer, but encoder better)
```

#### GPT Family

```
GPT-3:
- Parameters: 175B
- Layers: 96
- Hidden size: 12,288
- Context: 2,048 tokens
- Few-shot learning capability

GPT-3.5 (ChatGPT):
- GPT-3 + RLHF
- Instruction following
- Conversational

GPT-4:
- Multimodal (text + images)
- Improved reasoning
- 32K context (extended version)
- Better at math, coding

Strengths:
✓ Versatile
✓ Strong few-shot learning
✓ High-quality generation

Weaknesses:
✗ Closed-source
✗ Expensive
✗ Data privacy concerns
```

#### Llama Family

```
Llama 2:
- Sizes: 7B, 13B, 70B
- Open weights
- Context: 4,096 tokens
- Commercial license

Architecture improvements:
- RMSNorm (instead of LayerNorm)
- SwiGLU activation
- Grouped-query attention
- RoPE positional embeddings

Llama 2 Chat:
- RLHF-tuned
- Conversational
- Safety guidelines

Code Llama:
- Specialized for code
- 7B, 13B, 34B
- Python, general code

Strengths:
✓ Open weights
✓ Commercial use
✓ Competitive performance
✓ Cost-effective (self-host)

Use Cases:
- Custom chatbots
- Code assistants
- Domain-specific applications
- Research
```

#### Mistral Models

```
Mistral 7B:
- Parameters: 7B
- Outperforms Llama 2 13B
- Sliding window attention
- Apache 2.0 license

Mixtral 8x7B:
- Mixture of Experts
- 47B total parameters
- 13B active per token
- Efficient scaling

Strengths:
✓ Excellent performance/size ratio
✓ Open source
✓ Efficient inference
```

### Encoder-Decoder Models

```
Architecture: Encoder + Decoder

Key Models:
- T5
- BART
- UL2

Training: Span corruption / denoising

Best For:
✓ Translation
✓ Summarization
✓ Question answering (abstractive)
✓ Text-to-text tasks

Example:
Input: "Translate to French: Hello"
Output: "Bonjour"
```

#### T5 (Text-to-Text Transfer Transformer)

```
Sizes:
- T5-Small: 60M
- T5-Base: 220M
- T5-Large: 770M
- T5-3B: 3B
- T5-11B: 11B

Everything is text-to-text:

Classification:
Input: "sentiment: This movie was great!"
Output: "positive"

Translation:
Input: "translate English to German: Hello"
Output: "Hallo"

Summarization:
Input: "summarize: [long text]"
Output: [summary]

Strengths:
✓ Unified framework
✓ Multi-task training
✓ Flexible

Use Cases:
- Translation systems
- Summarization
- Data-to-text generation
```

---

## Model Size Considerations

### Small Models (< 1B parameters)

```
Examples:
- DistilBERT: 66M
- BERT-Base: 110M
- T5-Small: 60M
- GPT-2: 117M-355M

Memory (FP16):
- 100M params: ~200 MB
- 500M params: ~1 GB

Inference:
- Latency: 10-50ms
- Hardware: CPU or single GPU
- Cost: Very low

Use Cases:
✓ Mobile/edge deployment
✓ Real-time applications
✓ High-volume, simple tasks
✓ Cost-sensitive applications

Limitations:
✗ Limited reasoning
✗ Less knowledge
✗ Weaker few-shot learning
```

### Medium Models (1B-10B parameters)

```
Examples:
- BERT-Large: 340M
- GPT-2-XL: 1.5B
- T5-3B: 3B
- Llama 2 7B: 7B
- Mistral 7B: 7B

Memory (FP16):
- 1B params: ~2 GB
- 7B params: ~14 GB

Inference:
- Latency: 50-200ms
- Hardware: Single GPU (A10, A100)
- Cost: Low-medium

Use Cases:
✓ General-purpose applications
✓ Chatbots
✓ Content generation
✓ Code assistants
✓ Domain-specific tasks

Sweet Spot:
- Good performance
- Reasonable cost
- Fits on single GPU
```

### Large Models (10B-100B parameters)

```
Examples:
- GPT-3: 175B
- Llama 2 70B: 70B
- PaLM 2: ~340B (estimated)

Memory (FP16):
- 70B params: ~140 GB
- 175B params: ~350 GB

Inference:
- Latency: 200-1000ms
- Hardware: Multiple GPUs
- Cost: High

Use Cases:
✓ Complex reasoning
✓ Advanced coding
✓ Research
✓ Few-shot learning
✓ High-quality generation

Considerations:
- Multi-GPU required
- Higher latency
- Expensive inference
- Best for quality-critical tasks
```

### Frontier Models (100B+ parameters)

```
Examples:
- GPT-4: ~1T+ (estimated)
- PaLM: 540B
- Claude: Unknown

Characteristics:
- Emergent abilities
- Best reasoning
- Multimodal capabilities
- Highest quality

Access:
- Typically via API only
- Very expensive
- Rate limits

Use When:
- Absolute best quality needed
- Complex reasoning required
- Prototyping before fine-tuning
- High-value applications
```

### Quantization Trade-offs

```
Model: 7B parameters

FP16 (16-bit):
- Memory: 14 GB
- Quality: 100% (baseline)
- Speed: 1x

INT8 (8-bit):
- Memory: 7 GB (2x reduction)
- Quality: 99%
- Speed: 1.5-2x faster

INT4 (4-bit):
- Memory: 3.5 GB (4x reduction)
- Quality: 95-97%
- Speed: 2-3x faster

Rule of thumb:
- INT8: Almost no quality loss
- INT4: Acceptable for most tasks
- Lower: Experimental
```

---

## Embeddings

### What are Embeddings?

```
Embeddings: Dense vector representations of text

"cat" → [0.2, -0.5, 0.8, ..., 0.3]  (768 dims)
"dog" → [0.3, -0.4, 0.7, ..., 0.2]  (similar to "cat")
"car" → [-0.5, 0.1, -0.3, ..., 0.9] (different from "cat")

Properties:
✓ Similar meaning → Similar vectors
✓ Captures semantic relationships
✓ Fixed-size representation
✓ Works with vector operations

Why useful:
- Semantic search
- Clustering
- Classification
- Recommendation
- RAG systems
```

### Word Embeddings

#### Word2Vec

```
Two architectures:

1. CBOW (Continuous Bag of Words):
   Context → Predict center word
   "The [?] sat on mat" → "cat"

2. Skip-gram:
   Center word → Predict context
   "cat" → ["The", "sat", "on", "mat"]

Dimension: 100-300 typically

Properties:
- king - man + woman ≈ queen
- Paris - France + Italy ≈ Rome

Limitations:
✗ One vector per word (no context)
✗ "bank" (river) vs "bank" (financial) → same vector
```

#### GloVe (Global Vectors)

```
Combines:
- Global matrix factorization
- Local context windows

Pre-trained:
- GloVe-50d, GloVe-100d, GloVe-300d
- Trained on Wikipedia + Gigaword

Use:
from gensim.downloader import load
glove = load('glove-wiki-gigaword-100')

vector = glove['computer']  # 100-dim vector
similar = glove.most_similar('computer')
# [('software', 0.89), ('technology', 0.85), ...]
```

### Sentence Embeddings

#### Sentence-BERT (SBERT)

```
BERT modified for sentence embeddings:

Architecture:
Sentence → BERT → Pooling → Fixed-size vector

Pooling strategies:
1. CLS token: Use [CLS] embedding
2. Mean pooling: Average all tokens
3. Max pooling: Max over tokens

Dimension: 384, 768, 1024

Models:
- all-MiniLM-L6-v2: 384-dim, fast
- all-mpnet-base-v2: 768-dim, accurate
- multi-qa-mpnet-base: For Q&A

Usage:
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

sentences = [
    "The cat sits on the mat",
    "A feline rests on a rug",
    "The dog runs in the park"
]

embeddings = model.encode(sentences)
# embeddings.shape = (3, 384)

# Similarity
from sklearn.metrics.pairwise import cosine_similarity

sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
# 0.85 (high similarity - cat and feline)

sim = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]
# 0.42 (lower similarity - cat and dog different actions)
```

### Embedding Models Comparison

```
OpenAI Embeddings:
- text-embedding-ada-002
- Dimensions: 1536
- Max tokens: 8,191
- Cost: $0.0001 per 1K tokens
- Quality: Excellent
- API-only

Sentence-BERT:
- Various models
- Dimensions: 384-1024
- Free, open-source
- Run locally
- Good quality

Instructor Embeddings:
- Task-specific instructions
- "Represent the query for retrieval: [text]"
- Better domain adaptation

Cohere Embeddings:
- embed-english-v3.0
- Dimensions: 1024
- API-based
- Compression support

Recommendation:
- Prototyping: OpenAI (easy)
- Production: SBERT (cost-effective)
- Domain-specific: Instructor or fine-tune SBERT
```

---

## Vector Databases

### Why Vector Databases?

```
Problem: Search billions of embeddings efficiently

Traditional database:
- Exact match: "Find id=123" → Fast
- Similarity search: Slow for high dimensions

Vector database:
- Approximate nearest neighbor (ANN)
- Fast similarity search
- Scale to billions of vectors
```

### Popular Vector Databases

#### 1. Pinecone

```
Managed cloud service

Features:
✓ Fully managed
✓ Auto-scaling
✓ Real-time updates
✓ Metadata filtering
✓ Hybrid search

Pricing: Based on usage

Usage:
import pinecone

pinecone.init(api_key="your-key")

index = pinecone.Index("my-index")

# Upsert vectors
index.upsert([
    ("id1", [0.1, 0.2, ...], {"text": "..."}),
    ("id2", [0.3, 0.4, ...], {"text": "..."})
])

# Query
results = index.query(
    vector=[0.15, 0.25, ...],
    top_k=5,
    include_metadata=True
)
```

#### 2. Weaviate

```
Open-source, self-hosted or cloud

Features:
✓ Open source
✓ GraphQL API
✓ Vectorization modules
✓ Hybrid search (vector + keyword)
✓ Multi-tenancy

Usage:
import weaviate

client = weaviate.Client("http://localhost:8080")

# Create schema
client.schema.create({
    "class": "Document",
    "vectorizer": "text2vec-transformers",
    "properties": [
        {"name": "content", "dataType": ["text"]}
    ]
})

# Add data
client.data_object.create({
    "content": "Your text here"
}, "Document")

# Query
result = client.query.get("Document", ["content"]) \
    .with_near_text({"concepts": ["search query"]}) \
    .with_limit(5) \
    .do()
```

#### 3. Milvus

```
Open-source, distributed

Features:
✓ Highly scalable
✓ Multiple index types
✓ GPU acceleration
✓ Cloud-native
✓ High performance

Usage:
from pymilvus import connections, Collection

connections.connect("default", host="localhost", port="19530")

collection = Collection("my_collection")

# Insert
collection.insert([
    [1, 2, 3],  # IDs
    [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]  # Vectors
])

# Search
results = collection.search(
    data=[[0.15, 0.25, ...]],
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"nprobe": 10}},
    limit=5
)
```

#### 4. ChromaDB

```
Lightweight, embedded

Features:
✓ Easy to use
✓ Embedded or client-server
✓ Built for LLM applications
✓ Free, open-source

Usage:
import chromadb

client = chromadb.Client()

collection = client.create_collection("my_docs")

# Add documents (auto-vectorization)
collection.add(
    documents=["This is document 1", "This is document 2"],
    metadatas=[{"source": "web"}, {"source": "book"}],
    ids=["id1", "id2"]
)

# Query
results = collection.query(
    query_texts=["search query"],
    n_results=5
)
```

#### 5. Qdrant

```
High-performance, Rust-based

Features:
✓ Fast (Rust)
✓ Rich filtering
✓ Payload support
✓ Cloud or self-hosted
✓ Good for production

Usage:
from qdrant_client import QdrantClient

client = QdrantClient("localhost", port=6333)

# Create collection
client.create_collection(
    collection_name="my_collection",
    vectors_config={"size": 384, "distance": "Cosine"}
)

# Upsert
client.upsert(
    collection_name="my_collection",
    points=[
        {"id": 1, "vector": [0.1, 0.2, ...], "payload": {"text": "..."}}
    ]
)

# Search
results = client.search(
    collection_name="my_collection",
    query_vector=[0.15, 0.25, ...],
    limit=5
)
```

### Comparison

| Database | Hosting | Open Source | Best For |
|----------|---------|-------------|----------|
| **Pinecone** | Cloud only | ✗ | Ease of use, managed |
| **Weaviate** | Both | ✓ | Hybrid search, GraphQL |
| **Milvus** | Both | ✓ | Large scale, performance |
| **ChromaDB** | Both | ✓ | LLM apps, simplicity |
| **Qdrant** | Both | ✓ | Production, filtering |

---

## Semantic Search

### How Semantic Search Works

```
Traditional Search (Keyword):
Query: "best Italian restaurant"
Matches: Documents containing "best", "Italian", "restaurant"
Misses: "top pasta places", "finest trattorias"

Semantic Search (Vector):
Query: "best Italian restaurant"
  ↓ Embed
Query vector: [0.2, -0.5, ...]
  ↓ Compare with
Document vectors in database
  ↓ Find nearest neighbors
Results: "top pasta places", "finest trattorias", ...

Captures meaning, not just keywords!
```

### Implementation

```python
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 1. Prepare model
model = SentenceTransformer('all-MiniLM-L6-v2')

# 2. Your documents
documents = [
    "Python is a programming language",
    "Machine learning uses algorithms",
    "Neural networks are inspired by the brain",
    "The weather is sunny today",
    "I love eating pizza"
]

# 3. Generate embeddings
doc_embeddings = model.encode(documents)

# 4. User query
query = "What is Python?"
query_embedding = model.encode([query])

# 5. Compute similarities
similarities = cosine_similarity(query_embedding, doc_embeddings)[0]

# 6. Rank results
results = sorted(
    zip(documents, similarities),
    key=lambda x: x[1],
    reverse=True
)

# 7. Display top results
for doc, score in results[:3]:
    print(f"Score: {score:.3f} - {doc}")

# Output:
# Score: 0.652 - Python is a programming language
# Score: 0.431 - Machine learning uses algorithms
# Score: 0.387 - Neural networks are inspired by the brain
```

### Hybrid Search

```
Combine vector search + keyword search:

Vector Search:
- Semantic similarity
- Handles synonyms
- Contextual understanding

Keyword Search (BM25):
- Exact matches
- Important for proper nouns
- Fast

Hybrid Approach:
combined_score = α × vector_score + (1-α) × keyword_score

α = 0.7 (typical)

Benefits:
✓ Best of both worlds
✓ Robust to different query types
✓ Better precision and recall

Implementations:
- Weaviate (built-in)
- Elasticsearch + KNN
- Custom (combine scores)
```

---

## Model Evaluation and Benchmarks

### Common Benchmarks

#### MMLU (Massive Multitask Language Understanding)

```
Tests: 57 subjects (math, history, law, etc.)
Format: Multiple choice
Questions: 15,908 total

Example:
Q: What is the capital of France?
A) London  B) Berlin  C) Paris  D) Madrid

Scores (few-shot):
- GPT-4: 86%
- GPT-3.5: 70%
- Llama 2 70B: 69%
- Llama 2 7B: 46%

Measures: World knowledge + reasoning
```

#### HellaSwag

```
Tests: Commonsense reasoning
Format: Sentence completion

Example:
Context: "A woman is outside with a bucket and a dog..."
Correct: "starts to pet the dog"
Wrong: "is seen walking across a hall"

Scores:
- GPT-4: 95%
- Claude 2: 85%
- Llama 2 70B: 87%

Measures: Physical commonsense
```

#### HumanEval

```
Tests: Code generation
Format: Python function completion
Problems: 164 coding challenges

Example:
def add(a, b):
    """Return the sum of a and b."""
    # Complete this function

Scores (pass@1):
- GPT-4: 67%
- GPT-3.5: 48%
- CodeLlama 34B: 54%
- Llama 2 70B: 30%

Measures: Coding ability
```

#### TruthfulQA

```
Tests: Truthfulness
Format: Questions with misleading answers

Example:
Q: "What happens if you crack your knuckles?"
Correct: "Nothing harmful"
Tempting wrong: "You'll get arthritis"

Measures: Resistance to falsehoods
Important for reducing hallucinations
```

### Task-Specific Evaluation

```
For your use case:

1. Create evaluation set:
   - Representative examples
   - Include edge cases
   - 100-1000 examples

2. Define metrics:
   Classification: Accuracy, F1
   Generation: BLEU, ROUGE, human eval
   RAG: Precision, recall, answer quality

3. Test models:
   - Few-shot prompting
   - With/without RAG
   - Different temperatures

4. Analyze:
   - Quantitative metrics
   - Qualitative review
   - Error analysis

5. Select:
   - Best performance
   - Cost vs quality trade-off
   - Deployment constraints
```

---

## Choosing the Right Model

### Decision Framework

```
flowchart:

Start
  ↓
Need generation?
  Yes → Decoder-only (GPT, Llama)
  No → Need classification?
    Yes → Encoder-only (BERT)
    No → Seq2seq? → Encoder-decoder (T5)
  ↓
Quality requirements?
  Critical → Large model (70B+, GPT-4)
  High → Medium model (7B-70B)
  Moderate → Small model (<7B)
  ↓
Latency requirements?
  <50ms → Small model, quantized
  <200ms → Medium model
  >200ms → Large model OK
  ↓
Budget?
  High → API (GPT-4, Claude)
  Medium → Self-host medium model
  Low → Self-host small model
  ↓
Privacy?
  Critical → Self-host (Llama)
  Not critical → API OK
  ↓
Selected Model
```

### Common Use Cases

```
Chatbot (general):
→ Llama 2 7B Chat or GPT-3.5

Chatbot (high-quality):
→ GPT-4 or Claude 2

Code Assistant:
→ CodeLlama 13B or GPT-4

Classification:
→ Fine-tuned BERT or RoBERTa

Semantic Search:
→ SBERT (all-mpnet-base-v2) + ChromaDB

Translation:
→ Fine-tuned T5 or NLLB

Summarization:
→ T5 or GPT-3.5

RAG System:
→ Llama 2 13B + SBERT embeddings + Qdrant

Edge Deployment:
→ DistilBERT or Phi-2

Content Moderation:
→ Fine-tuned RoBERTa
```

### Cost Comparison

```
7B Model (Self-hosted):
- GPU: A10 ($1.50/hour)
- Throughput: ~100 req/hour
- Cost per 1M tokens: ~$0.50

70B Model (Self-hosted):
- GPU: 2× A100 ($5/hour)
- Throughput: ~50 req/hour
- Cost per 1M tokens: ~$3

GPT-3.5 (API):
- Cost per 1M tokens: $0.50-$2

GPT-4 (API):
- Cost per 1M tokens: $30-$60

Breakeven:
Self-hosting makes sense at >100K requests/month
```

---

## Key Takeaways

### Model Architectures

1. **Encoder-only** (BERT) for classification and understanding

2. **Decoder-only** (GPT, Llama) for text generation

3. **Encoder-decoder** (T5) for sequence-to-sequence tasks

4. **Model size** ranges from 100M to 1T+ parameters

5. **Quantization** reduces memory 2-4x with minimal quality loss

### Embeddings

6. **Word embeddings** (Word2Vec, GloVe) represent individual words

7. **Sentence embeddings** (SBERT) represent entire sentences

8. **Semantic similarity** measured by cosine similarity

9. **Embedding dimensions** typically 384-1536

10. **Task-specific embeddings** improve domain performance

### Vector Databases

11. **Vector databases** enable fast similarity search at scale

12. **Pinecone** is managed cloud, easiest to use

13. **ChromaDB** is lightweight, great for prototyping

14. **Milvus/Qdrant** are production-grade, high-performance

15. **Hybrid search** combines vector + keyword for best results

### Model Selection

16. **Benchmarks** (MMLU, HellaSwag) compare model capabilities

17. **Task-specific evaluation** crucial for real-world performance

18. **Cost-performance trade-off** varies by deployment

19. **Llama 2** offers best open-source option

20. **GPT-4** provides highest quality at highest cost

---

## Practice Questions

1. When would you choose BERT over GPT for a task?

2. Compare Llama 2 7B, 13B, and 70B - which for what use case?

3. How do sentence embeddings differ from word embeddings?

4. Calculate the memory needed for a 13B model in FP16 and INT8.

5. Design a semantic search system for a documentation site.

6. What vector database would you choose for 1M documents? Why?

7. Explain how hybrid search combines vector and keyword search.

8. What is cosine similarity and how is it computed?

9. Compare OpenAI embeddings vs SBERT for production use.

10. How would you evaluate a model for customer support chatbot?

11. Design an embedding + vector DB system for RAG.

12. What factors determine model selection for edge deployment?

13. Compare self-hosting Llama 2 70B vs using GPT-4 API (cost/performance).

14. How do you choose embedding dimensions (384 vs 1536)?

15. Create a decision tree for model selection given requirements.

---

## Related Modules

- **[O3: Generative AI and LLMs](O3.Generative_AI_and_LLM.md)** - Understanding LLM capabilities
- **[O4: Transformer Architecture](O4.Transformer_Architecture.md)** - How models work internally
- **[O6: Model Customization](O6.Model_Customization.md)** - RAG uses embeddings + vector DBs
- **[O7: Model Training](O7.Model_Training.md)** - Fine-tuning selected models
- **[O8: NVIDIA Ecosystem](O8.NVIDIA_Ecosystem.md)** - Tools for model deployment
- **[O1: AI Infrastructure](O1.AI_Infrastructure.md)** - Hardware requirements

---

**Next Module**: [O6: Model Customization](O6.Model_Customization.md)
